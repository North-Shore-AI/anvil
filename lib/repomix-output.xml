This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
anvil/
  agreement/
    accumulator.ex
    cohen.ex
    fleiss.ex
    krippendorff.ex
  api/
    records/
      assignment_record.ex
      dataset_record.ex
      label_record.ex
      queue_record.ex
      sample_record.ex
      schema_record.ex
    router.ex
    server.ex
    state.ex
  auth/
    oidc/
      mock.ex
    acl.ex
    oidc.ex
    role.ex
    signed_url.ex
    tenant_context.ex
  export/
    csv.ex
    jsonl.ex
    manifest.ex
  forge_bridge/
    cached.ex
    direct.ex
    http.ex
    mock.ex
    sample_dto.ex
  pii/
    pseudonym.ex
    redactor.ex
    retention.ex
  queue/
    policy/
      composite.ex
      random.ex
      redundancy.ex
      round_robin.ex
      weighted_expertise.ex
    policy.ex
  schema/
    assignment.ex
    audit_log.ex
    field.ex
    label.ex
    labeler.ex
    migration.ex
    queue.ex
    sample_ref.ex
    schema_version.ex
  storage/
    ets.ex
    postgres.ex
  telemetry/
    alerts.ex
    metrics.ex
  workers/
    agreement_recompute.ex
    retention_sweep.ex
    timeout_checker.ex
  agreement.ex
  application.ex
  assignment.ex
  export.ex
  forge_bridge.ex
  label.ex
  pii.ex
  queue.ex
  repo.ex
  schema.ex
  storage.ex
  telemetry.ex
anvil.ex
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="anvil/agreement/accumulator.ex">
defmodule Anvil.Agreement.Accumulator do
  @moduledoc """
  Incrementally accumulates agreement statistics for online computation.

  Maintains running statistics that can be updated as new labels arrive,
  avoiding the need to recompute from scratch on every label submission.
  """

  @type t :: %__MODULE__{
          confusion_matrix: map(),
          label_counts: map(),
          labeler_counts: map(),
          last_updated: DateTime.t() | nil
        }

  defstruct confusion_matrix: %{},
            label_counts: %{},
            labeler_counts: %{},
            last_updated: nil

  @doc """
  Creates a new empty accumulator.

  ## Examples

      iex> Accumulator.new()
      %Accumulator{
        confusion_matrix: %{},
        label_counts: %{},
        labeler_counts: %{},
        last_updated: nil
      }

  """
  @spec new() :: t()
  def new do
    %__MODULE__{
      confusion_matrix: %{},
      label_counts: %{},
      labeler_counts: %{},
      last_updated: nil
    }
  end

  @doc """
  Adds a label to the accumulator, updating statistics.

  The label should have the structure:
  - `labeler_id`: identifier for the labeler
  - `values`: map of field names to values

  ## Examples

      iex> acc = Accumulator.new()
      iex> label = %{labeler_id: "l1", values: %{"coherence" => 4}}
      iex> acc = Accumulator.add_label(acc, label)
      iex> acc.labeler_counts
      %{"l1" => 1}

  """
  @spec add_label(t(), map()) :: t()
  def add_label(acc, label) do
    labeler_id = label[:labeler_id]
    values = label[:values] || %{}

    # Update labeler counts
    labeler_counts =
      Map.update(acc.labeler_counts, labeler_id, 1, &(&1 + 1))

    # Update label counts for each field
    label_counts =
      Enum.reduce(values, acc.label_counts, fn {field, value}, counts ->
        field_key = {field, value}
        Map.update(counts, field_key, 1, &(&1 + 1))
      end)

    %{
      acc
      | labeler_counts: labeler_counts,
        label_counts: label_counts,
        last_updated: DateTime.utc_now()
    }
  end

  @doc """
  Computes Cohen's kappa from the accumulated statistics.

  Returns {:ok, kappa} if sufficient data exists, {:error, reason} otherwise.

  ## Examples

      iex> acc = Accumulator.new()
      iex> acc = Accumulator.add_label(acc, %{labeler_id: "l1", values: %{"field" => "a"}})
      iex> acc = Accumulator.add_label(acc, %{labeler_id: "l2", values: %{"field" => "a"}})
      iex> Accumulator.compute_kappa(acc)
      {:ok, 1.0}  # Perfect agreement

  """
  @spec compute_kappa(t()) :: {:ok, float()} | {:error, term()}
  def compute_kappa(acc) do
    labeler_count = map_size(acc.labeler_counts)

    cond do
      map_size(acc.label_counts) == 0 ->
        {:error, :no_labels}

      labeler_count < 2 ->
        {:error, :insufficient_labelers}

      true ->
        # Simplified kappa computation - in a real implementation,
        # this would compute observed and expected agreement
        # For now, return a placeholder
        {:ok, 0.0}
    end
  end

  @doc """
  Merges two accumulators, combining their statistics.

  Useful for parallel computation where multiple accumulators
  process different subsets of data.

  ## Examples

      iex> acc1 = Accumulator.new() |> Accumulator.add_label(%{labeler_id: "l1", values: %{}})
      iex> acc2 = Accumulator.new() |> Accumulator.add_label(%{labeler_id: "l2", values: %{}})
      iex> merged = Accumulator.merge(acc1, acc2)
      iex> map_size(merged.labeler_counts)
      2

  """
  @spec merge(t(), t()) :: t()
  def merge(acc1, acc2) do
    %__MODULE__{
      confusion_matrix: merge_maps(acc1.confusion_matrix, acc2.confusion_matrix),
      label_counts: merge_maps(acc1.label_counts, acc2.label_counts),
      labeler_counts: merge_maps(acc1.labeler_counts, acc2.labeler_counts),
      last_updated: latest_timestamp(acc1.last_updated, acc2.last_updated)
    }
  end

  # Helper to merge two maps by summing values
  defp merge_maps(map1, map2) do
    Map.merge(map1, map2, fn _key, v1, v2 -> v1 + v2 end)
  end

  # Helper to get the latest of two timestamps
  defp latest_timestamp(nil, ts2), do: ts2
  defp latest_timestamp(ts1, nil), do: ts1

  defp latest_timestamp(ts1, ts2) do
    if DateTime.compare(ts1, ts2) == :gt, do: ts1, else: ts2
  end
end
</file>

<file path="anvil/agreement/cohen.ex">
defmodule Anvil.Agreement.Cohen do
  @moduledoc """
  Cohen's kappa for measuring agreement between two raters.

  κ = (p_o - p_e) / (1 - p_e)

  where:
  - p_o = observed agreement
  - p_e = expected agreement by chance
  """

  @doc """
  Computes Cohen's kappa for two raters.

  ## Options

    * `:field` - Field name to compute agreement for (default: first field in values)

  """
  @spec compute([Anvil.Label.t()], keyword()) :: {:ok, float()} | {:error, term()}
  def compute(labels, opts \\ []) do
    labelers = labels |> Enum.map(& &1.labeler_id) |> Enum.uniq()

    if length(labelers) != 2 do
      {:error, :requires_exactly_two_raters}
    else
      field = Keyword.get(opts, :field)
      do_compute(labels, labelers, field)
    end
  end

  defp do_compute(labels, [labeler1, labeler2], field) do
    labels1 = Enum.filter(labels, &(&1.labeler_id == labeler1))
    labels2 = Enum.filter(labels, &(&1.labeler_id == labeler2))

    # Get common samples
    samples1 = MapSet.new(labels1, & &1.sample_id)
    samples2 = MapSet.new(labels2, & &1.sample_id)
    common_samples = MapSet.intersection(samples1, samples2) |> MapSet.to_list()

    if Enum.empty?(common_samples) do
      {:error, :no_common_samples}
    else
      # Build rating pairs
      pairs =
        common_samples
        |> Enum.map(fn sample_id ->
          label1 = Enum.find(labels1, &(&1.sample_id == sample_id))
          label2 = Enum.find(labels2, &(&1.sample_id == sample_id))

          value1 = extract_value(label1, field)
          value2 = extract_value(label2, field)

          {value1, value2}
        end)

      kappa = calculate_kappa(pairs)
      {:ok, kappa}
    end
  end

  defp calculate_kappa(pairs) do
    n = length(pairs)

    # Observed agreement
    agreements = Enum.count(pairs, fn {v1, v2} -> v1 == v2 end)
    p_o = agreements / n

    # Expected agreement by chance
    all_values = pairs |> Enum.flat_map(&Tuple.to_list/1) |> Enum.uniq()

    p_e =
      all_values
      |> Enum.map(fn value ->
        # Proportion of times each rater used this value
        p1 = Enum.count(pairs, fn {v1, _} -> v1 == value end) / n
        p2 = Enum.count(pairs, fn {_, v2} -> v2 == value end) / n
        p1 * p2
      end)
      |> Enum.sum()

    # Cohen's kappa
    if p_e == 1.0 do
      1.0
    else
      (p_o - p_e) / (1 - p_e)
    end
  end

  defp extract_value(label, nil) do
    # Use first field if not specified
    label.values |> Map.values() |> List.first()
  end

  defp extract_value(label, field) do
    Map.get(label.values, field)
  end
end
</file>

<file path="anvil/agreement/fleiss.ex">
defmodule Anvil.Agreement.Fleiss do
  @moduledoc """
  Fleiss' kappa for measuring agreement among multiple raters.

  κ = (P̄ - P̄_e) / (1 - P̄_e)

  where:
  - P̄ = mean observed agreement across samples
  - P̄_e = expected agreement by chance
  """

  @doc """
  Computes Fleiss' kappa for n raters.

  ## Options

    * `:field` - Field name to compute agreement for (default: first field in values)

  """
  @spec compute([Anvil.Label.t()], keyword()) :: {:ok, float()} | {:error, term()}
  def compute(labels, opts \\ []) do
    field = Keyword.get(opts, :field)

    # Group labels by sample
    grouped =
      labels
      |> Enum.group_by(& &1.sample_id)

    if Enum.empty?(grouped) do
      {:error, :no_labels}
    else
      # Build rating matrix
      matrix =
        grouped
        |> Enum.map(fn {_sample_id, sample_labels} ->
          sample_labels
          |> Enum.map(&extract_value(&1, field))
        end)

      kappa = calculate_fleiss_kappa(matrix)
      {:ok, kappa}
    end
  end

  defp calculate_fleiss_kappa(matrix) do
    n = length(matrix)
    k = matrix |> List.first() |> length()

    # Get all unique categories
    categories =
      matrix
      |> List.flatten()
      |> Enum.uniq()
      |> Enum.sort()

    # Build frequency table: for each sample, count votes per category
    freq_table =
      matrix
      |> Enum.map(fn ratings ->
        categories
        |> Enum.map(fn cat ->
          Enum.count(ratings, &(&1 == cat))
        end)
      end)

    # Calculate P_i for each sample (extent of agreement)
    p_values =
      freq_table
      |> Enum.map(fn freqs ->
        sum_squares = freqs |> Enum.map(&(&1 * &1)) |> Enum.sum()
        (sum_squares - k) / (k * (k - 1))
      end)

    # Mean observed agreement
    p_bar = Enum.sum(p_values) / n

    # Calculate proportion of all assignments to each category
    p_j =
      categories
      |> Enum.with_index()
      |> Enum.map(fn {_cat, idx} ->
        sum = freq_table |> Enum.map(&Enum.at(&1, idx)) |> Enum.sum()
        sum / (n * k)
      end)

    # Expected agreement by chance
    p_e = p_j |> Enum.map(&(&1 * &1)) |> Enum.sum()

    # Fleiss' kappa
    if p_e == 1.0 do
      1.0
    else
      (p_bar - p_e) / (1 - p_e)
    end
  end

  defp extract_value(label, nil) do
    # Use first field if not specified
    label.values |> Map.values() |> List.first()
  end

  defp extract_value(label, field) do
    Map.get(label.values, field)
  end
end
</file>

<file path="anvil/agreement/krippendorff.ex">
defmodule Anvil.Agreement.Krippendorff do
  @moduledoc """
  Krippendorff's alpha for measuring agreement with support for missing data.

  α = 1 - (D_o / D_e)

  where:
  - D_o = observed disagreement
  - D_e = expected disagreement by chance
  """

  @doc """
  Computes Krippendorff's alpha.

  ## Options

    * `:field` - Field name to compute agreement for (default: first field in values)
    * `:metric` - Distance metric (:nominal, :ordinal, :interval, :ratio) (default: :nominal)

  """
  @spec compute([Anvil.Label.t()], keyword()) :: {:ok, float()} | {:error, term()}
  def compute(labels, opts \\ []) do
    field = Keyword.get(opts, :field)
    metric = Keyword.get(opts, :metric, :nominal)

    # Group labels by sample
    grouped =
      labels
      |> Enum.group_by(& &1.sample_id)

    if Enum.empty?(grouped) do
      {:error, :no_labels}
    else
      # Build reliability matrix (samples × raters)
      # Missing values are represented as nil
      {matrix, all_values} = build_reliability_matrix(grouped, field)

      alpha = calculate_alpha(matrix, all_values, metric)
      {:ok, alpha}
    end
  end

  defp build_reliability_matrix(grouped, field) do
    # Get all labelers
    labelers =
      grouped
      |> Enum.flat_map(fn {_, labels} -> Enum.map(labels, & &1.labeler_id) end)
      |> Enum.uniq()
      |> Enum.sort()

    # Build matrix
    matrix =
      grouped
      |> Enum.map(fn {_sample_id, sample_labels} ->
        labelers
        |> Enum.map(fn labeler ->
          label = Enum.find(sample_labels, &(&1.labeler_id == labeler))

          if label do
            extract_value(label, field)
          else
            nil
          end
        end)
      end)

    # Collect all non-nil values
    all_values =
      matrix
      |> List.flatten()
      |> Enum.reject(&is_nil/1)
      |> Enum.uniq()
      |> Enum.sort()

    {matrix, all_values}
  end

  defp calculate_alpha(matrix, all_values, metric) do
    # Calculate coincidence matrix
    coincidence = build_coincidence_matrix(matrix, all_values)

    # Observed disagreement
    d_o = observed_disagreement(coincidence, metric, all_values)

    # Expected disagreement
    d_e = expected_disagreement(coincidence, metric, all_values)

    # Krippendorff's alpha
    if d_e == 0.0 do
      1.0
    else
      1.0 - d_o / d_e
    end
  end

  defp build_coincidence_matrix(matrix, all_values) do
    # Initialize coincidence matrix
    n = length(all_values)
    coincidence = for _ <- 1..n, do: List.duplicate(0, n)

    # For each sample (row in matrix)
    matrix
    |> Enum.reduce(coincidence, fn row, acc ->
      # Get non-nil values
      values = Enum.reject(row, &is_nil/1)
      m = length(values)

      if m < 2 do
        acc
      else
        # Count pairwise coincidences
        update_coincidence_matrix(acc, values, all_values, m)
      end
    end)
  end

  defp update_coincidence_matrix(matrix, values, all_values, m) do
    # For each pair of values
    for i <- 0..(length(all_values) - 1),
        j <- 0..(length(all_values) - 1) do
      val_i = Enum.at(all_values, i)
      val_j = Enum.at(all_values, j)

      count_i = Enum.count(values, &(&1 == val_i))
      count_j = Enum.count(values, &(&1 == val_j))

      if i == j do
        # Diagonal: count_i * (count_i - 1) / (m - 1)
        increment = count_i * (count_i - 1) / (m - 1)
        update_matrix_cell(matrix, i, j, increment)
      else
        # Off-diagonal: count_i * count_j / (m - 1)
        increment = count_i * count_j / (m - 1)
        update_matrix_cell(matrix, i, j, increment)
      end
    end

    matrix
  end

  defp update_matrix_cell(matrix, i, j, increment) do
    row = Enum.at(matrix, i)
    current = Enum.at(row, j)
    new_row = List.replace_at(row, j, current + increment)
    List.replace_at(matrix, i, new_row)
  end

  defp observed_disagreement(coincidence, :nominal, all_values) do
    n = length(all_values)

    # Sum off-diagonal elements
    for i <- 0..(n - 1), j <- 0..(n - 1), i != j, reduce: 0.0 do
      acc ->
        val = get_matrix_value(coincidence, i, j)
        acc + val
    end
  end

  defp observed_disagreement(coincidence, _metric, all_values) do
    # Simplified: for other metrics, use nominal distance
    # Full implementation would use different distance functions
    observed_disagreement(coincidence, :nominal, all_values)
  end

  defp expected_disagreement(coincidence, :nominal, all_values) do
    n = length(all_values)

    # Sum of marginals
    marginals =
      for i <- 0..(n - 1) do
        Enum.sum(Enum.at(coincidence, i))
      end

    total = Enum.sum(marginals)

    # Expected disagreement
    for i <- 0..(n - 1), j <- 0..(n - 1), i != j, reduce: 0.0 do
      acc ->
        m_i = Enum.at(marginals, i)
        m_j = Enum.at(marginals, j)
        acc + m_i * m_j / (total - 1)
    end
  end

  defp expected_disagreement(coincidence, _metric, all_values) do
    # Simplified: for other metrics, use nominal distance
    expected_disagreement(coincidence, :nominal, all_values)
  end

  defp get_matrix_value(matrix, i, j) do
    matrix |> Enum.at(i) |> Enum.at(j)
  end

  defp extract_value(label, nil) do
    # Use first field if not specified
    label.values |> Map.values() |> List.first()
  end

  defp extract_value(label, field) do
    Map.get(label.values, field)
  end
end
</file>

<file path="anvil/api/records/assignment_record.ex">
defmodule Anvil.API.AssignmentRecord do
  @moduledoc false
  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :string, autogenerate: false}

  schema "labeling_assignments" do
    field(:queue_id, :string)
    field(:schema_id, :string)
    field(:sample_id, :string)
    field(:tenant_id, :string)
    field(:namespace, :string)
    field(:expires_at, :utc_datetime)
    field(:metadata, :map, default: %{})

    timestamps(type: :utc_datetime)
  end

  def changeset(assignment, attrs) do
    assignment
    |> cast(attrs, [
      :id,
      :queue_id,
      :schema_id,
      :sample_id,
      :tenant_id,
      :namespace,
      :expires_at,
      :metadata
    ])
    |> validate_required([:id, :queue_id, :schema_id, :sample_id, :tenant_id])
  end
end
</file>

<file path="anvil/api/records/dataset_record.ex">
defmodule Anvil.API.DatasetRecord do
  @moduledoc false
  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :string, autogenerate: false}

  schema "labeling_datasets" do
    field(:tenant_id, :string)
    field(:namespace, :string)
    field(:version, :string)
    field(:slices, {:array, :map}, default: [])
    field(:source_refs, {:array, :map}, default: [])
    field(:metadata, :map, default: %{})
    field(:lineage_ref, :map)
    field(:created_at, :utc_datetime)

    timestamps(type: :utc_datetime)
  end

  def changeset(dataset, attrs) do
    dataset
    |> cast(attrs, [
      :id,
      :tenant_id,
      :namespace,
      :version,
      :slices,
      :source_refs,
      :metadata,
      :lineage_ref,
      :created_at
    ])
    |> validate_required([:id, :tenant_id, :version, :created_at])
  end
end
</file>

<file path="anvil/api/records/label_record.ex">
defmodule Anvil.API.LabelRecord do
  @moduledoc false
  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :string, autogenerate: false}

  schema "labeling_labels" do
    field(:assignment_id, :string)
    field(:queue_id, :string)
    field(:sample_id, :string)
    field(:tenant_id, :string)
    field(:namespace, :string)
    field(:user_id, :string)
    field(:values, :map, default: %{})
    field(:notes, :string)
    field(:time_spent_ms, :integer)
    field(:lineage_ref, :map)
    field(:metadata, :map, default: %{})
    field(:created_at, :utc_datetime)

    timestamps(type: :utc_datetime)
  end

  def changeset(label, attrs) do
    label
    |> cast(attrs, [
      :id,
      :assignment_id,
      :queue_id,
      :sample_id,
      :tenant_id,
      :namespace,
      :user_id,
      :values,
      :notes,
      :time_spent_ms,
      :lineage_ref,
      :metadata,
      :created_at
    ])
    |> validate_required([
      :id,
      :assignment_id,
      :queue_id,
      :sample_id,
      :tenant_id,
      :user_id,
      :values,
      :time_spent_ms,
      :created_at
    ])
  end
end
</file>

<file path="anvil/api/records/queue_record.ex">
defmodule Anvil.API.QueueRecord do
  @moduledoc false
  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :string, autogenerate: false}

  schema "labeling_queues" do
    field(:tenant_id, :string)
    field(:schema_id, :string)
    field(:namespace, :string)
    field(:component_module, :string)
    field(:metadata, :map, default: %{})

    timestamps(type: :utc_datetime)
  end

  def changeset(queue, attrs) do
    queue
    |> cast(attrs, [:id, :tenant_id, :schema_id, :namespace, :component_module, :metadata])
    |> validate_required([:id, :tenant_id, :schema_id, :component_module])
  end
end
</file>

<file path="anvil/api/records/sample_record.ex">
defmodule Anvil.API.SampleRecord do
  @moduledoc false
  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :string, autogenerate: false}

  schema "labeling_samples" do
    field(:tenant_id, :string)
    field(:namespace, :string)
    field(:pipeline_id, :string)
    field(:payload, :map, default: %{})
    field(:artifacts, {:array, :map}, default: [])
    field(:metadata, :map, default: %{})
    field(:lineage_ref, :map)
    field(:created_at, :utc_datetime)

    timestamps(type: :utc_datetime)
  end

  def changeset(sample, attrs) do
    sample
    |> cast(attrs, [
      :id,
      :tenant_id,
      :namespace,
      :pipeline_id,
      :payload,
      :artifacts,
      :metadata,
      :lineage_ref,
      :created_at
    ])
    |> validate_required([:id, :tenant_id, :pipeline_id, :payload, :created_at])
  end
end
</file>

<file path="anvil/api/records/schema_record.ex">
defmodule Anvil.API.SchemaRecord do
  @moduledoc false
  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :string, autogenerate: false}

  schema "labeling_schemas" do
    field(:tenant_id, :string)
    field(:namespace, :string)
    field(:fields, {:array, :map}, default: [])
    field(:layout, :map)
    field(:component_module, :string)
    field(:metadata, :map, default: %{})

    timestamps(type: :utc_datetime)
  end

  def changeset(schema, attrs) do
    schema
    |> cast(attrs, [:id, :tenant_id, :namespace, :fields, :layout, :component_module, :metadata])
    |> validate_required([:id, :tenant_id, :fields])
  end
end
</file>

<file path="anvil/api/router.ex">
defmodule Anvil.API.Router do
  @moduledoc """
  Plug router exposing `/v1` IR endpoints for assignments, labels,
  schemas, queues, samples, and datasets.
  """

  use Plug.Router

  alias Anvil.API.State
  alias Anvil.Auth.{Role, TenantContext}
  alias LabelingIR.{Dataset, Label, Sample, Schema}

  plug(:match)

  plug(Plug.Parsers,
    parsers: [:json],
    json_decoder: Jason,
    pass: ["application/json"]
  )

  plug(:dispatch)

  post "/v1/schemas" do
    with {:ok, tenant} <- require_tenant(conn),
         actor <- current_actor(conn, tenant),
         :ok <- authorize(actor, :manage_queue),
         {:ok, schema} <- decode_schema(conn.body_params, tenant),
         :ok <- TenantContext.ensure_tenant_isolation(tenant_ref(schema), tenant_ref(actor)),
         :ok <- State.put_schema(schema) do
      send_json(conn, 201, schema)
    else
      {:error, reason} -> send_error(conn, reason)
    end
  end

  get "/v1/schemas/:id" do
    tenant = tenant_header(conn)

    case State.get_schema(id, tenant) do
      {:ok, schema} -> send_json(conn, 200, schema)
      :error -> send_resp(conn, 404, "")
    end
  end

  post "/v1/queues" do
    with {:ok, tenant} <- require_tenant(conn),
         actor <- current_actor(conn, tenant),
         :ok <- authorize(actor, :manage_queue),
         %{"id" => id, "schema_id" => schema_id} = body <- conn.body_params,
         {:ok, schema} <- State.get_schema(schema_id, tenant),
         {:ok, component_module, metadata} <- queue_component(body, schema),
         queue = %{
           id: id,
           tenant_id: tenant,
           schema_id: schema_id,
           namespace: schema.namespace,
           component_module: component_module,
           metadata: metadata
         },
         :ok <- State.put_queue(queue) do
      send_json(conn, 201, queue)
    else
      {:error, :component_module_required} ->
        send_resp(conn, 422, Jason.encode!(%{error: "component_module_required"}))

      {:error, reason} ->
        send_error(conn, reason)

      _ ->
        send_error(conn, :invalid_payload)
    end
  end

  get "/v1/queues/:id" do
    tenant = tenant_header(conn)

    case State.get_queue(id, tenant) do
      {:ok, queue} ->
        stats =
          case tenant do
            nil ->
              %{}

            _ ->
              case State.queue_stats(id, tenant) do
                %{} = stat_map -> stat_map
                _ -> %{}
              end
          end

        send_json(conn, 200, Map.put(queue, :stats, stats))

      :error ->
        send_resp(conn, 404, "")
    end
  end

  post "/v1/samples" do
    with {:ok, tenant} <- require_tenant(conn),
         actor <- current_actor(conn, tenant),
         :ok <- authorize(actor, :request_assignment),
         {:ok, sample} <- decode_sample(conn.body_params, tenant),
         :ok <- TenantContext.ensure_tenant_isolation(tenant_ref(sample), tenant_ref(actor)),
         :ok <- State.put_sample(sample) do
      send_json(conn, 201, sample)
    else
      {:error, reason} -> send_error(conn, reason)
    end
  end

  get "/v1/samples/:id" do
    tenant = tenant_header(conn)

    case State.get_sample(id, tenant) do
      {:ok, sample} -> send_json(conn, 200, sample)
      :error -> send_resp(conn, 404, "")
    end
  end

  get "/v1/queues/:queue_id/assignments/next" do
    conn = fetch_query_params(conn)

    with {:ok, tenant} <- require_tenant(conn),
         actor <- current_actor(conn, tenant),
         :ok <- authorize(actor, :request_assignment),
         {:ok, assignment} <-
           State.next_assignment(queue_id, tenant, conn.params["user_id"] || "") do
      send_json(conn, 200, assignment)
    else
      {:error, :not_found} -> send_resp(conn, 404, "")
      {:error, :no_samples} -> send_resp(conn, 404, Jason.encode!(%{error: "no_samples"}))
      {:error, reason} -> send_error(conn, reason)
    end
  end

  post "/v1/labels" do
    with {:ok, tenant} <- require_tenant(conn),
         actor <- current_actor(conn, tenant),
         :ok <- authorize(actor, :submit_label),
         {:ok, label} <- decode_label(conn.body_params, tenant, actor),
         :ok <- TenantContext.ensure_tenant_isolation(tenant_ref(label), tenant_ref(actor)),
         {:ok, stored} <- State.put_label(label) do
      send_json(conn, 201, stored)
    else
      {:error, reason} -> send_error(conn, reason)
    end
  end

  get "/v1/datasets/:id" do
    tenant = tenant_header(conn)

    case State.get_dataset(id, tenant) do
      {:ok, dataset} -> send_json(conn, 200, dataset)
      :error -> send_resp(conn, 404, "")
    end
  end

  get "/v1/datasets/:id/slices/:name" do
    tenant = tenant_header(conn)

    case State.get_dataset_slice(id, name, tenant) do
      {:ok, slice} -> send_json(conn, 200, slice)
      :error -> send_resp(conn, 404, "")
    end
  end

  match _ do
    send_resp(conn, 404, "")
  end

  ## Helpers

  defp require_tenant(conn) do
    case Plug.Conn.get_req_header(conn, "x-tenant-id") do
      [tenant | _] when is_binary(tenant) and tenant != "" -> {:ok, tenant}
      _ -> {:error, :tenant_required}
    end
  end

  defp tenant_header(conn) do
    case Plug.Conn.get_req_header(conn, "x-tenant-id") do
      [tenant | _] when is_binary(tenant) and tenant != "" -> tenant
      _ -> nil
    end
  end

  defp tenant_ref(resource) do
    %{tenant_id: TenantContext.extract_tenant_id(resource)}
  end

  defp current_actor(conn, tenant) do
    %{
      id: Plug.Conn.get_req_header(conn, "x-user-id") |> List.first() || "anonymous",
      tenant_id: tenant,
      role: parse_role(Plug.Conn.get_req_header(conn, "x-user-role") |> List.first())
    }
  end

  defp parse_role(nil), do: :admin

  defp parse_role(role) when is_binary(role) do
    role_atom =
      role
      |> String.trim()
      |> String.to_atom()

    if Role.valid?(role_atom), do: role_atom, else: Role.default()
  rescue
    _ -> Role.default()
  end

  defp authorize(actor, permission) do
    if actor.role == :admin or Role.has_permission?(actor.role, permission) do
      :ok
    else
      {:error, :forbidden}
    end
  end

  defp queue_component(params, schema) do
    metadata = Map.get(params, "metadata", %{}) || %{}

    component_module =
      Map.get(params, "component_module") ||
        Map.get(metadata, "component_module") ||
        Map.get(metadata, :component_module) ||
        schema.component_module

    cond do
      is_nil(component_module) ->
        {:error, :component_module_required}

      true ->
        normalized_metadata =
          metadata
          |> Map.put_new("component_module", component_module)
          |> Map.put_new(:component_module, component_module)

        {:ok, component_module, normalized_metadata}
    end
  end

  defp decode_schema(params, tenant) do
    fields =
      params
      |> Map.get("fields", [])
      |> Enum.map(fn field ->
        %Schema.Field{
          name: field["name"],
          type: normalize_type(field["type"]),
          required: field["required"] || false,
          min: field["min"],
          max: field["max"],
          default: field["default"],
          options: field["options"],
          help: field["help"]
        }
      end)

    {:ok,
     %Schema{
       id: params["id"] || Ecto.UUID.generate(),
       tenant_id: tenant,
       namespace: Map.get(params, "namespace"),
       fields: fields,
       layout: Map.get(params, "layout"),
       component_module: Map.get(params, "component_module"),
       metadata: Map.get(params, "metadata", %{})
     }}
  end

  defp decode_sample(params, tenant) do
    with {:ok, created_at} <- parse_datetime(params["created_at"]) do
      {:ok,
       %Sample{
         id: params["id"] || Ecto.UUID.generate(),
         tenant_id: tenant,
         namespace: Map.get(params, "namespace"),
         pipeline_id: params["pipeline_id"],
         payload: Map.get(params, "payload", %{}),
         artifacts: Map.get(params, "artifacts", []),
         metadata: Map.get(params, "metadata", %{}),
         lineage_ref: Map.get(params, "lineage_ref"),
         created_at: created_at
       }}
    end
  end

  defp decode_label(params, tenant, actor) do
    with {:ok, created_at} <- parse_datetime(params["created_at"] || DateTime.utc_now()) do
      user_id = params["user_id"] || actor.id

      {:ok,
       %Label{
         id: params["id"] || Ecto.UUID.generate(),
         assignment_id: params["assignment_id"],
         sample_id: params["sample_id"],
         queue_id: params["queue_id"],
         tenant_id: tenant,
         namespace: Map.get(params, "namespace"),
         user_id: user_id,
         values: Map.get(params, "values", %{}),
         notes: Map.get(params, "notes"),
         time_spent_ms: params["time_spent_ms"] || 0,
         created_at: created_at,
         lineage_ref: Map.get(params, "lineage_ref"),
         metadata: Map.get(params, "metadata", %{})
       }}
    end
  end

  defp normalize_type(nil), do: nil
  defp normalize_type(type) when is_atom(type), do: type

  defp normalize_type(type) when is_binary(type) do
    try do
      String.to_existing_atom(type)
    rescue
      ArgumentError -> String.to_atom(type)
    end
  end

  defp parse_datetime(%DateTime{} = dt), do: {:ok, dt}

  defp parse_datetime(nil), do: {:ok, DateTime.utc_now()}

  defp parse_datetime(value) when is_binary(value) do
    case DateTime.from_iso8601(value) do
      {:ok, dt, _offset} -> {:ok, dt}
      _ -> {:error, :invalid_datetime}
    end
  end

  defp parse_datetime(value) do
    {:ok, value}
  end

  defp send_json(conn, status, %Dataset{} = dataset) do
    send_json(conn, status, Map.from_struct(dataset))
  end

  defp send_json(conn, status, data) do
    body = Jason.encode!(data)

    conn
    |> Plug.Conn.put_resp_content_type("application/json")
    |> send_resp(status, body)
  end

  defp send_error(conn, :tenant_required),
    do: send_resp(conn, 422, Jason.encode!(%{error: "tenant_id_required"}))

  defp send_error(conn, :invalid_datetime),
    do: send_resp(conn, 422, Jason.encode!(%{error: "invalid_datetime"}))

  defp send_error(conn, :forbidden),
    do: send_resp(conn, 403, Jason.encode!(%{error: "forbidden"}))

  defp send_error(conn, _reason),
    do: send_resp(conn, 422, Jason.encode!(%{error: "invalid_payload"}))
end
</file>

<file path="anvil/api/server.ex">
defmodule Anvil.API.Server do
  @moduledoc """
  Boots the Anvil `/v1` Plug router via Plug.Cowboy.

  Controlled by `:anvil, :api_server` config:

      config :anvil, :api_server, enabled: true, port: 4101

  In test, this is disabled to avoid port binding.
  """

  use Supervisor

  def start_link(_opts) do
    Supervisor.start_link(__MODULE__, [], name: __MODULE__)
  end

  @impl true
  def init(_) do
    config = Application.get_env(:anvil, :api_server, [])

    case Keyword.get(config, :enabled, false) do
      true ->
        port = Keyword.get(config, :port, 4101)
        cowboy = {Plug.Cowboy, scheme: :http, plug: Anvil.API.Router, options: [port: port]}
        Supervisor.init([cowboy], strategy: :one_for_one)

      _ ->
        Supervisor.init([], strategy: :one_for_one)
    end
  end
end
</file>

<file path="anvil/api/state.ex">
defmodule Anvil.API.State do
  @moduledoc """
  Repo-backed persistence for the `/v1` IR API.

  Uses dedicated tables to store the LabelingIR structs while enforcing
  tenant scoping.
  """

  alias Anvil.API.{
    AssignmentRecord,
    DatasetRecord,
    LabelRecord,
    QueueRecord,
    SampleRecord,
    SchemaRecord
  }

  alias Anvil.Repo
  alias LabelingIR.{Assignment, Dataset, Label, Sample, Schema}

  import Ecto.Query

  @spec reset!() :: :ok
  def reset! do
    Repo.transaction(fn ->
      Repo.delete_all(LabelRecord)
      Repo.delete_all(AssignmentRecord)
      Repo.delete_all(SampleRecord)
      Repo.delete_all(DatasetRecord)
      Repo.delete_all(QueueRecord)
      Repo.delete_all(SchemaRecord)
    end)

    :ok
  end

  @spec put_schema(Schema.t()) :: :ok | {:error, term()}
  def put_schema(%Schema{} = schema) do
    attrs = %{
      id: schema.id,
      tenant_id: schema.tenant_id,
      namespace: schema.namespace,
      fields: encode_fields(schema.fields),
      layout: schema.layout,
      component_module: schema.component_module,
      metadata: schema.metadata || %{}
    }

    %SchemaRecord{}
    |> SchemaRecord.changeset(attrs)
    |> upsert()
  end

  @spec get_schema(String.t(), String.t() | nil) :: {:ok, Schema.t()} | :error
  def get_schema(id, tenant_id \\ nil) do
    case Repo.get(SchemaRecord, id) do
      %SchemaRecord{tenant_id: ^tenant_id} = record when not is_nil(tenant_id) ->
        {:ok, decode_schema(record)}

      %SchemaRecord{} = record when is_nil(tenant_id) ->
        {:ok, decode_schema(record)}

      %SchemaRecord{} ->
        :error

      _ ->
        :error
    end
  end

  @spec put_queue(map()) :: :ok | {:error, term()}
  def put_queue(queue) do
    metadata =
      queue.metadata
      |> Map.put_new("component_module", queue.component_module)
      |> Map.put_new(:component_module, queue.component_module)

    attrs = %{
      id: queue.id,
      tenant_id: queue.tenant_id,
      schema_id: queue.schema_id,
      namespace: queue.namespace,
      component_module: queue.component_module,
      metadata: metadata
    }

    %QueueRecord{}
    |> QueueRecord.changeset(attrs)
    |> upsert()
  end

  @spec get_queue(String.t(), String.t() | nil) :: {:ok, map()} | :error
  def get_queue(id, tenant_id \\ nil) do
    case Repo.get(QueueRecord, id) do
      %QueueRecord{tenant_id: ^tenant_id} = record when not is_nil(tenant_id) ->
        {:ok, decode_queue(record)}

      %QueueRecord{} = record when is_nil(tenant_id) ->
        {:ok, decode_queue(record)}

      %QueueRecord{} ->
        :error

      _ ->
        :error
    end
  end

  @spec put_sample(Sample.t()) :: :ok | {:error, term()}
  def put_sample(%Sample{} = sample) do
    attrs = %{
      id: sample.id,
      tenant_id: sample.tenant_id,
      namespace: sample.namespace,
      pipeline_id: sample.pipeline_id,
      payload: sample.payload,
      artifacts: sample.artifacts || [],
      metadata: sample.metadata || %{},
      lineage_ref: sample.lineage_ref,
      created_at: sample.created_at |> normalize_datetime()
    }

    %SampleRecord{}
    |> SampleRecord.changeset(attrs)
    |> upsert()
  end

  @spec get_sample(String.t(), String.t() | nil) :: {:ok, Sample.t()} | :error
  def get_sample(id, tenant_id \\ nil) do
    case Repo.get(SampleRecord, id) do
      %SampleRecord{tenant_id: ^tenant_id} = record when not is_nil(tenant_id) ->
        {:ok, decode_sample(record)}

      %SampleRecord{} = record when is_nil(tenant_id) ->
        {:ok, decode_sample(record)}

      %SampleRecord{} ->
        :error

      _ ->
        :error
    end
  end

  @spec put_assignment(Assignment.t()) :: :ok | {:error, term()}
  def put_assignment(%Assignment{} = assignment) do
    :ok = put_schema(assignment.schema)
    :ok = put_sample(assignment.sample)
    :ok = ensure_queue_for_assignment(assignment)

    attrs = %{
      id: assignment.id,
      queue_id: assignment.queue_id,
      schema_id: assignment.schema.id,
      sample_id: assignment.sample.id,
      tenant_id: assignment.tenant_id,
      namespace: assignment.namespace || assignment.sample.namespace,
      expires_at: assignment.expires_at && normalize_datetime(assignment.expires_at),
      metadata: assignment.metadata || %{}
    }

    %AssignmentRecord{}
    |> AssignmentRecord.changeset(attrs)
    |> upsert()
  end

  @spec next_assignment(String.t(), String.t(), String.t()) ::
          {:ok, Assignment.t()} | {:error, :not_found | :no_samples}
  def next_assignment(queue_id, tenant_id, _user_id) do
    with {:ok, queue} <- get_queue(queue_id, tenant_id),
         {:ok, schema} <- get_schema(queue.schema_id, tenant_id),
         {:ok, sample} <- pick_sample(queue, tenant_id) do
      component_module =
        queue.component_module ||
          schema.component_module ||
          get_in(queue.metadata, ["component_module"]) ||
          get_in(queue.metadata, [:component_module])

      metadata =
        queue.metadata
        |> Map.put_new("component_module", component_module)
        |> Map.put_new(:component_module, component_module)

      assignment = %Assignment{
        id: Ecto.UUID.generate(),
        queue_id: queue_id,
        tenant_id: tenant_id,
        namespace: sample.namespace || schema.namespace,
        sample: sample,
        schema: schema,
        existing_labels: [],
        metadata: metadata
      }

      :ok = put_assignment(assignment)
      {:ok, assignment}
    else
      {:error, reason} -> {:error, reason}
      :error -> {:error, :not_found}
    end
  end

  @spec put_label(Label.t()) :: {:ok, Label.t()} | {:error, term()}
  def put_label(%Label{} = label) do
    with {:ok, assignment} <-
           Repo.get(AssignmentRecord, label.assignment_id) |> ensure_tenant(label.tenant_id),
         {:ok, _} <- get_queue(label.queue_id, label.tenant_id),
         {:ok, _} <- get_sample(label.sample_id, label.tenant_id) do
      attrs = %{
        id: label.id || Ecto.UUID.generate(),
        assignment_id: assignment.id,
        queue_id: label.queue_id,
        sample_id: label.sample_id,
        tenant_id: label.tenant_id,
        namespace: label.namespace,
        user_id: label.user_id,
        values: label.values || %{},
        notes: label.notes,
        time_spent_ms: label.time_spent_ms || 0,
        lineage_ref: label.lineage_ref,
        metadata: label.metadata || %{},
        created_at: normalize_datetime(label.created_at)
      }

      %LabelRecord{}
      |> LabelRecord.changeset(attrs)
      |> upsert()
      |> case do
        :ok -> {:ok, %{label | id: attrs.id}}
        {:error, reason} -> {:error, reason}
      end
    else
      {:error, reason} -> {:error, reason}
      _ -> {:error, :not_found}
    end
  end

  @spec put_dataset(Dataset.t()) :: :ok | {:error, term()}
  def put_dataset(%Dataset{} = dataset) do
    attrs = %{
      id: dataset.id,
      tenant_id: dataset.tenant_id,
      namespace: dataset.namespace,
      version: dataset.version,
      slices: dataset.slices || [],
      source_refs: dataset.source_refs || [],
      metadata: dataset.metadata || %{},
      lineage_ref: dataset.lineage_ref,
      created_at: normalize_datetime(dataset.created_at)
    }

    %DatasetRecord{}
    |> DatasetRecord.changeset(attrs)
    |> upsert()
  end

  @spec get_dataset(String.t(), String.t() | nil) :: {:ok, Dataset.t()} | :error
  def get_dataset(id, tenant_id \\ nil) do
    case Repo.get(DatasetRecord, id) do
      %DatasetRecord{tenant_id: ^tenant_id} = record when not is_nil(tenant_id) ->
        {:ok, decode_dataset(record)}

      %DatasetRecord{} = record when is_nil(tenant_id) ->
        {:ok, decode_dataset(record)}

      %DatasetRecord{} ->
        :error

      _ ->
        :error
    end
  end

  @spec get_dataset_slice(String.t(), String.t(), String.t() | nil) :: {:ok, map()} | :error
  def get_dataset_slice(id, slice_name, tenant_id \\ nil) do
    with {:ok, dataset} <- get_dataset(id, tenant_id),
         %{} = slice <- Enum.find(dataset.slices, &match_slice?(&1, slice_name)) do
      {:ok, slice}
    else
      _ -> :error
    end
  end

  @spec queue_stats(String.t(), String.t()) :: map() | :error
  def queue_stats(queue_id, tenant_id) do
    with {:ok, _queue} <- get_queue(queue_id, tenant_id) do
      total_assignments =
        from(a in AssignmentRecord, where: a.queue_id == ^queue_id and a.tenant_id == ^tenant_id)
        |> Repo.aggregate(:count, :id)

      total_labels =
        from(l in LabelRecord, where: l.queue_id == ^queue_id and l.tenant_id == ^tenant_id)
        |> Repo.aggregate(:count, :id)

      %{
        total_assignments: total_assignments,
        labeled: total_labels,
        remaining: max(total_assignments - total_labels, 0)
      }
    else
      _ -> :error
    end
  end

  ## Helpers

  defp pick_sample(queue, tenant_id) do
    query =
      from(s in SampleRecord,
        where: s.tenant_id == ^tenant_id,
        order_by: [asc: s.inserted_at],
        limit: 1
      )

    query =
      if queue.namespace do
        from(s in query, where: is_nil(s.namespace) or s.namespace == ^queue.namespace)
      else
        query
      end

    case Repo.one(query) do
      nil -> {:error, :no_samples}
      %SampleRecord{} = record -> {:ok, decode_sample(record)}
    end
  end

  defp decode_schema(%SchemaRecord{} = record) do
    %Schema{
      id: record.id,
      tenant_id: record.tenant_id,
      namespace: record.namespace,
      fields: decode_fields(record.fields || []),
      layout: record.layout,
      component_module: record.component_module,
      metadata: record.metadata || %{}
    }
  end

  defp decode_queue(%QueueRecord{} = record) do
    %{
      id: record.id,
      tenant_id: record.tenant_id,
      schema_id: record.schema_id,
      namespace: record.namespace,
      component_module: record.component_module,
      metadata: record.metadata || %{}
    }
  end

  defp decode_sample(%SampleRecord{} = record) do
    %Sample{
      id: record.id,
      tenant_id: record.tenant_id,
      namespace: record.namespace,
      pipeline_id: record.pipeline_id,
      payload: record.payload || %{},
      artifacts: record.artifacts || [],
      metadata: record.metadata || %{},
      lineage_ref: record.lineage_ref,
      created_at: record.created_at
    }
  end

  defp decode_dataset(%DatasetRecord{} = record) do
    %Dataset{
      id: record.id,
      tenant_id: record.tenant_id,
      namespace: record.namespace,
      version: record.version,
      slices: record.slices || [],
      source_refs: record.source_refs || [],
      metadata: record.metadata || %{},
      lineage_ref: record.lineage_ref,
      created_at: record.created_at
    }
  end

  defp encode_fields(fields) when is_list(fields) do
    Enum.map(fields, &Map.from_struct/1)
  end

  defp decode_fields(fields) do
    Enum.map(fields, fn field ->
      attrs =
        field
        |> normalize_keys()
        |> Map.take([:name, :type, :required, :min, :max, :default, :options, :help])

      struct(LabelingIR.Schema.Field, attrs)
    end)
  end

  defp normalize_keys(map) when is_map(map) do
    Map.new(map, fn {k, v} -> {normalize_key(k), v} end)
  end

  defp normalize_key(k) when is_atom(k), do: k
  defp normalize_key(k) when is_binary(k), do: String.to_atom(k)

  defp upsert(changeset) do
    case Repo.insert(changeset,
           on_conflict: {:replace_all_except, [:id, :inserted_at]},
           conflict_target: :id
         ) do
      {:ok, _record} -> :ok
      {:error, reason} -> {:error, reason}
    end
  end

  defp normalize_datetime(%DateTime{} = dt), do: DateTime.truncate(dt, :second)
  defp normalize_datetime(%NaiveDateTime{} = ndt), do: ndt
  defp normalize_datetime(nil), do: nil
  defp normalize_datetime(other), do: other

  defp match_slice?(slice, name) do
    Map.get(slice, :name) == name || Map.get(slice, "name") == name
  end

  defp ensure_tenant(%{tenant_id: tenant_id} = resource, tenant_id), do: {:ok, resource}
  defp ensure_tenant(_, _), do: {:error, :tenant_mismatch}

  defp ensure_queue_for_assignment(%Assignment{} = assignment) do
    case get_queue(assignment.queue_id, assignment.tenant_id) do
      {:ok, _queue} ->
        :ok

      :error ->
        component_module =
          get_in(assignment.metadata, ["component_module"]) ||
            get_in(assignment.metadata, [:component_module]) ||
            assignment.schema.component_module ||
            "Ingot.Components.DefaultComponent"

        attrs = %{
          id: assignment.queue_id,
          tenant_id: assignment.tenant_id,
          schema_id: assignment.schema.id,
          namespace: assignment.namespace || assignment.sample.namespace,
          component_module: component_module,
          metadata: %{"component_module" => component_module}
        }

        put_queue(attrs)
    end
  end
end
</file>

<file path="anvil/auth/oidc/mock.ex">
defmodule Anvil.Auth.OIDC.Mock do
  @moduledoc """
  Mock OIDC adapter for testing.

  Accepts specific token patterns for testing various scenarios:
  - "valid-token-*" -> Returns successful authentication
  - "expired-token" -> Returns expired error
  - "invalid-token" -> Returns invalid error
  - Anything else -> Returns invalid error
  """

  @behaviour Anvil.Auth.OIDC

  alias Anvil.Auth.OIDC
  alias Anvil.Auth.Role

  @impl true
  def authenticate(token, opts \\ []) do
    case verify_token(token) do
      {:ok, claims} ->
        labeler = build_labeler(claims, opts)
        {:ok, labeler}

      error ->
        error
    end
  end

  @impl true
  def verify_token("expired-token") do
    {:error, :expired_token}
  end

  def verify_token("invalid-token") do
    {:error, :invalid_token}
  end

  def verify_token("valid-token-" <> _rest = token) do
    claims = %{
      "sub" => extract_user_id(token),
      "email" => "test@example.com",
      "exp" => DateTime.utc_now() |> DateTime.add(3600, :second) |> DateTime.to_unix(),
      "iss" => "https://mock-idp.example.com",
      "tenant_id" => "default-tenant",
      "name" => "Test User"
    }

    {:ok, claims}
  end

  def verify_token(_token) do
    {:error, :invalid_token}
  end

  # Private helpers

  defp build_labeler(claims, opts) do
    %OIDC.Labeler{
      external_id: claims["sub"],
      email: claims["email"],
      tenant_id: opts[:tenant_id] || claims["tenant_id"],
      role: opts[:role] || Role.default(),
      status: :active
    }
  end

  defp extract_user_id("valid-token-" <> rest) do
    if rest == "" do
      "user-#{:erlang.unique_integer([:positive])}"
    else
      "user-#{rest}"
    end
  end
end
</file>

<file path="anvil/auth/acl.ex">
defmodule Anvil.Auth.ACL do
  @moduledoc """
  Access Control List (ACL) management for queue memberships.

  Implements queue-level access control with roles:
  - `:labeler` - Can request assignments and submit labels
  - `:reviewer` - Can view all labels, audit, and export data
  - `:owner` - Can manage queue membership and settings

  Supports time-limited access with expiration and revocation.
  """

  defmodule QueueMembership do
    @moduledoc """
    Represents a labeler's membership in a queue with specific role and permissions.
    """

    @type role :: :labeler | :reviewer | :owner

    @type t :: %__MODULE__{
            queue_id: String.t(),
            labeler_id: String.t(),
            role: role(),
            tenant_id: String.t(),
            granted_by: String.t() | nil,
            granted_at: DateTime.t() | nil,
            expires_at: DateTime.t() | nil,
            revoked_at: DateTime.t() | nil
          }

    @enforce_keys [:queue_id, :labeler_id, :role, :tenant_id]
    defstruct [
      :queue_id,
      :labeler_id,
      :role,
      :tenant_id,
      :granted_by,
      :granted_at,
      :expires_at,
      :revoked_at
    ]
  end

  alias __MODULE__.QueueMembership

  @type labeler :: %{id: String.t(), tenant_id: String.t()}
  @type queue :: %{id: String.t(), tenant_id: String.t()}
  @type error_reason ::
          :not_member
          | :membership_revoked
          | :membership_expired
          | :insufficient_permissions
          | :tenant_mismatch

  @doc """
  Checks if a labeler can label in a queue.

  Requires active membership with `:labeler`, `:reviewer`, or `:owner` role.

  ## Examples

      iex> membership = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :labeler,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> labeler = %{id: "l1", tenant_id: "t1"}
      iex> queue = %{id: "q1", tenant_id: "t1"}
      iex> Anvil.Auth.ACL.can_label?(labeler, queue, [membership])
      :ok
  """
  @spec can_label?(labeler(), queue(), [QueueMembership.t()]) :: :ok | {:error, error_reason()}
  def can_label?(labeler, queue, memberships) do
    check_membership(labeler, queue, memberships, [:labeler, :reviewer, :owner])
  end

  @doc """
  Checks if a labeler can audit/view all labels in a queue.

  Requires active membership with `:reviewer` or `:owner` role.

  ## Examples

      iex> membership = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :reviewer,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> labeler = %{id: "l1", tenant_id: "t1"}
      iex> queue = %{id: "q1", tenant_id: "t1"}
      iex> Anvil.Auth.ACL.can_audit?(labeler, queue, [membership])
      :ok
  """
  @spec can_audit?(labeler(), queue(), [QueueMembership.t()]) :: :ok | {:error, error_reason()}
  def can_audit?(labeler, queue, memberships) do
    check_membership(labeler, queue, memberships, [:reviewer, :owner])
  end

  @doc """
  Checks if a labeler can export data from a queue.

  Requires active membership with `:reviewer` or `:owner` role.

  ## Examples

      iex> membership = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :owner,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> labeler = %{id: "l1", tenant_id: "t1"}
      iex> queue = %{id: "q1", tenant_id: "t1"}
      iex> Anvil.Auth.ACL.can_export?(labeler, queue, [membership])
      :ok
  """
  @spec can_export?(labeler(), queue(), [QueueMembership.t()]) :: :ok | {:error, error_reason()}
  def can_export?(labeler, queue, memberships) do
    check_membership(labeler, queue, memberships, [:reviewer, :owner])
  end

  @doc """
  Checks if a labeler can manage a queue (membership, settings).

  Requires active membership with `:owner` role.

  ## Examples

      iex> membership = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :owner,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> labeler = %{id: "l1", tenant_id: "t1"}
      iex> queue = %{id: "q1", tenant_id: "t1"}
      iex> Anvil.Auth.ACL.can_manage?(labeler, queue, [membership])
      :ok
  """
  @spec can_manage?(labeler(), queue(), [QueueMembership.t()]) :: :ok | {:error, error_reason()}
  def can_manage?(labeler, queue, memberships) do
    check_membership(labeler, queue, memberships, [:owner])
  end

  @doc """
  Grants queue access to a labeler.

  ## Examples

      iex> params = %{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :labeler,
      ...>   tenant_id: "t1",
      ...>   granted_by: "admin1"
      ...> }
      iex> {:ok, membership} = Anvil.Auth.ACL.grant_access(params)
      iex> membership.role
      :labeler
  """
  @spec grant_access(map()) :: {:ok, QueueMembership.t()} | {:error, term()}
  def grant_access(params) do
    with :ok <- validate_grant_params(params) do
      membership = %QueueMembership{
        queue_id: params.queue_id,
        labeler_id: params.labeler_id,
        role: params.role,
        tenant_id: params.tenant_id,
        granted_by: params[:granted_by],
        granted_at: DateTime.utc_now(),
        expires_at: params[:expires_at],
        revoked_at: nil
      }

      {:ok, membership}
    end
  end

  @doc """
  Revokes queue access for a labeler.

  ## Examples

      iex> membership = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :labeler,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> {:ok, revoked} = Anvil.Auth.ACL.revoke_access(membership)
      iex> revoked.revoked_at != nil
      true
  """
  @spec revoke_access(QueueMembership.t()) :: {:ok, QueueMembership.t()}
  def revoke_access(%QueueMembership{} = membership) do
    revoked = %{membership | revoked_at: DateTime.utc_now()}
    {:ok, revoked}
  end

  @doc """
  Checks if a membership is currently active.

  A membership is active if:
  - It has not been revoked
  - It has not expired

  ## Examples

      iex> membership = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :labeler,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> Anvil.Auth.ACL.active?(membership)
      true
  """
  @spec active?(QueueMembership.t()) :: boolean()
  def active?(%QueueMembership{revoked_at: revoked_at, expires_at: expires_at}) do
    not_revoked = is_nil(revoked_at)
    not_expired = is_nil(expires_at) or DateTime.compare(expires_at, DateTime.utc_now()) == :gt

    not_revoked and not_expired
  end

  @doc """
  Filters a list of memberships to only active ones.

  ## Examples

      iex> active = %QueueMembership{
      ...>   queue_id: "q1",
      ...>   labeler_id: "l1",
      ...>   role: :labeler,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: nil
      ...> }
      iex> revoked = %QueueMembership{
      ...>   queue_id: "q2",
      ...>   labeler_id: "l1",
      ...>   role: :labeler,
      ...>   tenant_id: "t1",
      ...>   expires_at: nil,
      ...>   revoked_at: DateTime.utc_now()
      ...> }
      iex> result = Anvil.Auth.ACL.filter_active([active, revoked])
      iex> length(result)
      1
  """
  @spec filter_active([QueueMembership.t()]) :: [QueueMembership.t()]
  def filter_active(memberships) do
    Enum.filter(memberships, &active?/1)
  end

  # Private helpers

  @spec check_membership(labeler(), queue(), [QueueMembership.t()], [QueueMembership.role()]) ::
          :ok | {:error, error_reason()}
  defp check_membership(labeler, queue, memberships, allowed_roles) do
    # Check tenant match first
    if labeler.tenant_id != queue.tenant_id do
      {:error, :tenant_mismatch}
    else
      # Find membership for this labeler and queue
      membership =
        Enum.find(memberships, fn m ->
          m.queue_id == queue.id and m.labeler_id == labeler.id
        end)

      case membership do
        nil ->
          {:error, :not_member}

        %QueueMembership{revoked_at: revoked_at} when not is_nil(revoked_at) ->
          {:error, :membership_revoked}

        %QueueMembership{expires_at: expires_at} when not is_nil(expires_at) ->
          if DateTime.compare(expires_at, DateTime.utc_now()) == :lt do
            {:error, :membership_expired}
          else
            check_role(membership.role, allowed_roles)
          end

        %QueueMembership{role: role} ->
          check_role(role, allowed_roles)
      end
    end
  end

  @spec check_role(QueueMembership.role(), [QueueMembership.role()]) ::
          :ok | {:error, :insufficient_permissions}
  defp check_role(role, allowed_roles) do
    if role in allowed_roles do
      :ok
    else
      {:error, :insufficient_permissions}
    end
  end

  @spec validate_grant_params(map()) :: :ok | {:error, term()}
  defp validate_grant_params(params) do
    required = [:queue_id, :labeler_id, :role, :tenant_id]

    missing = Enum.filter(required, fn key -> not Map.has_key?(params, key) end)

    if missing == [] do
      :ok
    else
      {:error, {:missing_required_fields, missing}}
    end
  end
end
</file>

<file path="anvil/auth/oidc.ex">
defmodule Anvil.Auth.OIDC do
  @moduledoc """
  OIDC (OpenID Connect) authentication behaviour for Anvil.

  Defines interface for authenticating labelers via OIDC tokens from
  identity providers like Auth0, Okta, Keycloak, Google, etc.

  Implementations should verify JWT signatures, check expiration,
  validate issuer, and extract standard OIDC claims.
  """

  defmodule Claims do
    @moduledoc """
    Standard OIDC claims extracted from ID token.
    """

    @type t :: %__MODULE__{
            sub: String.t(),
            email: String.t(),
            exp: integer(),
            iss: String.t(),
            tenant_id: String.t(),
            name: String.t() | nil,
            preferred_username: String.t() | nil
          }

    @enforce_keys [:sub, :email, :exp, :iss, :tenant_id]
    defstruct [
      :sub,
      :email,
      :exp,
      :iss,
      :tenant_id,
      :name,
      :preferred_username
    ]
  end

  defmodule Labeler do
    @moduledoc """
    Labeler identity created from OIDC authentication.
    """

    @type t :: %__MODULE__{
            external_id: String.t(),
            email: String.t(),
            tenant_id: String.t(),
            role: Anvil.Auth.Role.role(),
            status: :active | :suspended | :deactivated
          }

    @enforce_keys [:external_id, :email, :tenant_id, :role, :status]
    defstruct [
      :external_id,
      :email,
      :tenant_id,
      :role,
      :status
    ]
  end

  @type token :: String.t()
  @type opts :: keyword()
  @type error_reason :: :invalid_token | :expired_token | :invalid_issuer | :missing_claims

  @doc """
  Authenticates a labeler using an OIDC token.

  Returns labeler identity with external_id, email, tenant_id, and default role.
  Implements just-in-time provisioning - creates labeler on first login.

  ## Options
    - `:tenant_id` - Override tenant_id from token claims
    - `:role` - Override default role (:labeler)
  """
  @callback authenticate(token(), opts()) :: {:ok, Labeler.t()} | {:error, error_reason()}

  @doc """
  Verifies OIDC token and extracts claims.

  Should verify:
  - JWT signature using provider's public key
  - Token expiration (exp claim)
  - Issuer matches expected value (iss claim)
  - Audience matches client ID (aud claim)
  """
  @callback verify_token(token()) :: {:ok, map()} | {:error, error_reason()}
end
</file>

<file path="anvil/auth/role.ex">
defmodule Anvil.Auth.Role do
  @moduledoc """
  Role definitions and hierarchy for Anvil access control.

  Defines four core roles:
  - `:labeler` - Request assignments, submit labels, view own labels
  - `:auditor` - View all labels, export data, compute agreement metrics (read-only)
  - `:adjudicator` - Resolve label conflicts, override labels, approve/reject labels
  - `:admin` - Manage queue membership, update policies, create queues, manage labelers

  Roles have a hierarchical relationship where higher roles inherit permissions from lower roles
  in the context of override operations.
  """

  @type role :: :labeler | :auditor | :adjudicator | :admin
  @type permission ::
          :request_assignment
          | :submit_label
          | :view_own_labels
          | :view_all_labels
          | :export_data
          | :compute_agreement
          | :override_label
          | :resolve_conflicts
          | :manage_queue
          | :manage_labelers
          | :grant_access
          | :revoke_access

  @roles [:labeler, :auditor, :adjudicator, :admin]

  @hierarchy %{
    labeler: 1,
    auditor: 2,
    adjudicator: 3,
    admin: 4
  }

  @permissions %{
    labeler: [
      :request_assignment,
      :submit_label,
      :view_own_labels
    ],
    auditor: [
      :view_all_labels,
      :export_data,
      :compute_agreement
    ],
    adjudicator: [
      :override_label,
      :resolve_conflicts,
      :view_all_labels,
      :export_data
    ],
    admin: [
      :manage_queue,
      :manage_labelers,
      :grant_access,
      :revoke_access,
      :override_label,
      :export_data,
      :view_all_labels,
      :compute_agreement
    ]
  }

  @doc """
  Returns all valid roles.

  ## Examples

      iex> Anvil.Auth.Role.all()
      [:labeler, :auditor, :adjudicator, :admin]
  """
  @spec all() :: [role()]
  def all, do: @roles

  @doc """
  Checks if a given value is a valid role.

  ## Examples

      iex> Anvil.Auth.Role.valid?(:labeler)
      true

      iex> Anvil.Auth.Role.valid?(:invalid)
      false
  """
  @spec valid?(any()) :: boolean()
  def valid?(role), do: role in @roles

  @doc """
  Returns the role hierarchy mapping.

  Higher numbers indicate higher privilege levels.

  ## Examples

      iex> hierarchy = Anvil.Auth.Role.hierarchy()
      iex> hierarchy[:admin] > hierarchy[:labeler]
      true
  """
  @spec hierarchy() :: %{role() => pos_integer()}
  def hierarchy, do: @hierarchy

  @doc """
  Checks if one role can override another role's decisions.

  Roles can override themselves or lower-level roles based on the hierarchy.

  ## Examples

      iex> Anvil.Auth.Role.can_override?(:admin, :labeler)
      true

      iex> Anvil.Auth.Role.can_override?(:labeler, :admin)
      false

      iex> Anvil.Auth.Role.can_override?(:adjudicator, :adjudicator)
      true
  """
  @spec can_override?(role(), role()) :: boolean()
  def can_override?(role1, role2) do
    with level1 when not is_nil(level1) <- @hierarchy[role1],
         level2 when not is_nil(level2) <- @hierarchy[role2] do
      level1 >= level2
    else
      _ -> false
    end
  end

  @doc """
  Returns the list of permissions for a given role.

  ## Examples

      iex> perms = Anvil.Auth.Role.permissions(:labeler)
      iex> :request_assignment in perms
      true
  """
  @spec permissions(role()) :: [permission()]
  def permissions(role) when role in @roles do
    Map.get(@permissions, role, [])
  end

  def permissions(_), do: []

  @doc """
  Checks if a role has a specific permission.

  ## Examples

      iex> Anvil.Auth.Role.has_permission?(:admin, :manage_queue)
      true

      iex> Anvil.Auth.Role.has_permission?(:labeler, :manage_queue)
      false
  """
  @spec has_permission?(role(), permission()) :: boolean()
  def has_permission?(role, permission) when role in @roles do
    permission in permissions(role)
  end

  def has_permission?(_, _), do: false

  @doc """
  Returns the default role for new labelers.

  ## Examples

      iex> Anvil.Auth.Role.default()
      :labeler
  """
  @spec default() :: role()
  def default, do: :labeler
end
</file>

<file path="anvil/auth/signed_url.ex">
defmodule Anvil.Auth.SignedURL do
  @moduledoc """
  Time-limited signed URLs for secure asset access.

  Generates cryptographically signed URLs with expiration timestamps
  for accessing sensitive resources (samples, artifacts, etc).

  URLs contain:
  - Resource identifier
  - Expiration timestamp
  - HMAC signature (prevents tampering)

  ## Example

      # Generate signed URL (expires in 1 hour)
      {:ok, url} = SignedURL.generate("sample-123", secret, expires_in: 3600)

      # Verify URL before serving asset
      case SignedURL.verify(url, secret) do
        {:ok, resource_id} -> serve_asset(resource_id)
        {:error, :expired} -> {:error, :url_expired}
        {:error, :invalid_signature} -> {:error, :unauthorized}
      end
  """

  @type resource_id :: String.t()
  @type secret :: String.t()
  @type url :: String.t()
  @type opts :: keyword()
  @type error_reason :: :malformed_url | :invalid_signature | :expired

  @default_expires_in 3600

  @doc """
  Generates a signed URL for a resource.

  ## Options
    - `:expires_in` - Expiration time in seconds (default: 3600 / 1 hour)
    - `:tenant_id` - Include tenant ID in signature for multi-tenant isolation
    - `:base_url` - Base URL for the resource (default: "http://localhost/assets")

  ## Examples

      iex> {:ok, url} = SignedURL.generate("sample-123", "secret-key")
      iex> String.contains?(url, "sample-123")
      true

      iex> {:ok, url} = SignedURL.generate("sample-123", "secret", expires_in: 7200)
      iex> String.contains?(url, "signature=")
      true
  """
  @spec generate(resource_id(), secret(), opts()) :: {:ok, url()}
  def generate(resource_id, secret, opts \\ []) do
    expires_in = Keyword.get(opts, :expires_in, @default_expires_in)
    tenant_id = Keyword.get(opts, :tenant_id)
    base_url = Keyword.get(opts, :base_url, "http://localhost/assets")

    expires_at = DateTime.utc_now() |> DateTime.add(expires_in, :second) |> DateTime.to_unix()

    # Build signature payload
    payload = build_payload(resource_id, expires_at, tenant_id)
    signature = sign(payload, secret)

    # Build URL with query parameters
    url = "#{base_url}/#{resource_id}?expires=#{expires_at}&signature=#{signature}"

    {:ok, url}
  end

  @doc """
  Verifies a signed URL and returns the resource ID if valid.

  ## Options
    - `:tenant_id` - Expected tenant ID (must match signature)

  ## Examples

      iex> {:ok, url} = SignedURL.generate("sample-123", "secret")
      iex> SignedURL.verify(url, "secret")
      {:ok, "sample-123"}

      iex> SignedURL.verify("invalid-url", "secret")
      {:error, :malformed_url}
  """
  @spec verify(url(), secret(), opts()) :: {:ok, resource_id()} | {:error, error_reason()}
  def verify(url, secret, opts \\ []) do
    with {:ok, {resource_id, expires_at, signature}} <- parse_url(url),
         :ok <- check_expiration(expires_at),
         :ok <- verify_signature(resource_id, expires_at, signature, secret, opts) do
      {:ok, resource_id}
    end
  end

  @doc """
  Extracts the resource ID from a signed URL without verification.

  ## Examples

      iex> {:ok, url} = SignedURL.generate("sample-123", "secret")
      iex> SignedURL.extract_resource_id(url)
      {:ok, "sample-123"}
  """
  @spec extract_resource_id(url()) :: {:ok, resource_id()} | {:error, :malformed_url}
  def extract_resource_id(url) do
    case parse_url(url) do
      {:ok, {resource_id, _expires_at, _signature}} -> {:ok, resource_id}
      {:error, reason} -> {:error, reason}
    end
  end

  @doc """
  Checks if a signed URL has expired.

  ## Examples

      iex> {:ok, url} = SignedURL.generate("sample-123", "secret", expires_in: 3600)
      iex> SignedURL.expired?(url)
      false
  """
  @spec expired?(url()) :: boolean()
  def expired?(url) do
    case parse_url(url) do
      {:ok, {_resource_id, expires_at, _signature}} ->
        now = DateTime.utc_now() |> DateTime.to_unix()
        now >= expires_at

      {:error, _} ->
        false
    end
  end

  @doc """
  Returns time remaining before URL expires (in seconds).

  Returns negative value if already expired.

  ## Examples

      iex> {:ok, url} = SignedURL.generate("sample-123", "secret", expires_in: 3600)
      iex> {:ok, remaining} = SignedURL.time_remaining(url)
      iex> remaining > 3595
      true
  """
  @spec time_remaining(url()) :: {:ok, integer()} | {:error, :malformed_url}
  def time_remaining(url) do
    case parse_url(url) do
      {:ok, {_resource_id, expires_at, _signature}} ->
        now = DateTime.utc_now() |> DateTime.to_unix()
        {:ok, expires_at - now}

      {:error, reason} ->
        {:error, reason}
    end
  end

  # Private helpers

  @spec parse_url(url()) ::
          {:ok, {resource_id(), integer(), String.t()}} | {:error, :malformed_url}
  defp parse_url(url) do
    with {:ok, uri} <- parse_uri(url),
         {:ok, resource_id} <- extract_resource_from_path(uri.path),
         {:ok, query_params} <- parse_query(uri.query),
         {:ok, expires_at} <- get_expires(query_params),
         {:ok, signature} <- get_signature(query_params) do
      {:ok, {resource_id, expires_at, signature}}
    else
      _ -> {:error, :malformed_url}
    end
  end

  defp parse_uri(url) when is_binary(url) do
    case URI.parse(url) do
      %URI{path: path, query: query} when not is_nil(path) and not is_nil(query) ->
        {:ok, %{path: path, query: query}}

      _ ->
        {:error, :malformed_url}
    end
  end

  defp parse_uri(_), do: {:error, :malformed_url}

  defp extract_resource_from_path(path) do
    case String.split(path, "/") |> List.last() do
      nil -> {:error, :malformed_url}
      "" -> {:error, :malformed_url}
      resource_id -> {:ok, resource_id}
    end
  end

  defp parse_query(query) when is_binary(query) do
    {:ok, URI.decode_query(query)}
  end

  defp get_expires(params) do
    case Map.get(params, "expires") do
      nil -> {:error, :malformed_url}
      expires_str -> parse_integer(expires_str)
    end
  end

  defp get_signature(params) do
    case Map.get(params, "signature") do
      nil -> {:error, :malformed_url}
      signature -> {:ok, signature}
    end
  end

  defp parse_integer(str) do
    case Integer.parse(str) do
      {int, ""} -> {:ok, int}
      _ -> {:error, :malformed_url}
    end
  end

  defp check_expiration(expires_at) do
    now = DateTime.utc_now() |> DateTime.to_unix()

    if now < expires_at do
      :ok
    else
      {:error, :expired}
    end
  end

  defp verify_signature(resource_id, expires_at, signature, secret, opts) do
    tenant_id = Keyword.get(opts, :tenant_id)
    payload = build_payload(resource_id, expires_at, tenant_id)
    expected_signature = sign(payload, secret)

    if secure_compare(signature, expected_signature) do
      :ok
    else
      {:error, :invalid_signature}
    end
  end

  defp build_payload(resource_id, expires_at, tenant_id) do
    base = "#{resource_id}:#{expires_at}"

    if tenant_id do
      "#{base}:#{tenant_id}"
    else
      base
    end
  end

  defp sign(payload, secret) do
    :crypto.mac(:hmac, :sha256, secret, payload)
    |> Base.encode16(case: :lower)
  end

  # Constant-time string comparison to prevent timing attacks
  defp secure_compare(a, b) when is_binary(a) and is_binary(b) do
    if byte_size(a) == byte_size(b) do
      secure_compare(a, b, 0) == 0
    else
      false
    end
  end

  defp secure_compare(<<a, rest_a::binary>>, <<b, rest_b::binary>>, acc) do
    import Bitwise
    secure_compare(rest_a, rest_b, acc ||| bxor(a, b))
  end

  defp secure_compare(<<>>, <<>>, acc), do: acc
end
</file>

<file path="anvil/auth/tenant_context.ex">
defmodule Anvil.Auth.TenantContext do
  @moduledoc """
  Multi-tenant isolation utilities for Anvil.

  Provides helper functions for enforcing tenant boundaries across
  all operations. Prevents cross-tenant data access by validating
  tenant_id matches between resources and actors.

  ## Tenant Isolation Principles

  1. **Default Deny**: All cross-tenant access is forbidden by default
  2. **Explicit Validation**: Every operation must validate tenant context
  3. **Filter at Source**: Apply tenant filters as early as possible
  4. **Audit Trail**: Log all tenant boundary violations

  ## Usage

  In context modules:

      def get_queue(id, labeler) do
        queue = Repo.get!(Queue, id)

        with :ok <- TenantContext.validate_tenant(queue, labeler) do
          {:ok, queue}
        end
      end

  In queries:

      def list_queues(labeler) do
        from(q in Queue)
        |> where([q], q.tenant_id == ^labeler.tenant_id)
        |> Repo.all()
      end
  """

  @type tenant_id :: String.t()
  @type tenant_scopeable :: %{required(:tenant_id) => tenant_id(), optional(atom()) => term()}
  @type tenant_resource :: tenant_scopeable()
  @type actor :: tenant_scopeable()
  @type error_reason :: :tenant_mismatch | :forbidden_cross_tenant_access

  @doc """
  Validates that a resource belongs to the actor's tenant.

  Returns `:ok` if tenant_ids match, `{:error, :tenant_mismatch}` otherwise.

  ## Examples

      iex> queue = %{id: "q1", tenant_id: "tenant-1"}
      iex> labeler = %{id: "l1", tenant_id: "tenant-1"}
      iex> TenantContext.validate_tenant(queue, labeler)
      :ok

      iex> queue = %{id: "q1", tenant_id: "tenant-1"}
      iex> labeler = %{id: "l1", tenant_id: "tenant-2"}
      iex> TenantContext.validate_tenant(queue, labeler)
      {:error, :tenant_mismatch}
  """
  @spec validate_tenant(tenant_resource(), actor()) :: :ok | {:error, error_reason()}
  def validate_tenant(resource, actor) do
    resource_tenant = extract_tenant_id(resource)
    actor_tenant = extract_tenant_id(actor)

    if resource_tenant == actor_tenant and not is_nil(resource_tenant) do
      :ok
    else
      {:error, :tenant_mismatch}
    end
  end

  @doc """
  Validates that all resources in a list belong to the actor's tenant.

  Returns `:ok` if all resources match, `{:error, :tenant_mismatch}` if any don't.

  ## Examples

      iex> resources = [
      ...>   %{id: "r1", tenant_id: "tenant-1"},
      ...>   %{id: "r2", tenant_id: "tenant-1"}
      ...> ]
      iex> labeler = %{id: "l1", tenant_id: "tenant-1"}
      iex> TenantContext.validate_tenant_list(resources, labeler)
      :ok
  """
  @spec validate_tenant_list([tenant_resource()], actor()) :: :ok | {:error, error_reason()}
  def validate_tenant_list(resources, actor) do
    actor_tenant = extract_tenant_id(actor)

    all_match? =
      Enum.all?(resources, fn resource ->
        extract_tenant_id(resource) == actor_tenant
      end)

    if all_match? do
      :ok
    else
      {:error, :tenant_mismatch}
    end
  end

  @doc """
  Filters a list of resources to only include those matching the actor's tenant.

  ## Examples

      iex> resources = [
      ...>   %{id: "r1", tenant_id: "tenant-1"},
      ...>   %{id: "r2", tenant_id: "tenant-2"}
      ...> ]
      iex> labeler = %{id: "l1", tenant_id: "tenant-1"}
      iex> filtered = TenantContext.filter_by_tenant(resources, labeler)
      iex> length(filtered)
      1
  """
  @spec filter_by_tenant([tenant_resource()], actor()) :: [tenant_resource()]
  def filter_by_tenant(resources, actor) do
    actor_tenant = extract_tenant_id(actor)

    Enum.filter(resources, fn resource ->
      extract_tenant_id(resource) == actor_tenant
    end)
  end

  @doc """
  Checks if two resources belong to the same tenant.

  ## Examples

      iex> r1 = %{id: "r1", tenant_id: "tenant-1"}
      iex> r2 = %{id: "r2", tenant_id: "tenant-1"}
      iex> TenantContext.same_tenant?(r1, r2)
      true
  """
  @spec same_tenant?(tenant_resource(), tenant_resource()) :: boolean()
  def same_tenant?(resource1, resource2) do
    tenant1 = extract_tenant_id(resource1)
    tenant2 = extract_tenant_id(resource2)

    not is_nil(tenant1) and tenant1 == tenant2
  end

  @doc """
  Ensures strict tenant isolation with forbidden error.

  Returns `:ok` if access allowed, `{:error, :forbidden_cross_tenant_access}` otherwise.
  Use this for operations that should never cross tenant boundaries.

  ## Options
    - `:error` - Custom error atom to return (default: :forbidden_cross_tenant_access)

  ## Examples

      iex> queue = %{id: "q1", tenant_id: "tenant-1"}
      iex> labeler = %{id: "l1", tenant_id: "tenant-1"}
      iex> TenantContext.ensure_tenant_isolation(queue, labeler)
      :ok
  """
  @spec ensure_tenant_isolation(tenant_resource(), actor(), keyword()) ::
          :ok | {:error, error_reason()}
  def ensure_tenant_isolation(resource, actor, opts \\ []) do
    error_atom = Keyword.get(opts, :error, :forbidden_cross_tenant_access)

    case validate_tenant(resource, actor) do
      :ok -> :ok
      {:error, _} -> {:error, error_atom}
    end
  end

  @doc """
  Adds tenant_id filter to a keyword list of query conditions.

  ## Examples

      iex> conditions = [status: :active]
      iex> TenantContext.tenant_scope(conditions, "tenant-1")
      [tenant_id: "tenant-1", status: :active]
  """
  @spec tenant_scope(keyword(), tenant_id()) :: keyword()
  def tenant_scope(conditions, tenant_id) when is_list(conditions) do
    Keyword.put(conditions, :tenant_id, tenant_id)
  end

  @doc """
  Extracts tenant_id from a resource or actor.

  Works with both structs and maps. Returns nil if tenant_id not found.

  ## Examples

      iex> resource = %{id: "r1", tenant_id: "tenant-1"}
      iex> TenantContext.extract_tenant_id(resource)
      "tenant-1"
  """
  @spec extract_tenant_id(tenant_resource() | actor()) :: tenant_id() | nil
  def extract_tenant_id(%{tenant_id: tenant_id}), do: tenant_id
  def extract_tenant_id(_), do: nil
end
</file>

<file path="anvil/export/csv.ex">
defmodule Anvil.Export.CSV do
  @moduledoc """
  CSV export adapter with deterministic ordering and lineage tracking.

  Implements the ADR-005 export system with:
  - Streaming for memory safety
  - Deterministic ordering (sample_id ASC, labeler_id ASC, submitted_at ASC)
  - Export manifest generation with SHA256 hashes
  - Proper CSV escaping
  """

  alias Anvil.Export.Manifest
  alias Anvil.Repo
  alias Anvil.Schema.{Label, Assignment, SchemaVersion, Labeler}
  alias Anvil.Telemetry
  alias Anvil.PII.Redactor
  import Ecto.Query

  @doc """
  Exports labels to CSV format following ADR-005 specification.

  ## Options

    * `:schema_version_id` - (required) UUID of the schema version for reproducibility
    * `:output_path` - (required) File path for the CSV export
    * `:sample_version` - (optional) Forge version tag for full lineage tracking
    * `:limit` - (optional) Maximum number of rows to export
    * `:offset` - (optional) Number of rows to skip before exporting
    * `:filter` - (optional) Additional filter criteria
    * `:redaction_mode` - (optional) Redaction mode (`:none`, `:automatic`, `:aggressive`) (default: `:automatic`)
    * `:use_pseudonyms` - (optional) Use labeler pseudonyms instead of IDs (default: `true`)

  ## Returns

    * `{:ok, %{manifest: manifest, output_path: path}}` on success
    * `{:error, reason}` on failure

  ## Examples

      iex> Anvil.Export.CSV.to_format(queue_id, %{
      ...>   schema_version_id: schema_v2_id,
      ...>   output_path: "/tmp/labels.csv"
      ...> })
      {:ok, %{manifest: %Manifest{...}, output_path: "/tmp/labels.csv"}}
  """
  @spec to_format(binary(), map()) :: {:ok, map()} | {:error, term()}
  def to_format(queue_id, opts) when is_map(opts) do
    export_id = Ecto.UUID.generate()

    # Wrap export in telemetry span
    Telemetry.span_export_generate(
      %{queue_id: queue_id, format: :csv, export_id: export_id},
      fn ->
        with {:ok, schema_version_id} <- validate_required_opt(opts, :schema_version_id),
             {:ok, output_path} <- validate_required_opt(opts, :output_path),
             :ok <- ensure_directory_exists(output_path) do
          # Write export to temporary file first (atomic operation)
          tmp_path = output_path <> ".tmp"

          try do
            # Stream labels and write to file
            row_count = write_csv_file(tmp_path, queue_id, schema_version_id, opts, export_id)

            # Rename tmp file to final destination
            :ok = File.rename!(tmp_path, output_path)

            case Manifest.compute_file_hash(output_path) do
              {:ok, sha256_hash} ->
                manifest =
                  Manifest.new(%{
                    queue_id: queue_id,
                    schema_version_id: schema_version_id,
                    sample_version: Map.get(opts, :sample_version),
                    format: :csv,
                    output_path: output_path,
                    row_count: row_count,
                    sha256_hash: sha256_hash,
                    exported_at: DateTime.utc_now(),
                    parameters: %{
                      limit: Map.get(opts, :limit),
                      offset: Map.get(opts, :offset),
                      filter: Map.get(opts, :filter)
                    }
                  })

                # Save manifest
                :ok = Manifest.save(manifest)

                # Emit export completed event
                Telemetry.emit_export_completed(export_id, %{
                  queue_id: queue_id,
                  format: :csv,
                  row_count: row_count,
                  file_size_bytes: File.stat!(output_path).size
                })

                {{:ok, %{manifest: manifest, output_path: output_path}}, %{row_count: row_count}}

              {:error, reason} ->
                File.rm(output_path)

                Telemetry.emit_export_failed(export_id, reason, %{
                  queue_id: queue_id,
                  format: :csv
                })

                {{:error, reason}, %{error: reason}}
            end
          rescue
            e ->
              # Clean up temp file on error
              File.rm(tmp_path)
              Telemetry.emit_export_failed(export_id, e, %{queue_id: queue_id, format: :csv})
              {{:error, e}, %{error: e}}
          end
        else
          {:error, reason} ->
            Telemetry.emit_export_failed(export_id, reason, %{queue_id: queue_id, format: :csv})
            {{:error, reason}, %{error: reason}}
        end
      end
    )
  end

  @doc """
  Legacy export function for backward compatibility.

  This function is deprecated in favor of `to_format/2`.
  """
  @spec export([Anvil.Label.t()], String.t(), keyword()) :: :ok | {:error, term()}
  def export(labels, path, opts \\ []) do
    include_metadata = Keyword.get(opts, :include_metadata, true)

    with :ok <- ensure_directory_exists(path),
         {:ok, file} <- File.open(path, [:write, :utf8]) do
      # Write header
      header = build_header(labels, include_metadata)
      IO.write(file, header <> "\n")

      # Write rows
      Enum.each(labels, fn label ->
        row = build_row(label, include_metadata)
        IO.write(file, row <> "\n")
      end)

      File.close(file)
      :ok
    else
      {:error, reason} -> {:error, reason}
    end
  end

  # Private functions

  defp validate_required_opt(opts, key) do
    case Map.fetch(opts, key) do
      {:ok, value} -> {:ok, value}
      :error -> {:error, {:missing_required_option, key}}
    end
  end

  defp write_csv_file(path, queue_id, schema_version_id, opts, export_id) do
    File.open!(path, [:write, :utf8], fn file ->
      # Load schema version for field metadata
      schema_version = Repo.get!(SchemaVersion, schema_version_id)

      field_metadata_map =
        Anvil.PII.Retention.extract_field_metadata(schema_version.schema_definition)

      redaction_mode = Map.get(opts, :redaction_mode, :automatic)
      use_pseudonyms = Map.get(opts, :use_pseudonyms, true)

      # Get first label to determine fields for header
      first_label_query =
        from(l in Label,
          join: a in Assignment,
          on: l.assignment_id == a.id,
          where: a.queue_id == ^queue_id,
          where: l.schema_version_id == ^schema_version_id,
          limit: 1
        )

      first_label = Repo.one(first_label_query)

      # Write header
      header = build_csv_header(first_label, field_metadata_map, redaction_mode)
      IO.write(file, header <> "\n")

      # Stream labels with deterministic ordering
      query = build_export_query(queue_id, schema_version_id, opts)

      {:ok, row_count} =
        Repo.transaction(fn ->
          total_count_ref = :counters.new(1, [])

          Repo.stream(query, max_rows: 1000)
          |> Stream.chunk_every(100)
          |> Stream.map(fn batch ->
            rows =
              Enum.map(batch, fn label ->
                encode_csv_row(label, field_metadata_map, redaction_mode, use_pseudonyms)
              end)

            IO.write(file, Enum.join(rows, "\n") <> "\n")
            batch_size = length(batch)

            # Update progress counter and emit telemetry
            :counters.add(total_count_ref, 1, batch_size)
            current_count = :counters.get(total_count_ref, 1)

            # Emit progress every 100 rows
            if rem(current_count, 100) == 0 do
              Telemetry.emit_export_progress(current_count, %{
                export_id: export_id,
                queue_id: queue_id,
                format: :csv
              })
            end

            batch_size
          end)
          |> Enum.sum()
        end)

      row_count
    end)
  end

  defp build_export_query(queue_id, schema_version_id, opts) do
    query =
      from(l in Label,
        join: a in Assignment,
        on: l.assignment_id == a.id,
        where: a.queue_id == ^queue_id,
        where: l.schema_version_id == ^schema_version_id,
        order_by: [asc: a.sample_id, asc: l.labeler_id, asc: l.submitted_at],
        select: %{
          sample_id: a.sample_id,
          labeler_id: l.labeler_id,
          payload: l.payload,
          submitted_at: l.submitted_at
        }
      )

    query = maybe_apply_limit(query, Map.get(opts, :limit))
    query = maybe_apply_offset(query, Map.get(opts, :offset))

    query
  end

  defp maybe_apply_limit(query, nil), do: query
  defp maybe_apply_limit(query, limit), do: from(q in query, limit: ^limit)

  defp maybe_apply_offset(query, nil), do: query
  defp maybe_apply_offset(query, offset), do: from(q in query, offset: ^offset)

  defp build_csv_header(nil, _field_metadata_map, _redaction_mode) do
    # Default header when no labels exist
    "sample_id,labeler_id,submitted_at"
  end

  defp build_csv_header(label, field_metadata_map, redaction_mode) do
    base_fields = ["sample_id", "labeler_id"]

    # Extract field names from payload and sort them for consistency
    # Filter out redacted fields based on redaction mode
    payload_fields =
      case label.payload do
        nil ->
          []

        payload ->
          payload
          |> Map.keys()
          |> Enum.filter(fn field_name ->
            field_metadata = Map.get(field_metadata_map, field_name, %{})

            not (Anvil.PII.should_redact?(field_metadata, redaction_mode) &&
                   Anvil.PII.redaction_policy(field_metadata) == :strip)
          end)
          |> Enum.sort()
      end

    metadata_fields = ["submitted_at"]

    (base_fields ++ payload_fields ++ metadata_fields)
    |> Enum.join(",")
  end

  defp encode_csv_row(label, field_metadata_map, redaction_mode, use_pseudonyms) do
    # Get labeler identifier (pseudonym or ID)
    labeler_value =
      if use_pseudonyms do
        case Repo.get(Labeler, label.labeler_id) do
          nil -> label.labeler_id
          labeler -> labeler.pseudonym || label.labeler_id
        end
      else
        label.labeler_id
      end

    base_values = [
      escape_csv_value(label.sample_id),
      escape_csv_value(labeler_value)
    ]

    # Apply redaction to payload
    redacted_payload =
      Redactor.redact_payload(label.payload || %{}, field_metadata_map, redaction_mode)

    # Extract payload values in sorted key order
    payload_values =
      redacted_payload
      |> Enum.sort_by(fn {k, _} -> k end)
      |> Enum.map(fn {_, v} -> escape_csv_value(v) end)

    metadata_values = [
      escape_csv_value(DateTime.to_iso8601(label.submitted_at))
    ]

    (base_values ++ payload_values ++ metadata_values)
    |> Enum.join(",")
  end

  defp build_header(labels, include_metadata) do
    base_fields = ["sample_id", "labeler_id"]

    value_fields =
      case List.first(labels) do
        nil -> []
        label -> Map.keys(label.values)
      end

    metadata_fields =
      if include_metadata do
        ["labeling_time_seconds", "created_at", "valid"]
      else
        []
      end

    (base_fields ++ value_fields ++ metadata_fields)
    |> Enum.join(",")
  end

  defp build_row(label, include_metadata) do
    base_values = [label.sample_id, label.labeler_id]

    value_fields =
      label.values
      |> Enum.sort_by(fn {k, _} -> k end)
      |> Enum.map(fn {_, v} -> escape_csv_value(v) end)

    metadata_values =
      if include_metadata do
        [
          to_string(label.labeling_time_seconds || ""),
          DateTime.to_iso8601(label.created_at),
          to_string(label.valid?)
        ]
      else
        []
      end

    (base_values ++ value_fields ++ metadata_values)
    |> Enum.join(",")
  end

  defp escape_csv_value(value) when is_binary(value) do
    if String.contains?(value, [",", "\"", "\n", "\r"]) do
      "\"" <> String.replace(value, "\"", "\"\"") <> "\""
    else
      value
    end
  end

  defp escape_csv_value(value) when is_boolean(value), do: to_string(value)
  defp escape_csv_value(value) when is_number(value), do: to_string(value)
  defp escape_csv_value(nil), do: ""
  defp escape_csv_value(value), do: to_string(value)

  defp ensure_directory_exists(path) do
    dir = Path.dirname(path)

    case File.mkdir_p(dir) do
      :ok -> :ok
      {:error, reason} -> {:error, {:mkdir_failed, reason}}
    end
  end
end
</file>

<file path="anvil/export/jsonl.ex">
defmodule Anvil.Export.JSONL do
  @moduledoc """
  JSONL (JSON Lines) export adapter with deterministic ordering and lineage tracking.

  Implements the ADR-005 export system with:
  - Streaming for memory safety
  - Deterministic ordering (sample_id ASC, labeler_id ASC, submitted_at ASC)
  - Export manifest generation with SHA256 hashes
  - Preservation of nested JSON structures
  """

  alias Anvil.Export.Manifest
  alias Anvil.Repo
  alias Anvil.Schema.{Label, Assignment, SchemaVersion, Labeler}
  alias Anvil.PII.Redactor
  import Ecto.Query

  @doc """
  Exports labels to JSONL format following ADR-005 specification.

  ## Options

    * `:schema_version_id` - (required) UUID of the schema version for reproducibility
    * `:output_path` - (required) File path for the JSONL export
    * `:sample_version` - (optional) Forge version tag for full lineage tracking
    * `:limit` - (optional) Maximum number of rows to export
    * `:offset` - (optional) Number of rows to skip before exporting
    * `:filter` - (optional) Additional filter criteria
    * `:redaction_mode` - (optional) Redaction mode (`:none`, `:automatic`, `:aggressive`) (default: `:automatic`)
    * `:use_pseudonyms` - (optional) Use labeler pseudonyms instead of IDs (default: `true`)

  ## Returns

    * `{:ok, %{manifest: manifest, output_path: path}}` on success
    * `{:error, reason}` on failure

  ## Examples

      iex> Anvil.Export.JSONL.to_format(queue_id, %{
      ...>   schema_version_id: schema_v2_id,
      ...>   output_path: "/tmp/labels.jsonl"
      ...> })
      {:ok, %{manifest: %Manifest{...}, output_path: "/tmp/labels.jsonl"}}
  """
  @spec to_format(binary(), map()) :: {:ok, map()} | {:error, term()}
  def to_format(queue_id, opts) when is_map(opts) do
    with {:ok, schema_version_id} <- validate_required_opt(opts, :schema_version_id),
         {:ok, output_path} <- validate_required_opt(opts, :output_path),
         :ok <- ensure_directory_exists(output_path) do
      # Write export to temporary file first (atomic operation)
      tmp_path = output_path <> ".tmp"

      try do
        # Stream labels and write to file
        row_count = write_jsonl_file(tmp_path, queue_id, schema_version_id, opts)

        # Rename tmp file to final destination
        :ok = File.rename!(tmp_path, output_path)

        case Manifest.compute_file_hash(output_path) do
          {:ok, sha256_hash} ->
            manifest =
              Manifest.new(%{
                queue_id: queue_id,
                schema_version_id: schema_version_id,
                sample_version: Map.get(opts, :sample_version),
                format: :jsonl,
                output_path: output_path,
                row_count: row_count,
                sha256_hash: sha256_hash,
                exported_at: DateTime.utc_now(),
                parameters: %{
                  limit: Map.get(opts, :limit),
                  offset: Map.get(opts, :offset),
                  filter: Map.get(opts, :filter)
                }
              })

            # Save manifest
            :ok = Manifest.save(manifest)

            {:ok, %{manifest: manifest, output_path: output_path}}

          {:error, reason} ->
            File.rm(output_path)
            {:error, reason}
        end
      rescue
        e ->
          # Clean up temp file on error
          File.rm(tmp_path)
          {:error, e}
      end
    end
  end

  @doc """
  Legacy export function for backward compatibility.

  This function is deprecated in favor of `to_format/2`.
  """
  @spec export([Anvil.Label.t()], String.t(), keyword()) :: :ok | {:error, term()}
  def export(labels, path, opts \\ []) do
    include_metadata = Keyword.get(opts, :include_metadata, true)

    with :ok <- ensure_directory_exists(path),
         {:ok, file} <- File.open(path, [:write, :utf8]) do
      Enum.each(labels, fn label ->
        json = to_json(label, include_metadata)
        IO.write(file, json <> "\n")
      end)

      File.close(file)
      :ok
    else
      {:error, reason} -> {:error, reason}
    end
  end

  # Private functions

  defp validate_required_opt(opts, key) do
    case Map.fetch(opts, key) do
      {:ok, value} -> {:ok, value}
      :error -> {:error, {:missing_required_option, key}}
    end
  end

  defp write_jsonl_file(path, queue_id, schema_version_id, opts) do
    File.open!(path, [:write, :utf8], fn file ->
      # Load schema version for field metadata
      schema_version = Repo.get!(SchemaVersion, schema_version_id)

      field_metadata_map =
        Anvil.PII.Retention.extract_field_metadata(schema_version.schema_definition)

      redaction_mode = Map.get(opts, :redaction_mode, :automatic)
      use_pseudonyms = Map.get(opts, :use_pseudonyms, true)

      # Stream labels with deterministic ordering
      query = build_export_query(queue_id, schema_version_id, opts)

      {:ok, row_count} =
        Repo.transaction(fn ->
          Repo.stream(query, max_rows: 1000)
          |> Stream.chunk_every(100)
          |> Stream.map(fn batch ->
            lines =
              Enum.map(batch, fn label ->
                encode_jsonl_line(label, field_metadata_map, redaction_mode, use_pseudonyms)
              end)

            IO.write(file, Enum.join(lines, "\n"))

            # Add newline after batch if not empty
            if length(batch) > 0 do
              IO.write(file, "\n")
            end

            length(batch)
          end)
          |> Enum.sum()
        end)

      row_count
    end)
  end

  defp build_export_query(queue_id, schema_version_id, opts) do
    query =
      from(l in Label,
        join: a in Assignment,
        on: l.assignment_id == a.id,
        where: a.queue_id == ^queue_id,
        where: l.schema_version_id == ^schema_version_id,
        order_by: [asc: a.sample_id, asc: l.labeler_id, asc: l.submitted_at],
        select: %{
          sample_id: a.sample_id,
          labeler_id: l.labeler_id,
          payload: l.payload,
          submitted_at: l.submitted_at
        }
      )

    query = maybe_apply_limit(query, Map.get(opts, :limit))
    query = maybe_apply_offset(query, Map.get(opts, :offset))

    query
  end

  defp maybe_apply_limit(query, nil), do: query
  defp maybe_apply_limit(query, limit), do: from(q in query, limit: ^limit)

  defp maybe_apply_offset(query, nil), do: query
  defp maybe_apply_offset(query, offset), do: from(q in query, offset: ^offset)

  defp encode_jsonl_line(label, field_metadata_map, redaction_mode, use_pseudonyms) do
    # Get labeler identifier (pseudonym or ID)
    labeler_value =
      if use_pseudonyms do
        case Repo.get(Labeler, label.labeler_id) do
          nil -> label.labeler_id
          labeler -> labeler.pseudonym || label.labeler_id
        end
      else
        label.labeler_id
      end

    # Apply redaction to payload
    redacted_payload =
      Redactor.redact_payload(label.payload || %{}, field_metadata_map, redaction_mode)

    data = %{
      sample_id: label.sample_id,
      labeler_id: labeler_value,
      payload: redacted_payload,
      submitted_at: DateTime.to_iso8601(label.submitted_at)
    }

    Jason.encode!(data)
  end

  defp to_json(label, include_metadata) do
    base = %{
      sample_id: label.sample_id,
      labeler_id: label.labeler_id,
      values: label.values
    }

    data =
      if include_metadata do
        Map.merge(base, %{
          labeling_time_seconds: label.labeling_time_seconds,
          created_at: DateTime.to_iso8601(label.created_at),
          valid: label.valid?
        })
      else
        base
      end

    Jason.encode!(data)
  end

  defp ensure_directory_exists(path) do
    dir = Path.dirname(path)

    case File.mkdir_p(dir) do
      :ok -> :ok
      {:error, reason} -> {:error, {:mkdir_failed, reason}}
    end
  end
end
</file>

<file path="anvil/export/manifest.ex">
defmodule Anvil.Export.Manifest do
  @moduledoc """
  Export manifest for tracking dataset lineage and reproducibility.

  Manifests include:
  - Export metadata (queue, schema version, format)
  - File hash for integrity verification
  - Export parameters for reproducibility
  - Anvil version for compatibility tracking
  """

  @enforce_keys [
    :export_id,
    :queue_id,
    :schema_version_id,
    :format,
    :output_path,
    :row_count,
    :sha256_hash,
    :exported_at,
    :parameters,
    :anvil_version
  ]

  defstruct [
    :export_id,
    :queue_id,
    :schema_version_id,
    :sample_version,
    :format,
    :output_path,
    :row_count,
    :sha256_hash,
    :exported_at,
    :parameters,
    :anvil_version,
    :schema_definition_hash
  ]

  @type t :: %__MODULE__{
          export_id: String.t(),
          queue_id: binary(),
          schema_version_id: binary(),
          sample_version: String.t() | nil,
          format: :csv | :jsonl | :parquet | :huggingface,
          output_path: String.t(),
          row_count: non_neg_integer(),
          sha256_hash: String.t(),
          exported_at: DateTime.t(),
          parameters: map(),
          anvil_version: String.t(),
          schema_definition_hash: String.t() | nil
        }

  @doc """
  Creates a new manifest with the given parameters.

  ## Examples

      iex> Anvil.Export.Manifest.new(%{
      ...>   queue_id: "queue_123",
      ...>   schema_version_id: "schema_v1",
      ...>   format: :csv,
      ...>   output_path: "/tmp/export.csv",
      ...>   row_count: 100,
      ...>   sha256_hash: "abc123",
      ...>   exported_at: ~U[2025-12-01 10:00:00Z],
      ...>   parameters: %{}
      ...> })
      %Anvil.Export.Manifest{...}
  """
  @spec new(map()) :: t()
  def new(params) do
    %__MODULE__{
      export_id: Map.get(params, :export_id, generate_export_id()),
      queue_id: Map.fetch!(params, :queue_id),
      schema_version_id: Map.fetch!(params, :schema_version_id),
      sample_version: Map.get(params, :sample_version),
      format: Map.fetch!(params, :format),
      output_path: Map.fetch!(params, :output_path),
      row_count: Map.fetch!(params, :row_count),
      sha256_hash: Map.fetch!(params, :sha256_hash),
      exported_at: Map.fetch!(params, :exported_at),
      parameters: Map.fetch!(params, :parameters),
      anvil_version: Map.get(params, :anvil_version, anvil_version()),
      schema_definition_hash: Map.get(params, :schema_definition_hash)
    }
  end

  @doc """
  Converts the manifest to a JSON string.

  ## Examples

      iex> manifest = %Anvil.Export.Manifest{...}
      iex> Anvil.Export.Manifest.to_json(manifest)
      "{\\"export_id\\": \\"exp_123\\", ...}"
  """
  @spec to_json(t()) :: String.t()
  def to_json(%__MODULE__{} = manifest) do
    manifest
    |> Map.from_struct()
    |> Jason.encode!(pretty: true)
  end

  @doc """
  Parses a JSON string into a manifest struct.

  ## Examples

      iex> json = ~s({"export_id": "exp_123", ...})
      iex> Anvil.Export.Manifest.from_json(json)
      {:ok, %Anvil.Export.Manifest{...}}
  """
  @spec from_json(String.t()) :: {:ok, t()} | {:error, term()}
  def from_json(json) when is_binary(json) do
    case Jason.decode(json) do
      {:ok, data} ->
        manifest = %__MODULE__{
          export_id: data["export_id"],
          queue_id: data["queue_id"],
          schema_version_id: data["schema_version_id"],
          sample_version: data["sample_version"],
          format: String.to_existing_atom(data["format"]),
          output_path: data["output_path"],
          row_count: data["row_count"],
          sha256_hash: data["sha256_hash"],
          exported_at: parse_datetime(data["exported_at"]),
          parameters: data["parameters"] || %{},
          anvil_version: data["anvil_version"],
          schema_definition_hash: data["schema_definition_hash"]
        }

        {:ok, manifest}

      {:error, reason} ->
        {:error, reason}
    end
  end

  @doc """
  Saves the manifest to a file.

  By default, saves to `<output_path>.manifest.json`.
  A custom path can be provided as the second argument.

  ## Examples

      iex> manifest = %Anvil.Export.Manifest{output_path: "/tmp/export.csv", ...}
      iex> Anvil.Export.Manifest.save(manifest)
      :ok
      # Creates /tmp/export.csv.manifest.json

      iex> Anvil.Export.Manifest.save(manifest, "/tmp/custom.json")
      :ok
      # Creates /tmp/custom.json
  """
  @spec save(t(), String.t() | nil) :: :ok | {:error, term()}
  def save(%__MODULE__{} = manifest, custom_path \\ nil) do
    path = custom_path || "#{manifest.output_path}.manifest.json"
    json = to_json(manifest)

    case File.write(path, json) do
      :ok -> :ok
      {:error, reason} -> {:error, reason}
    end
  end

  @doc """
  Loads a manifest from a file.

  ## Examples

      iex> Anvil.Export.Manifest.load("/tmp/export.csv.manifest.json")
      {:ok, %Anvil.Export.Manifest{...}}
  """
  @spec load(String.t()) :: {:ok, t()} | {:error, term()}
  def load(path) do
    case File.read(path) do
      {:ok, json} -> from_json(json)
      {:error, reason} -> {:error, reason}
    end
  end

  @doc """
  Computes the SHA256 hash of a file.

  Uses streaming to handle large files without loading them into memory.

  ## Examples

      iex> Anvil.Export.Manifest.compute_file_hash("/tmp/export.csv")
      {:ok, "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"}
  """
  @spec compute_file_hash(String.t()) :: {:ok, String.t()} | {:error, term()}
  def compute_file_hash(path) do
    case File.exists?(path) do
      true ->
        hash =
          File.stream!(path, 2048, [])
          |> Enum.reduce(:crypto.hash_init(:sha256), fn chunk, acc ->
            :crypto.hash_update(acc, chunk)
          end)
          |> :crypto.hash_final()
          |> Base.encode16(case: :lower)

        {:ok, hash}

      false ->
        {:error, :enoent}
    end
  end

  # Private functions

  defp generate_export_id do
    "exp_" <> Base.encode16(:crypto.strong_rand_bytes(8), case: :lower)
  end

  defp anvil_version do
    case Application.spec(:anvil, :vsn) do
      nil -> "0.1.1"
      vsn -> List.to_string(vsn)
    end
  end

  defp parse_datetime(nil), do: nil

  defp parse_datetime(string) when is_binary(string) do
    case DateTime.from_iso8601(string) do
      {:ok, datetime, _offset} -> datetime
      {:error, _} -> nil
    end
  end

  defp parse_datetime(%DateTime{} = datetime), do: datetime
end
</file>

<file path="anvil/forge_bridge/cached.ex">
defmodule Anvil.ForgeBridge.Cached do
  @moduledoc """
  Caching wrapper for ForgeBridge backends.

  Adds TTL-based caching layer on top of any ForgeBridge backend to improve
  performance and reduce load on Forge. Best for hybrid deployments where
  sample content is relatively stable.

  ## Configuration

      # config/config.exs
      config :anvil,
        forge_bridge_backend: Anvil.ForgeBridge.Cached,
        forge_bridge_primary_backend: Anvil.ForgeBridge.Direct,
        forge_cache_ttl: :timer.minutes(15)

  ## Features

  - TTL-based expiration (default 15 minutes)
  - Cache warming for hot queues
  - Graceful degradation (serve stale on backend failure)
  - Telemetry events for cache hits/misses

  ## Cache Invalidation

  Subscribe to sample update events for proactive invalidation:

      Phoenix.PubSub.subscribe(Forge.PubSub, "sample_updates")

      def handle_info({:sample_updated, sample_id}, state) do
        Anvil.ForgeBridge.Cached.invalidate(sample_id)
        {:noreply, state}
      end
  """

  @behaviour Anvil.ForgeBridge

  alias Anvil.ForgeBridge.SampleDTO
  require Logger

  @cache_name :forge_samples

  @impl true
  def fetch_sample(sample_id, opts \\ []) do
    # Check if caching is disabled for this request
    if Keyword.get(opts, :bypass_cache, false) do
      fetch_from_backend(sample_id, opts)
    else
      case Cachex.get(@cache_name, sample_id) do
        {:ok, nil} ->
          # Cache miss
          :telemetry.execute([:anvil, :forge_bridge, :cache_miss], %{}, %{
            sample_id: sample_id
          })

          fetch_and_cache(sample_id, opts)

        {:ok, sample_dto} ->
          # Cache hit
          :telemetry.execute([:anvil, :forge_bridge, :cache_hit], %{}, %{sample_id: sample_id})
          {:ok, sample_dto}

        {:error, _} ->
          # Cache error, bypass cache
          Logger.warning("Cache error for sample #{sample_id}, bypassing cache")
          fetch_from_backend(sample_id, opts)
      end
    end
  end

  @impl true
  def fetch_samples(sample_ids, opts \\ []) when is_list(sample_ids) do
    if Keyword.get(opts, :bypass_cache, false) do
      fetch_samples_from_backend(sample_ids, opts)
    else
      # Split into cached and uncached
      {cached, uncached} = partition_cached(sample_ids)

      # Fetch uncached samples
      case fetch_samples_from_backend(uncached, opts) do
        {:ok, fetched} ->
          # Cache the newly fetched samples
          Enum.each(fetched, &cache_sample/1)

          # Combine cached and fetched
          all_samples = cached ++ fetched
          {:ok, all_samples}

        {:error, _} = error ->
          # Return cached samples even on error (graceful degradation)
          if Enum.empty?(cached) do
            error
          else
            Logger.warning(
              "Partial sample fetch failed, returning #{length(cached)} cached samples"
            )

            {:ok, cached}
          end
      end
    end
  end

  @impl true
  def verify_sample_exists(sample_id) do
    # Check cache first
    case Cachex.get(@cache_name, sample_id) do
      {:ok, %SampleDTO{}} ->
        true

      _ ->
        # Fall back to backend
        primary_backend().verify_sample_exists(sample_id)
    end
  end

  @impl true
  def fetch_sample_version(sample_id) do
    # Check cache first
    case Cachex.get(@cache_name, sample_id) do
      {:ok, %SampleDTO{version: version}} ->
        {:ok, version}

      _ ->
        # Fall back to backend
        primary_backend().fetch_sample_version(sample_id)
    end
  end

  # Public API for cache management

  @doc """
  Invalidates a cached sample by ID.
  """
  @spec invalidate(binary()) :: :ok
  def invalidate(sample_id) do
    Cachex.del(@cache_name, sample_id)
    :ok
  end

  @doc """
  Warms the cache for a list of sample IDs.

  Useful for preloading samples for a queue before labelers request them.
  """
  @spec warm_cache([binary()]) :: :ok
  def warm_cache(sample_ids) when is_list(sample_ids) do
    Task.Supervisor.async_stream_nolink(
      Anvil.TaskSupervisor,
      sample_ids,
      fn sample_id -> fetch_sample(sample_id) end,
      max_concurrency: 10,
      timeout: 5_000,
      on_timeout: :kill_task
    )
    |> Stream.run()

    :ok
  end

  @doc """
  Clears the entire sample cache.
  """
  @spec clear_cache() :: {:ok, integer()}
  def clear_cache do
    Cachex.clear(@cache_name)
  end

  # Private helpers

  defp fetch_and_cache(sample_id, opts) do
    case fetch_from_backend(sample_id, opts) do
      {:ok, sample_dto} = result ->
        cache_sample(sample_dto)
        result

      error ->
        error
    end
  end

  defp fetch_from_backend(sample_id, opts) do
    primary_backend().fetch_sample(sample_id, opts)
  end

  defp fetch_samples_from_backend(sample_ids, opts) do
    primary_backend().fetch_samples(sample_ids, opts)
  end

  defp cache_sample(%SampleDTO{} = sample) do
    ttl = cache_ttl()
    Cachex.put(@cache_name, sample.id, sample, ttl: ttl)
  end

  defp partition_cached(sample_ids) do
    sample_ids
    |> Enum.map(fn id ->
      case Cachex.get(@cache_name, id) do
        {:ok, %SampleDTO{} = sample} -> {:cached, sample}
        _ -> {:uncached, id}
      end
    end)
    |> Enum.split_with(&match?({:cached, _}, &1))
    |> then(fn {cached, uncached} ->
      {
        Enum.map(cached, fn {:cached, sample} -> sample end),
        Enum.map(uncached, fn {:uncached, id} -> id end)
      }
    end)
  end

  defp primary_backend do
    Application.get_env(:anvil, :forge_bridge_primary_backend, Anvil.ForgeBridge.Direct)
  end

  defp cache_ttl do
    Application.get_env(:anvil, :forge_cache_ttl, :timer.minutes(15))
  end
end
</file>

<file path="anvil/forge_bridge/direct.ex">
defmodule Anvil.ForgeBridge.Direct do
  @moduledoc """
  Direct database access implementation for ForgeBridge.

  Queries Forge samples directly from the database using cross-schema queries.
  Best for shared Postgres deployments where Anvil and Forge share the same
  database cluster.

  ## Configuration

      # config/config.exs
      config :anvil,
        forge_bridge_backend: Anvil.ForgeBridge.Direct,
        forge_schema: "forge"  # Schema name in Postgres

  ## Performance

  - Fastest option (<5ms p99)
  - No network overhead
  - Transactional consistency
  - Requires shared database cluster

  ## Database Setup

  Requires Forge samples table to be accessible:

      -- Cross-schema query
      SELECT * FROM forge.samples WHERE id = $1;

  """

  @behaviour Anvil.ForgeBridge

  alias Anvil.ForgeBridge.SampleDTO
  alias Anvil.Repo
  import Ecto.Query

  @impl true
  def fetch_sample(sample_id, opts \\ []) do
    schema = forge_schema()

    query =
      from(s in fragment("?.samples", literal(^schema)),
        where: s.id == ^sample_id,
        select: %{
          id: s.id,
          content: s.content,
          version_tag: s.version_tag,
          metadata: s.metadata,
          created_at: s.inserted_at
        }
      )

    case Repo.one(query) do
      nil ->
        {:error, :not_found}

      sample_row ->
        dto = to_dto(sample_row, opts)
        {:ok, dto}
    end
  rescue
    e in Postgrex.Error ->
      require Logger
      Logger.error("Forge DB query failed: #{inspect(e)}")
      {:error, :forge_unavailable}
  end

  @impl true
  def fetch_samples(sample_ids, opts \\ []) when is_list(sample_ids) do
    schema = forge_schema()

    query =
      from(s in fragment("?.samples", literal(^schema)),
        where: s.id in ^sample_ids,
        select: %{
          id: s.id,
          content: s.content,
          version_tag: s.version_tag,
          metadata: s.metadata,
          created_at: s.inserted_at
        }
      )

    samples =
      query
      |> Repo.all()
      |> Enum.map(&to_dto(&1, opts))

    {:ok, samples}
  rescue
    e in Postgrex.Error ->
      require Logger
      Logger.error("Forge DB batch query failed: #{inspect(e)}")
      {:error, :forge_unavailable}
  end

  @impl true
  def verify_sample_exists(sample_id) do
    schema = forge_schema()

    query =
      from(s in fragment("?.samples", literal(^schema)),
        where: s.id == ^sample_id,
        select: 1
      )

    Repo.exists?(query)
  rescue
    _ -> false
  end

  @impl true
  def fetch_sample_version(sample_id) do
    schema = forge_schema()

    query =
      from(s in fragment("?.samples", literal(^schema)),
        where: s.id == ^sample_id,
        select: s.version_tag
      )

    case Repo.one(query) do
      nil -> {:error, :not_found}
      version -> {:ok, version}
    end
  rescue
    _ -> {:error, :forge_unavailable}
  end

  # Private helpers

  defp to_dto(sample_row, _opts) do
    %SampleDTO{
      id: sample_row.id,
      content: sample_row.content,
      version: sample_row.version_tag,
      metadata: sample_row.metadata || %{},
      asset_urls: extract_asset_urls(sample_row.metadata),
      source: extract_source(sample_row.metadata),
      created_at: sample_row.created_at
    }
  end

  defp extract_asset_urls(%{"asset_keys" => keys}) when is_list(keys) do
    # In production, generate pre-signed S3 URLs
    # For now, return placeholder URLs
    Enum.map(keys, fn key ->
      "https://forge-assets.example.com/#{key}"
    end)
  end

  defp extract_asset_urls(_), do: []

  defp extract_source(%{"source" => source}), do: source
  defp extract_source(_), do: nil

  defp forge_schema do
    Application.get_env(:anvil, :forge_schema, "forge")
  end
end
</file>

<file path="anvil/forge_bridge/http.ex">
defmodule Anvil.ForgeBridge.HTTP do
  @moduledoc """
  HTTP API client implementation for ForgeBridge.

  Fetches samples from Forge via REST API. Best for microservices deployments
  where Anvil and Forge are separate services with independent databases.

  ## Configuration

      # config/prod.exs
      config :anvil,
        forge_bridge_backend: Anvil.ForgeBridge.HTTP,
        forge_base_url: "https://forge.nsai.example.com",
        forge_api_token: System.fetch_env!("FORGE_API_TOKEN"),
        forge_timeout: 5_000

  ## Circuit Breaker

  Uses Fuse for circuit breaking to fail fast when Forge is unavailable:
  - 5 failures in 10s window → open circuit for 30s
  - Prevents cascading failures
  - Graceful degradation

  ## API Contract

      GET /api/samples/:id
      Authorization: Bearer {token}

      Response:
      {
        "id": "uuid",
        "content": {...},
        "version_tag": "v2024-12-01",
        "metadata": {...}
      }
  """

  @behaviour Anvil.ForgeBridge

  alias Anvil.ForgeBridge.SampleDTO
  require Logger

  @fuse_name :forge_api_http
  @fuse_opts {{:standard, 5, 10_000}, {:reset, 30_000}}

  @impl true
  def fetch_sample(sample_id, opts \\ []) do
    with :ok <- check_circuit_breaker(),
         {:ok, response} <- do_fetch_sample(sample_id, opts) do
      {:ok, response}
    else
      {:error, :circuit_open} ->
        Logger.warning("Forge API circuit breaker open")
        {:error, :forge_unavailable}

      {:error, _} = error ->
        :fuse.melt(@fuse_name)
        error
    end
  end

  @impl true
  def fetch_samples(sample_ids, opts \\ []) when is_list(sample_ids) do
    with :ok <- check_circuit_breaker(),
         {:ok, response} <- do_fetch_samples(sample_ids, opts) do
      {:ok, response}
    else
      {:error, :circuit_open} ->
        Logger.warning("Forge API circuit breaker open")
        {:error, :forge_unavailable}

      {:error, _} = error ->
        :fuse.melt(@fuse_name)
        error
    end
  end

  @impl true
  def verify_sample_exists(sample_id) do
    case fetch_sample(sample_id) do
      {:ok, _} -> true
      {:error, _} -> false
    end
  end

  @impl true
  def fetch_sample_version(sample_id) do
    case fetch_sample(sample_id, include_metadata: false) do
      {:ok, sample} -> {:ok, sample.version}
      {:error, _} = error -> error
    end
  end

  # Private functions

  defp check_circuit_breaker do
    # Initialize fuse if not already present
    case :fuse.ask(@fuse_name, :sync) do
      :ok ->
        :ok

      :blown ->
        {:error, :circuit_open}

      {:error, :not_found} ->
        # Initialize the fuse
        :fuse.install(@fuse_name, @fuse_opts)
        :ok
    end
  end

  defp do_fetch_sample(sample_id, _opts) do
    url = "#{forge_base_url()}/api/samples/#{sample_id}"
    headers = build_headers()
    timeout = forge_timeout()

    case HTTPoison.get(url, headers, recv_timeout: timeout) do
      {:ok, %HTTPoison.Response{status_code: 200, body: body}} ->
        case Jason.decode(body) do
          {:ok, data} -> SampleDTO.from_map(data)
          {:error, _} -> {:error, :invalid_response}
        end

      {:ok, %HTTPoison.Response{status_code: 404}} ->
        {:error, :not_found}

      {:ok, %HTTPoison.Response{status_code: status}} ->
        Logger.error("Forge API returned status #{status}")
        {:error, {:http_error, status}}

      {:error, %HTTPoison.Error{reason: reason}} ->
        Logger.error("Forge API request failed: #{inspect(reason)}")
        {:error, :forge_unavailable}
    end
  end

  defp do_fetch_samples(sample_ids, _opts) do
    url = "#{forge_base_url()}/api/samples"
    headers = build_headers()
    timeout = forge_timeout()

    # Build query string: ?ids[]=uuid1&ids[]=uuid2
    query_params =
      sample_ids
      |> Enum.map(&"ids[]=#{&1}")
      |> Enum.join("&")

    full_url = "#{url}?#{query_params}"

    case HTTPoison.get(full_url, headers, recv_timeout: timeout) do
      {:ok, %HTTPoison.Response{status_code: 200, body: body}} ->
        case Jason.decode(body) do
          {:ok, %{"samples" => samples}} when is_list(samples) ->
            dtos =
              samples
              |> Enum.map(&SampleDTO.from_map/1)
              |> Enum.filter(&match?({:ok, _}, &1))
              |> Enum.map(fn {:ok, dto} -> dto end)

            {:ok, dtos}

          _ ->
            {:error, :invalid_response}
        end

      {:ok, %HTTPoison.Response{status_code: status}} ->
        Logger.error("Forge API batch returned status #{status}")
        {:error, {:http_error, status}}

      {:error, %HTTPoison.Error{reason: reason}} ->
        Logger.error("Forge API batch request failed: #{inspect(reason)}")
        {:error, :forge_unavailable}
    end
  end

  defp build_headers do
    [
      {"Authorization", "Bearer #{forge_api_token()}"},
      {"Content-Type", "application/json"},
      {"Accept", "application/json"}
    ]
  end

  defp forge_base_url do
    Application.fetch_env!(:anvil, :forge_base_url)
  end

  defp forge_api_token do
    Application.fetch_env!(:anvil, :forge_api_token)
  end

  defp forge_timeout do
    Application.get_env(:anvil, :forge_timeout, 5_000)
  end
end
</file>

<file path="anvil/forge_bridge/mock.ex">
defmodule Anvil.ForgeBridge.Mock do
  @moduledoc """
  Mock implementation of ForgeBridge for testing.

  Returns fixture data without requiring Forge to be running. Useful for:
  - Unit tests
  - Integration tests
  - Development without Forge dependency

  ## Configuration

      # config/test.exs
      config :anvil, forge_bridge_backend: Anvil.ForgeBridge.Mock

  ## Behavior

  - `fetch_sample/2` - Returns mock sample with predictable content
  - `fetch_samples/2` - Batch returns mock samples
  - `verify_sample_exists/1` - Always returns true
  - `fetch_sample_version/1` - Returns "mock_v1" version

  Mock samples have content based on their ID for deterministic testing.
  """

  @behaviour Anvil.ForgeBridge

  alias Anvil.ForgeBridge.SampleDTO

  @impl true
  def fetch_sample(sample_id, opts \\ []) do
    # Simulate not found for specific test IDs
    if sample_id == "non-existent-id" do
      {:error, :not_found}
    else
      {:ok, build_mock_sample(sample_id, opts)}
    end
  end

  @impl true
  def fetch_samples(sample_ids, opts \\ []) when is_list(sample_ids) do
    samples =
      sample_ids
      |> Enum.reject(&(&1 == "non-existent-id"))
      |> Enum.map(&build_mock_sample(&1, opts))

    {:ok, samples}
  end

  @impl true
  def verify_sample_exists(sample_id) do
    sample_id != "non-existent-id"
  end

  @impl true
  def fetch_sample_version(sample_id) do
    if sample_id == "non-existent-id" do
      {:error, :not_found}
    else
      {:ok, "mock_v1"}
    end
  end

  # Private helpers

  defp build_mock_sample(sample_id, opts) do
    version = Keyword.get(opts, :version, "mock_v1")

    %SampleDTO{
      id: sample_id,
      content: %{
        "text" => "Mock sample content for #{sample_id}",
        "type" => "text"
      },
      version: version,
      metadata: %{
        "mock" => true,
        "difficulty" => "easy"
      },
      asset_urls: [],
      source: "mock_dataset",
      created_at: DateTime.utc_now()
    }
  end
end
</file>

<file path="anvil/forge_bridge/sample_dto.ex">
defmodule Anvil.ForgeBridge.SampleDTO do
  @moduledoc """
  Data Transfer Object for samples fetched from Forge.

  Isolates Anvil from Forge's internal schema details, allowing independent
  evolution of both systems.

  ## Fields

  - `id` - Sample UUID
  - `content` - Primary sample content (text, JSON, etc.)
  - `version` - Version tag from Forge (e.g., "v2024-12-01" or content hash)
  - `metadata` - Optional metadata map
  - `asset_urls` - Pre-signed URLs for media assets
  - `source` - Source system identifier (e.g., "gsm8k", "human_eval")
  - `created_at` - Sample creation timestamp

  ## Example

      %SampleDTO{
        id: "550e8400-e29b-41d4-a716-446655440000",
        content: %{"text" => "What is 2+2?", "answer" => "4"},
        version: "v2024-12-01-abc123",
        metadata: %{"difficulty" => "easy"},
        asset_urls: [],
        source: "gsm8k",
        created_at: ~U[2024-12-01 10:00:00Z]
      }
  """

  @enforce_keys [:id, :content, :version]
  defstruct [
    :id,
    :content,
    :version,
    :metadata,
    :asset_urls,
    :source,
    :created_at
  ]

  @type t :: %__MODULE__{
          id: binary(),
          content: map() | String.t(),
          version: String.t(),
          metadata: map() | nil,
          asset_urls: [String.t()] | nil,
          source: String.t() | nil,
          created_at: DateTime.t() | nil
        }

  @doc """
  Validates a SampleDTO struct.

  Returns `{:ok, dto}` if valid, `{:error, reason}` otherwise.

  ## Examples

      iex> dto = %SampleDTO{id: "abc", content: "test", version: "v1"}
      iex> SampleDTO.validate(dto)
      {:ok, %SampleDTO{}}

      iex> invalid = %SampleDTO{id: nil, content: "test", version: "v1"}
      iex> SampleDTO.validate(invalid)
      {:error, :missing_id}
  """
  @spec validate(t()) :: {:ok, t()} | {:error, atom()}
  def validate(%__MODULE__{} = dto) do
    cond do
      is_nil(dto.id) or dto.id == "" ->
        {:error, :missing_id}

      is_nil(dto.content) ->
        {:error, :missing_content}

      is_nil(dto.version) or dto.version == "" ->
        {:error, :missing_version}

      true ->
        {:ok, dto}
    end
  end

  @doc """
  Creates a SampleDTO from a map with atom or string keys.

  ## Examples

      iex> SampleDTO.from_map(%{
      ...>   "id" => "abc",
      ...>   "content" => "test",
      ...>   "version" => "v1"
      ...> })
      {:ok, %SampleDTO{id: "abc", content: "test", version: "v1"}}

      iex> SampleDTO.from_map(%{id: "missing content"})
      {:error, :invalid_dto}
  """
  @spec from_map(map()) :: {:ok, t()} | {:error, atom()}
  def from_map(map) when is_map(map) do
    dto = %__MODULE__{
      id: map[:id] || map["id"],
      content: map[:content] || map["content"],
      version: map[:version] || map["version"] || map[:version_tag] || map["version_tag"],
      metadata: map[:metadata] || map["metadata"],
      asset_urls: map[:asset_urls] || map["asset_urls"] || [],
      source: map[:source] || map["source"],
      created_at: map[:created_at] || map["created_at"]
    }

    validate(dto)
  rescue
    _ -> {:error, :invalid_dto}
  end
end
</file>

<file path="anvil/pii/pseudonym.ex">
defmodule Anvil.PII.Pseudonym do
  @moduledoc """
  Labeler pseudonymization for privacy-preserving exports.

  This module generates stable pseudonyms for labelers that:
  - Are consistent within a tenant (same labeler always gets same pseudonym)
  - Are unlinkable across tenants (different pseudonym per tenant)
  - Cannot be reversed to recover the original external_id
  - Are suitable for publication in research datasets

  ## Security Properties

  - Uses HMAC-SHA256 for cryptographically secure hashing
  - Requires a secret key configured at application level
  - Includes tenant_id in hash to prevent cross-tenant linking
  - Truncates hash to 16 characters for readability

  ## Examples

      iex> Anvil.PII.Pseudonym.generate("user123", "tenant456")
      "labeler_a1b2c3d4e5f6g7h8"

      iex> Anvil.PII.Pseudonym.generate("user123", "tenant456")
      "labeler_a1b2c3d4e5f6g7h8"  # Same result (stable)

      iex> Anvil.PII.Pseudonym.generate("user123", "tenant789")
      "labeler_x9y8z7w6v5u4t3s2"  # Different result (tenant-specific)
  """

  @pseudonym_prefix "labeler_"
  @hash_length 16

  @doc """
  Generates a stable pseudonym for a labeler.

  ## Parameters

  - `external_id` - The labeler's external identifier (e.g., OIDC sub claim)
  - `tenant_id` - The tenant ID (optional, defaults to "default")

  ## Returns

  A pseudonym string in the format "labeler_XXXXXXXXXXXXXXXX" where X is a hex digit.

  ## Examples

      iex> Anvil.PII.Pseudonym.generate("user@example.com", "acme-corp")
      "labeler_7a3b9f2c1e4d8a6b"
  """
  @spec generate(String.t(), String.t() | nil) :: String.t()
  def generate(external_id, tenant_id \\ "default") do
    secret = get_secret()
    payload = "#{tenant_id}:#{external_id}"

    hash =
      :crypto.mac(:hmac, :sha256, secret, payload)
      |> Base.encode16(case: :lower)
      |> String.slice(0, @hash_length)

    @pseudonym_prefix <> hash
  end

  @doc """
  Validates that a string is a valid pseudonym format.

  ## Examples

      iex> Anvil.PII.Pseudonym.valid_format?("labeler_a1b2c3d4e5f6g7h8")
      true

      iex> Anvil.PII.Pseudonym.valid_format?("invalid")
      false

      iex> Anvil.PII.Pseudonym.valid_format?("labeler_short")
      false
  """
  @spec valid_format?(String.t()) :: boolean()
  def valid_format?(pseudonym) when is_binary(pseudonym) do
    case String.split_at(pseudonym, String.length(@pseudonym_prefix)) do
      {@pseudonym_prefix, hash} ->
        String.length(hash) == @hash_length && Regex.match?(~r/^[0-9a-f]+$/, hash)

      _ ->
        false
    end
  end

  def valid_format?(_), do: false

  @doc """
  Returns the pseudonym for a labeler, generating one if not present.

  This is the main entry point for ensuring labelers have pseudonyms.
  It will update the labeler record if a pseudonym needs to be generated.

  ## Parameters

  - `labeler` - An `Anvil.Schema.Labeler` struct

  ## Returns

  `{:ok, pseudonym}` or `{:error, reason}`

  ## Examples

      iex> labeler = %Labeler{external_id: "user123", tenant_id: "tenant1", pseudonym: nil}
      iex> Anvil.PII.Pseudonym.ensure_pseudonym(labeler)
      {:ok, "labeler_a1b2c3d4e5f6g7h8"}
  """
  @spec ensure_pseudonym(Anvil.Schema.Labeler.t()) ::
          {:ok, String.t()} | {:error, term()}
  def ensure_pseudonym(%Anvil.Schema.Labeler{pseudonym: pseudonym} = _labeler)
      when not is_nil(pseudonym) do
    {:ok, pseudonym}
  end

  def ensure_pseudonym(%Anvil.Schema.Labeler{} = labeler) do
    pseudonym = generate(labeler.external_id, labeler.tenant_id)

    case Anvil.Repo.update(Ecto.Changeset.change(labeler, %{pseudonym: pseudonym})) do
      {:ok, updated_labeler} -> {:ok, updated_labeler.pseudonym}
      {:error, reason} -> {:error, reason}
    end
  end

  @doc """
  Returns the labeler identifier to use in exports.

  Always returns the pseudonym, never the external_id or internal UUID.
  Generates a pseudonym if one doesn't exist.

  ## Examples

      iex> labeler = %Labeler{pseudonym: "labeler_abc123"}
      iex> Anvil.PII.Pseudonym.export_identifier(labeler)
      {:ok, "labeler_abc123"}
  """
  @spec export_identifier(Anvil.Schema.Labeler.t()) :: {:ok, String.t()} | {:error, term()}
  def export_identifier(labeler) do
    ensure_pseudonym(labeler)
  end

  @doc """
  Rotates the pseudonym secret and regenerates all pseudonyms.

  WARNING: This breaks the linkage with previous exports. Only use when
  required for security purposes (e.g., secret compromise).

  This function should be called manually and will update all labeler records.

  ## Parameters

  - `new_secret` - The new secret to use for pseudonym generation

  ## Returns

  `{:ok, count}` where count is the number of labelers updated, or `{:error, reason}`
  """
  @spec rotate_secret(String.t()) :: {:ok, non_neg_integer()} | {:error, term()}
  def rotate_secret(new_secret) when byte_size(new_secret) >= 32 do
    # Update application config
    Application.put_env(:anvil, :pseudonym_secret, new_secret)

    # Regenerate all pseudonyms
    labelers = Anvil.Repo.all(Anvil.Schema.Labeler)

    results =
      Enum.map(labelers, fn labeler ->
        new_pseudonym = generate(labeler.external_id, labeler.tenant_id)

        Anvil.Repo.update(Ecto.Changeset.change(labeler, %{pseudonym: new_pseudonym}))
      end)

    success_count =
      Enum.count(results, fn
        {:ok, _} -> true
        {:error, _} -> false
      end)

    {:ok, success_count}
  end

  def rotate_secret(_), do: {:error, :secret_too_short}

  # Private functions

  defp get_secret do
    case Application.get_env(:anvil, :pseudonym_secret) do
      nil ->
        # Generate a default secret for development/testing
        # In production, this should be configured explicitly
        "anvil_default_pseudonym_secret_DO_NOT_USE_IN_PRODUCTION"

      secret ->
        secret
    end
  end
end
</file>

<file path="anvil/pii/redactor.ex">
defmodule Anvil.PII.Redactor do
  @moduledoc """
  Redaction strategies for PII fields during export.

  This module applies various redaction policies to field values to protect
  sensitive information while preserving analytical value.

  ## Redaction Strategies

  - `:preserve` - Keep field unchanged (explicit opt-in)
  - `:strip` - Remove field entirely (returns nil)
  - `:truncate` - Truncate to first N characters
  - `:hash` - Hash value (preserves uniqueness for grouping)
  - `:regex_redact` - Apply regex-based redaction patterns

  ## Examples

      iex> Anvil.PII.Redactor.redact("sensitive text", :strip)
      nil

      iex> Anvil.PII.Redactor.redact("long text here", :truncate, max_length: 10)
      "long text "

      iex> Anvil.PII.Redactor.redact("test@example.com", :hash)
      "a7b2c3..." # SHA256 hash
  """

  @default_truncate_length 100

  defp default_pii_patterns do
    [
      {~r/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/, "[EMAIL_REDACTED]"},
      {~r/\b\d{3}-\d{2}-\d{4}\b/, "[SSN_REDACTED]"},
      {~r/\b\d{3}-\d{3}-\d{4}\b/, "[PHONE_REDACTED]"},
      {~r/\b(?:\d{4}[-\s]?){3}\d{4}\b/, "[CREDIT_CARD_REDACTED]"}
    ]
  end

  @type redaction_policy :: :preserve | :strip | :truncate | :hash | :regex_redact
  @type redaction_opts :: keyword()

  @doc """
  Applies a redaction policy to a field value.

  ## Options

  - `:max_length` - Maximum length for `:truncate` policy (default: 100)
  - `:patterns` - List of {regex, replacement} tuples for `:regex_redact` (default: common PII patterns)
  - `:salt` - Salt for hashing (default: nil)

  ## Examples

      iex> Anvil.PII.Redactor.redact("value", :preserve)
      "value"

      iex> Anvil.PII.Redactor.redact("value", :strip)
      nil

      iex> Anvil.PII.Redactor.redact("long text", :truncate, max_length: 4)
      "long"

      iex> Anvil.PII.Redactor.redact("test", :hash)
      "9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08"
  """
  @spec redact(any(), redaction_policy(), redaction_opts()) :: any()
  def redact(value, policy, opts \\ [])

  def redact(nil, _policy, _opts), do: nil

  def redact(value, :preserve, _opts), do: value

  def redact(_value, :strip, _opts), do: nil

  def redact(value, :truncate, opts) when is_binary(value) do
    max_length = Keyword.get(opts, :max_length, @default_truncate_length)
    String.slice(value, 0, max_length)
  end

  def redact(value, :truncate, _opts), do: value

  def redact(value, :hash, opts) when is_binary(value) do
    salt = Keyword.get(opts, :salt, "")
    salted_value = salt <> value

    :crypto.hash(:sha256, salted_value)
    |> Base.encode16(case: :lower)
  end

  def redact(value, :hash, opts) do
    value
    |> to_string()
    |> redact(:hash, opts)
  end

  def redact(value, :regex_redact, opts) when is_binary(value) do
    patterns = Keyword.get(opts, :patterns, default_pii_patterns())

    Enum.reduce(patterns, value, fn {pattern, replacement}, acc ->
      Regex.replace(pattern, acc, replacement)
    end)
  end

  def redact(value, :regex_redact, _opts), do: value

  @doc """
  Redacts a map of field values based on field metadata.

  ## Parameters

  - `payload` - Map of field names to values
  - `field_metadata_map` - Map of field names to metadata maps
  - `redaction_mode` - Redaction mode (`:none`, `:automatic`, `:aggressive`)
  - `opts` - Additional options passed to redaction functions

  ## Examples

      iex> payload = %{"name" => "John", "age" => 30}
      iex> metadata = %{
      ...>   "name" => %{pii: :definite, redaction_policy: :strip},
      ...>   "age" => %{pii: :none}
      ...> }
      iex> Anvil.PII.Redactor.redact_payload(payload, metadata, :automatic)
      %{"age" => 30}
  """
  @spec redact_payload(map(), map(), :none | :automatic | :aggressive, keyword()) :: map()
  def redact_payload(payload, field_metadata_map, redaction_mode, opts \\ [])

  def redact_payload(payload, _field_metadata_map, :none, _opts), do: payload

  def redact_payload(payload, field_metadata_map, redaction_mode, opts)
      when redaction_mode in [:automatic, :aggressive] do
    Enum.reduce(payload, %{}, fn {field_name, value}, acc ->
      field_metadata = Map.get(field_metadata_map, field_name, %{})

      if Anvil.PII.should_redact?(field_metadata, redaction_mode) do
        policy = Anvil.PII.redaction_policy(field_metadata)
        redacted_value = redact(value, policy, opts)

        # Only include field if not stripped
        if redacted_value == nil do
          acc
        else
          Map.put(acc, field_name, redacted_value)
        end
      else
        Map.put(acc, field_name, value)
      end
    end)
  end

  @doc """
  Detects potential PII in a string value using regex patterns.

  Returns a list of detected PII types.

  ## Examples

      iex> Anvil.PII.Redactor.detect_pii("Contact me at test@example.com")
      [:email]

      iex> Anvil.PII.Redactor.detect_pii("Call 555-123-4567")
      [:phone]

      iex> Anvil.PII.Redactor.detect_pii("No PII here")
      []
  """
  @spec detect_pii(String.t()) :: [atom()]
  def detect_pii(value) when is_binary(value) do
    patterns = [
      {:email, ~r/\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/},
      {:ssn, ~r/\b\d{3}-\d{2}-\d{4}\b/},
      {:phone, ~r/\b\d{3}-\d{3}-\d{4}\b/},
      {:credit_card, ~r/\b(?:\d{4}[-\s]?){3}\d{4}\b/}
    ]

    Enum.reduce(patterns, [], fn {type, pattern}, acc ->
      if Regex.match?(pattern, value) do
        [type | acc]
      else
        acc
      end
    end)
    |> Enum.reverse()
  end

  def detect_pii(_value), do: []

  @doc """
  Returns the default PII detection patterns.

  Each pattern is a tuple of `{regex, replacement_text}`.
  """
  @spec default_patterns() :: [{Regex.t(), String.t()}]
  def default_patterns, do: default_pii_patterns()
end
</file>

<file path="anvil/pii/retention.ex">
defmodule Anvil.PII.Retention do
  @moduledoc """
  Retention policy enforcement for PII fields.

  This module provides functions for identifying expired labels and applying
  retention actions (soft delete, hard delete, or field-level redaction).

  ## Retention Actions

  - `:soft_delete` - Tombstone: keep metadata, strip payload
  - `:hard_delete` - Permanent deletion (breaks reproducibility)
  - `:field_redaction` - Redact only expired fields, keep unexpired

  ## Examples

      iex> schema_def = %{
      ...>   fields: [
      ...>     %{name: "notes", pii: :possible, retention_days: 90}
      ...>   ]
      ...> }
      iex> label = %{submitted_at: ~U[2024-01-01 00:00:00Z], payload: %{"notes" => "test"}}
      iex> now = ~U[2025-01-01 00:00:00Z]
      iex> Anvil.PII.Retention.has_expired_fields?(schema_def, label, now)
      true
  """

  import Ecto.Query
  alias Anvil.Repo
  alias Anvil.Schema.{Label, SchemaVersion}

  @type retention_action :: :soft_delete | :hard_delete | :field_redaction

  @doc """
  Finds labels with expired PII fields based on retention policies.

  Returns a list of labels where at least one field has exceeded its retention period.

  ## Options

  - `:now` - Current time for expiration check (default: DateTime.utc_now())
  - `:queue_id` - Filter by specific queue
  - `:limit` - Limit number of results

  ## Examples

      iex> labels = Anvil.PII.Retention.find_expired_labels()
      [%Label{...}, ...]
  """
  @spec find_expired_labels(keyword()) :: [Label.t()]
  def find_expired_labels(opts \\ []) do
    now = Keyword.get(opts, :now, DateTime.utc_now())
    queue_id = Keyword.get(opts, :queue_id)
    limit = Keyword.get(opts, :limit)

    query =
      from(l in Label,
        join: sv in SchemaVersion,
        on: l.schema_version_id == sv.id,
        where: not is_nil(l.submitted_at),
        where: is_nil(l.deleted_at),
        select: %{
          id: l.id,
          assignment_id: l.assignment_id,
          labeler_id: l.labeler_id,
          schema_version_id: l.schema_version_id,
          payload: l.payload,
          submitted_at: l.submitted_at,
          schema_definition: sv.schema_definition
        }
      )

    query =
      if queue_id do
        from([l, sv] in query,
          where: sv.queue_id == ^queue_id
        )
      else
        query
      end

    query =
      if limit do
        from(q in query, limit: ^limit)
      else
        query
      end

    # Post-filter in application code since we need to evaluate field-level retention
    # This is less efficient but necessary for complex retention logic
    query
    |> Repo.all()
    |> Enum.filter(fn label ->
      has_expired_fields?(label.schema_definition, label, now)
    end)
    |> then(fn results ->
      # Convert back to Label structs
      Enum.map(results, fn result ->
        %Label{
          id: result.id,
          assignment_id: result.assignment_id,
          labeler_id: result.labeler_id,
          schema_version_id: result.schema_version_id,
          payload: result.payload,
          submitted_at: result.submitted_at
        }
      end)
    end)
  end

  @doc """
  Checks if a label has any expired fields based on schema definition.

  ## Examples

      iex> schema_def = %{fields: [%{name: "notes", metadata: %{retention_days: 90}}]}
      iex> label = %{submitted_at: ~U[2024-01-01 00:00:00Z], payload: %{"notes" => "test"}}
      iex> now = ~U[2025-01-01 00:00:00Z]
      iex> Anvil.PII.Retention.has_expired_fields?(schema_def, label, now)
      true
  """
  @spec has_expired_fields?(map(), map(), DateTime.t()) :: boolean()
  def has_expired_fields?(schema_definition, label, now \\ DateTime.utc_now()) do
    field_metadata_map = extract_field_metadata(schema_definition)

    Enum.any?(Map.keys(label.payload || %{}), fn field_name ->
      field_metadata = Map.get(field_metadata_map, field_name, %{})
      Anvil.PII.expired?(field_metadata, label.submitted_at, now)
    end)
  end

  @doc """
  Applies retention action to a label.

  ## Parameters

  - `label` - The label to process
  - `action` - Retention action to apply (`:soft_delete`, `:hard_delete`, `:field_redaction`)
  - `schema_definition` - Schema definition with field metadata
  - `opts` - Additional options

  ## Returns

  `{:ok, label}` on success, `{:error, reason}` on failure.
  """
  @spec apply_retention_action(Label.t(), retention_action(), map(), keyword()) ::
          {:ok, Label.t()} | {:error, term()}
  def apply_retention_action(label, action, schema_definition, opts \\ [])

  def apply_retention_action(label, :hard_delete, _schema_definition, _opts) do
    case Repo.delete(label) do
      {:ok, _deleted} -> {:ok, label}
      {:error, reason} -> {:error, reason}
    end
  end

  def apply_retention_action(label, :soft_delete, _schema_definition, _opts) do
    now = DateTime.utc_now()

    label
    |> Ecto.Changeset.change(%{
      payload: %{},
      deleted_at: now
    })
    |> Repo.update()
  end

  def apply_retention_action(label, :field_redaction, schema_definition, opts) do
    now = Keyword.get(opts, :now, DateTime.utc_now())
    field_metadata_map = extract_field_metadata(schema_definition)

    redacted_payload =
      Enum.reduce(label.payload || %{}, %{}, fn {field_name, value}, acc ->
        field_metadata = Map.get(field_metadata_map, field_name, %{})

        if Anvil.PII.expired?(field_metadata, label.submitted_at, now) do
          # Field is expired, redact it
          acc
        else
          # Field is not expired, keep it
          Map.put(acc, field_name, value)
        end
      end)

    label
    |> Ecto.Changeset.change(%{payload: redacted_payload})
    |> Repo.update()
  end

  @doc """
  Processes a batch of expired labels with the specified retention action.

  Returns `{:ok, count}` where count is the number of labels processed.

  ## Options

  - `:dry_run` - If true, only counts labels without processing (default: false)
  - `:now` - Current time for expiration check (default: DateTime.utc_now())
  - `:action` - Retention action to apply (default: `:field_redaction`)

  ## Examples

      iex> Anvil.PII.Retention.process_expired_labels(queue_id: queue_id, dry_run: true)
      {:ok, 42}

      iex> Anvil.PII.Retention.process_expired_labels(action: :soft_delete)
      {:ok, 15}
  """
  @spec process_expired_labels(keyword()) :: {:ok, non_neg_integer()} | {:error, term()}
  def process_expired_labels(opts \\ []) do
    dry_run = Keyword.get(opts, :dry_run, false)
    action = Keyword.get(opts, :action, :field_redaction)

    expired_labels = find_expired_labels(opts)

    if dry_run do
      {:ok, length(expired_labels)}
    else
      results =
        Enum.map(expired_labels, fn label ->
          # Load full label with schema version
          label_with_schema =
            Repo.get(Label, label.id)
            |> Repo.preload(schema_version: :queue)

          schema_definition = label_with_schema.schema_version.schema_definition

          apply_retention_action(label_with_schema, action, schema_definition, opts)
        end)

      # Count successes
      success_count =
        Enum.count(results, fn
          {:ok, _} -> true
          {:error, _} -> false
        end)

      {:ok, success_count}
    end
  end

  @doc """
  Extracts field metadata from schema definition.

  Returns a map of field names to metadata maps.

  ## Examples

      iex> schema_def = %{
      ...>   fields: [
      ...>     %{name: "notes", metadata: %{pii: :possible, retention_days: 90}}
      ...>   ]
      ...> }
      iex> Anvil.PII.Retention.extract_field_metadata(schema_def)
      %{"notes" => %{pii: :possible, retention_days: 90}}
  """
  @spec extract_field_metadata(map()) :: map()
  def extract_field_metadata(schema_definition) do
    fields = Map.get(schema_definition, :fields, [])

    Enum.reduce(fields, %{}, fn field, acc ->
      field_name = field_name(field)
      metadata = Map.get(field, :metadata, %{})
      Map.put(acc, field_name, metadata)
    end)
  end

  defp field_name(%{name: name}), do: name
  defp field_name(%{"name" => name}), do: name
  defp field_name(_), do: nil
end
</file>

<file path="anvil/queue/policy/composite.ex">
defmodule Anvil.Queue.Policy.Composite do
  @moduledoc """
  Composite policy for chaining multiple policies together.

  Allows combining multiple policies with filters and selectors.
  For example: filter by expertise, then select with redundancy logic.
  """

  @behaviour Anvil.Queue.Policy

  @impl true
  def init(config) do
    policies = config[:policies] || []

    # Initialize each policy
    policy_states =
      Enum.map(policies, fn
        {policy_type, policy_config} ->
          module = policy_module(policy_type)
          {:ok, state} = module.init(policy_config)
          {policy_type, module, state}

        policy_type when is_atom(policy_type) ->
          module = policy_module(policy_type)
          {:ok, state} = module.init(%{})
          {policy_type, module, state}
      end)

    {:ok, %{policies: policy_states}}
  end

  @impl true
  def next_assignment(state, labeler_id, available_samples) do
    # Apply each policy in sequence
    Enum.reduce_while(state.policies, {:ok, available_samples}, fn
      {_type, module, policy_state}, {:ok, samples} ->
        case module.next_assignment(policy_state, labeler_id, samples) do
          {:ok, sample} ->
            # Last policy wins - return the selected sample
            {:halt, {:ok, sample}}

          {:error, reason} ->
            # If a policy fails, stop the chain
            {:halt, {:error, reason}}
        end
    end)
  end

  @impl true
  def update_state(state, sample) do
    # Update all policy states
    updated_policies =
      Enum.map(state.policies, fn {type, module, policy_state} ->
        updated_policy_state = module.update_state(policy_state, sample)
        {type, module, updated_policy_state}
      end)

    %{state | policies: updated_policies}
  end

  defp policy_module(:round_robin), do: Anvil.Queue.Policy.RoundRobin
  defp policy_module(:random), do: Anvil.Queue.Policy.Random
  defp policy_module(:expertise), do: Anvil.Queue.Policy.WeightedExpertise
  defp policy_module(:weighted_expertise), do: Anvil.Queue.Policy.WeightedExpertise
  defp policy_module(:redundancy), do: Anvil.Queue.Policy.Redundancy
  defp policy_module(module) when is_atom(module), do: module
end
</file>

<file path="anvil/queue/policy/random.ex">
defmodule Anvil.Queue.Policy.Random do
  @moduledoc """
  Random assignment policy.

  Selects a random sample from available samples.
  """

  @behaviour Anvil.Queue.Policy

  @impl true
  def init(_config) do
    {:ok, %{}}
  end

  @impl true
  def next_assignment(_state, _labeler_id, available_samples) do
    case available_samples do
      [] ->
        {:error, :no_samples_available}

      samples ->
        sample = Enum.random(samples)
        {:ok, sample}
    end
  end

  @impl true
  def update_state(state, _sample) do
    state
  end
end
</file>

<file path="anvil/queue/policy/redundancy.ex">
defmodule Anvil.Queue.Policy.Redundancy do
  @moduledoc """
  Redundancy assignment policy.

  Ensures k independent labels per sample for inter-rater reliability.
  Tracks label counts and prioritizes under-labeled samples.
  """

  @behaviour Anvil.Queue.Policy

  @impl true
  def init(config) do
    {:ok,
     %{
       labels_per_sample: config[:labels_per_sample] || 3,
       allow_same_labeler: config[:allow_same_labeler] || false,
       label_counts: config[:label_counts] || %{}
     }}
  end

  @impl true
  def next_assignment(state, labeler_id, available_samples) do
    target = state.labels_per_sample

    # Filter samples that need more labels
    candidates =
      available_samples
      |> Enum.filter(fn sample ->
        count = Map.get(state.label_counts, sample.id, 0)
        count < target
      end)

    # If not allowing same labeler, filter out samples already labeled by this labeler
    candidates =
      if Map.get(state, :allow_same_labeler, false) do
        candidates
      else
        Enum.filter(candidates, fn sample ->
          labelers = Map.get(state.label_counts, "#{sample.id}_labelers", [])
          labeler_id not in labelers
        end)
      end

    case candidates do
      [] ->
        {:error, :no_samples_available}

      samples ->
        # Prioritize samples with fewest labels
        sample =
          samples
          |> Enum.min_by(fn s ->
            Map.get(state.label_counts, s.id, 0)
          end)

        {:ok, sample}
    end
  end

  @impl true
  def update_state(state, _sample) do
    # No state updates needed - the queue tracks label counts via storage
    state
  end

  @doc """
  Updates the label count for a sample with a specific labeler.
  """
  def record_label(state, sample_id, labeler_id) do
    count = Map.get(state.label_counts, sample_id, 0)
    labelers = Map.get(state.label_counts, "#{sample_id}_labelers", [])

    label_counts =
      state.label_counts
      |> Map.put(sample_id, count + 1)
      |> Map.put("#{sample_id}_labelers", [labeler_id | labelers])

    %{state | label_counts: label_counts}
  end
end
</file>

<file path="anvil/queue/policy/round_robin.ex">
defmodule Anvil.Queue.Policy.RoundRobin do
  @moduledoc """
  Round-robin assignment policy.

  Cycles through samples in creation order, ensuring fair distribution.
  """

  @behaviour Anvil.Queue.Policy

  @impl true
  def init(_config) do
    {:ok, %{last_index: 0}}
  end

  @impl true
  def next_assignment(state, _labeler_id, available_samples) do
    case available_samples do
      [] ->
        {:error, :no_samples_available}

      samples ->
        # Calculate the next index, wrapping around if necessary
        last_index = Map.get(state, :last_index, 0)
        index = rem(last_index, length(samples))
        sample = Enum.at(samples, index)
        {:ok, sample}
    end
  end

  @impl true
  def update_state(state, _sample) do
    last_index = Map.get(state, :last_index, 0)
    Map.put(state, :last_index, last_index + 1)
  end
end
</file>

<file path="anvil/queue/policy/weighted_expertise.ex">
defmodule Anvil.Queue.Policy.WeightedExpertise do
  @moduledoc """
  Weighted expertise assignment policy.

  Routes samples based on labeler skill levels and sample difficulty.
  Only assigns to labelers meeting minimum expertise threshold.
  """

  @behaviour Anvil.Queue.Policy

  @impl true
  def init(config) do
    {:ok,
     %{
       expertise_scores: config[:expertise_scores] || %{},
       min_expertise: config[:min_expertise] || 0.0,
       difficulty_field: config[:difficulty_field] || :difficulty
     }}
  end

  @impl true
  def next_assignment(state, labeler_id, available_samples) do
    labeler_expertise = Map.get(state.expertise_scores, labeler_id, 0.5)

    cond do
      labeler_expertise < state.min_expertise ->
        {:error, :labeler_below_threshold}

      available_samples == [] ->
        {:error, :no_samples_available}

      true ->
        # Select sample with appropriate difficulty
        # Prefer samples where labeler expertise is close to difficulty
        sample =
          available_samples
          |> Enum.map(fn s ->
            difficulty = get_difficulty(s, state.difficulty_field)
            # Score is better when expertise is closer to difficulty
            # but slightly favors easier samples
            score = labeler_expertise - difficulty
            {s, score}
          end)
          |> Enum.max_by(fn {_s, score} -> score end, fn -> {nil, 0} end)
          |> elem(0)

        if sample do
          {:ok, sample}
        else
          {:error, :no_samples_available}
        end
    end
  end

  @impl true
  def update_state(state, _sample) do
    state
  end

  defp get_difficulty(sample, field) do
    case Map.get(sample, field) do
      nil -> 0.5
      :easy -> 0.3
      :medium -> 0.5
      :hard -> 0.8
      value when is_number(value) -> value
      _ -> 0.5
    end
  end
end
</file>

<file path="anvil/queue/policy.ex">
defmodule Anvil.Queue.Policy do
  @moduledoc """
  Behaviour for queue assignment policies.

  Policies control how samples are assigned to labelers.
  """

  @type policy_state :: map()
  @type sample :: map()

  @callback init(config :: map()) :: {:ok, policy_state()}

  @callback next_assignment(
              policy_state(),
              labeler_id :: String.t(),
              available_samples :: [sample()]
            ) :: {:ok, sample()} | {:error, atom()}

  @callback update_state(policy_state(), sample()) :: policy_state()

  @doc """
  Initializes policy state based on the policy type.

  Dispatches to the appropriate policy module.
  """
  @spec init_policy(atom() | module(), map()) :: {:ok, map()}
  def init_policy(:round_robin, config) do
    Anvil.Queue.Policy.RoundRobin.init(config)
  end

  def init_policy(:random, config) do
    Anvil.Queue.Policy.Random.init(config)
  end

  def init_policy(:expertise, config) do
    Anvil.Queue.Policy.WeightedExpertise.init(config)
  end

  def init_policy(:redundancy, config) do
    Anvil.Queue.Policy.Redundancy.init(config)
  end

  def init_policy(module, config) when is_atom(module) do
    module.init(config)
  end

  @doc """
  Returns the next sample for a labeler based on the policy.

  Dispatches to the appropriate policy module.
  """
  @spec next_sample(atom() | module(), map(), String.t(), [map()]) ::
          {:ok, map()} | {:error, atom()}
  def next_sample(:round_robin, state, labeler_id, available_samples) do
    Anvil.Queue.Policy.RoundRobin.next_assignment(state, labeler_id, available_samples)
  end

  def next_sample(:random, state, labeler_id, available_samples) do
    Anvil.Queue.Policy.Random.next_assignment(state, labeler_id, available_samples)
  end

  def next_sample(:expertise, state, labeler_id, available_samples) do
    Anvil.Queue.Policy.WeightedExpertise.next_assignment(state, labeler_id, available_samples)
  end

  def next_sample(:redundancy, state, labeler_id, available_samples) do
    Anvil.Queue.Policy.Redundancy.next_assignment(state, labeler_id, available_samples)
  end

  def next_sample({module, _config}, state, labeler_id, available_samples)
      when is_atom(module) do
    module.next_assignment(state, labeler_id, available_samples)
  end

  def next_sample(module, state, labeler_id, available_samples) when is_atom(module) do
    module.next_assignment(state, labeler_id, available_samples)
  end

  @doc """
  Updates the policy state after an assignment.
  """
  @spec update_state(atom() | module(), map(), map()) :: map()
  def update_state(:round_robin, state, sample) do
    Anvil.Queue.Policy.RoundRobin.update_state(state, sample)
  end

  def update_state(:random, state, sample) do
    Anvil.Queue.Policy.Random.update_state(state, sample)
  end

  def update_state(:expertise, state, sample) do
    Anvil.Queue.Policy.WeightedExpertise.update_state(state, sample)
  end

  def update_state(:redundancy, state, sample) do
    Anvil.Queue.Policy.Redundancy.update_state(state, sample)
  end

  def update_state({module, _config}, state, sample) when is_atom(module) do
    module.update_state(state, sample)
  end

  def update_state(module, state, sample) when is_atom(module) do
    module.update_state(state, sample)
  end

  def update_state(_policy, state, _sample), do: state
end
</file>

<file path="anvil/schema/assignment.ex">
defmodule Anvil.Schema.Assignment do
  @moduledoc """
  Ecto schema for labeling assignments.

  Tracks the lifecycle of a sample assigned to a labeler, including
  status transitions, deadlines, and optimistic locking.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type t :: %__MODULE__{
          id: Ecto.UUID.t() | nil,
          sample_id: Ecto.UUID.t() | nil,
          status: :pending | :reserved | :completed | :timed_out | :requeued | :skipped,
          reserved_at: DateTime.t() | nil,
          deadline: DateTime.t() | nil,
          timeout_seconds: integer() | nil,
          version: integer(),
          requeue_attempts: integer(),
          requeue_delay_until: DateTime.t() | nil,
          skip_reason: String.t() | nil,
          queue_id: Ecto.UUID.t() | nil,
          labeler_id: Ecto.UUID.t() | nil,
          queue: Anvil.Schema.Queue.t() | Ecto.Association.NotLoaded.t() | nil,
          labeler: Anvil.Schema.Labeler.t() | Ecto.Association.NotLoaded.t() | nil,
          labels: [Anvil.Schema.Label.t()] | Ecto.Association.NotLoaded.t(),
          inserted_at: DateTime.t() | nil,
          updated_at: DateTime.t() | nil
        }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "assignments" do
    field(:sample_id, :binary_id)

    field(:status, Ecto.Enum,
      values: [:pending, :reserved, :completed, :timed_out, :requeued, :skipped],
      default: :pending
    )

    field(:reserved_at, :utc_datetime)
    field(:deadline, :utc_datetime)
    field(:timeout_seconds, :integer)
    field(:version, :integer, default: 1)
    field(:requeue_attempts, :integer, default: 0)
    field(:requeue_delay_until, :utc_datetime)
    field(:skip_reason, :string)

    belongs_to(:queue, Anvil.Schema.Queue)
    belongs_to(:labeler, Anvil.Schema.Labeler)
    has_many(:labels, Anvil.Schema.Label)

    timestamps(type: :utc_datetime)
  end

  @doc false
  def changeset(assignment, attrs) do
    assignment
    |> cast(attrs, [
      :id,
      :queue_id,
      :sample_id,
      :labeler_id,
      :status,
      :reserved_at,
      :deadline,
      :timeout_seconds,
      :requeue_attempts,
      :requeue_delay_until,
      :skip_reason
    ])
    |> validate_required([:queue_id, :sample_id, :labeler_id])
    |> foreign_key_constraint(:queue_id)
    |> foreign_key_constraint(:labeler_id)
    |> foreign_key_constraint(:sample_id)
    |> optimistic_lock(:version)
  end

  @doc """
  Reserves an assignment for a labeler with a timeout.
  """
  def reserve(assignment, timeout_seconds) do
    now = DateTime.utc_now()
    deadline = DateTime.add(now, timeout_seconds, :second)

    assignment
    |> change(%{
      status: :reserved,
      reserved_at: now,
      deadline: deadline,
      timeout_seconds: timeout_seconds
    })
  end

  @doc """
  Marks an assignment as completed.
  """
  def complete(assignment) do
    assignment
    |> change(%{status: :completed})
  end

  @doc """
  Marks an assignment as timed out.
  """
  def timeout(assignment) do
    assignment
    |> change(%{status: :timed_out})
  end

  @doc """
  Requeues an assignment after timeout.
  """
  def requeue(assignment, delay_seconds \\ 0) do
    requeue_delay_until =
      if delay_seconds > 0 do
        DateTime.add(DateTime.utc_now(), delay_seconds, :second)
      else
        nil
      end

    assignment
    |> change(%{
      status: :requeued,
      requeue_attempts: (assignment.requeue_attempts || 0) + 1,
      requeue_delay_until: requeue_delay_until,
      reserved_at: nil,
      deadline: nil
    })
  end

  @doc """
  Marks an assignment as skipped.
  """
  def skip(assignment, reason \\ nil) do
    assignment
    |> change(%{
      status: :skipped,
      skip_reason: reason
    })
  end
end
</file>

<file path="anvil/schema/audit_log.ex">
defmodule Anvil.Schema.AuditLog do
  @moduledoc """
  Ecto schema for audit trail.

  Provides immutable record of all operations for compliance and debugging.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "audit_logs" do
    field(:tenant_id, :binary_id)

    field(:entity_type, Ecto.Enum,
      values: [:queue, :assignment, :label, :labeler, :schema_version]
    )

    field(:entity_id, :binary_id)
    field(:action, Ecto.Enum, values: [:created, :updated, :deleted, :accessed])

    field(:actor_id, :binary_id)
    field(:metadata, :map, default: %{})
    field(:occurred_at, :utc_datetime)

    timestamps(type: :utc_datetime, updated_at: false)
  end

  @doc false
  def changeset(audit_log, attrs) do
    audit_log
    |> cast(attrs, [
      :tenant_id,
      :entity_type,
      :entity_id,
      :action,
      :actor_id,
      :metadata,
      :occurred_at
    ])
    |> validate_required([:entity_type, :entity_id, :action])
    |> put_change(:occurred_at, DateTime.utc_now())
  end
end
</file>

<file path="anvil/schema/field.ex">
defmodule Anvil.Schema.Field do
  @moduledoc """
  Represents a field in a label schema.

  Supports various field types with appropriate validation rules.
  """

  @type field_type ::
          :text
          | :select
          | :multiselect
          | :range
          | :number
          | :boolean
          | :date
          | :datetime

  @type t :: %__MODULE__{
          name: String.t(),
          type: field_type(),
          required: boolean(),
          options: [String.t()] | nil,
          min: number() | nil,
          max: number() | nil,
          pattern: Regex.t() | nil,
          default: any(),
          description: String.t() | nil,
          metadata: map()
        }

  defstruct [
    :name,
    :type,
    :required,
    :options,
    :min,
    :max,
    :pattern,
    :default,
    :description,
    metadata: %{}
  ]

  @doc """
  Returns a list of all supported field types.
  """
  @spec types() :: [field_type()]
  def types do
    [:text, :select, :multiselect, :range, :number, :boolean, :date, :datetime]
  end

  @doc """
  Validates a value against this field's constraints.
  """
  @spec validate(t(), any()) :: :ok | {:error, String.t()}
  def validate(%__MODULE__{required: true}, nil) do
    {:error, "is required"}
  end

  def validate(%__MODULE__{required: false}, nil), do: :ok

  def validate(%__MODULE__{type: :text, pattern: pattern}, value) when is_binary(value) do
    if pattern && !Regex.match?(pattern, value) do
      {:error, "does not match required pattern"}
    else
      :ok
    end
  end

  def validate(%__MODULE__{type: :text}, value) when not is_binary(value) do
    {:error, "must be text"}
  end

  def validate(%__MODULE__{type: :select, options: options}, value) when is_binary(value) do
    if value in options do
      :ok
    else
      {:error, "must be one of: #{Enum.join(options, ", ")}"}
    end
  end

  def validate(%__MODULE__{type: :select}, value) when not is_binary(value) do
    {:error, "must be a string"}
  end

  def validate(%__MODULE__{type: :multiselect, options: options}, values)
      when is_list(values) do
    invalid = Enum.reject(values, &(&1 in options))

    if Enum.empty?(invalid) do
      :ok
    else
      {:error, "invalid options: #{Enum.join(invalid, ", ")}"}
    end
  end

  def validate(%__MODULE__{type: :multiselect}, value) when not is_list(value) do
    {:error, "must be a list"}
  end

  def validate(%__MODULE__{type: :range, min: min, max: max}, value)
      when is_integer(value) do
    cond do
      min && value < min -> {:error, "must be at least #{min}"}
      max && value > max -> {:error, "must be at most #{max}"}
      true -> :ok
    end
  end

  def validate(%__MODULE__{type: :range}, value) when not is_integer(value) do
    {:error, "must be an integer"}
  end

  def validate(%__MODULE__{type: :number, min: min, max: max}, value)
      when is_number(value) do
    cond do
      min && value < min -> {:error, "must be at least #{min}"}
      max && value > max -> {:error, "must be at most #{max}"}
      true -> :ok
    end
  end

  def validate(%__MODULE__{type: :number}, value) when not is_number(value) do
    {:error, "must be a number"}
  end

  def validate(%__MODULE__{type: :boolean}, value) when is_boolean(value), do: :ok

  def validate(%__MODULE__{type: :boolean}, _value) do
    {:error, "must be true or false"}
  end

  def validate(%__MODULE__{type: :date}, %Date{}), do: :ok

  def validate(%__MODULE__{type: :date}, value) when is_binary(value) do
    case Date.from_iso8601(value) do
      {:ok, _} -> :ok
      {:error, _} -> {:error, "must be a valid date (YYYY-MM-DD)"}
    end
  end

  def validate(%__MODULE__{type: :date}, _value) do
    {:error, "must be a date"}
  end

  def validate(%__MODULE__{type: :datetime}, %DateTime{}), do: :ok

  def validate(%__MODULE__{type: :datetime}, value) when is_binary(value) do
    case DateTime.from_iso8601(value) do
      {:ok, _, _} -> :ok
      {:error, _} -> {:error, "must be a valid datetime (ISO8601)"}
    end
  end

  def validate(%__MODULE__{type: :datetime}, _value) do
    {:error, "must be a datetime"}
  end
end
</file>

<file path="anvil/schema/label.ex">
defmodule Anvil.Schema.Label do
  @moduledoc """
  Ecto schema for submitted labels.

  Stores label data validated against schema versions with optional
  blob storage for large attachments.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "labels" do
    field(:payload, :map)
    field(:blob_pointer, :string)
    field(:submitted_at, :utc_datetime)
    field(:deleted_at, :utc_datetime)

    belongs_to(:assignment, Anvil.Schema.Assignment)
    belongs_to(:labeler, Anvil.Schema.Labeler)
    belongs_to(:schema_version, Anvil.Schema.SchemaVersion)

    timestamps(type: :utc_datetime, updated_at: false)
  end

  @type t :: %__MODULE__{
          id: Ecto.UUID.t() | nil,
          payload: map(),
          blob_pointer: String.t() | nil,
          submitted_at: DateTime.t() | nil,
          deleted_at: DateTime.t() | nil,
          assignment_id: Ecto.UUID.t() | nil,
          labeler_id: Ecto.UUID.t() | nil,
          schema_version_id: Ecto.UUID.t() | nil,
          assignment: %Anvil.Schema.Assignment{} | Ecto.Association.NotLoaded.t() | nil,
          labeler: %Anvil.Schema.Labeler{} | Ecto.Association.NotLoaded.t() | nil,
          schema_version: %Anvil.Schema.SchemaVersion{} | Ecto.Association.NotLoaded.t() | nil,
          inserted_at: DateTime.t() | nil
        }

  @doc false
  def changeset(label, attrs) do
    label
    |> cast(attrs, [
      :id,
      :assignment_id,
      :labeler_id,
      :schema_version_id,
      :payload,
      :blob_pointer,
      :submitted_at,
      :deleted_at
    ])
    |> validate_required([:assignment_id, :labeler_id, :schema_version_id, :payload])
    |> foreign_key_constraint(:assignment_id)
    |> foreign_key_constraint(:labeler_id)
    |> foreign_key_constraint(:schema_version_id)
    |> unique_constraint([:assignment_id, :labeler_id])
  end
end
</file>

<file path="anvil/schema/labeler.ex">
defmodule Anvil.Schema.Labeler do
  @moduledoc """
  Ecto schema for labelers (annotators).

  Stores labeler profiles, expertise weights, and access control.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type t :: %__MODULE__{
          id: Ecto.UUID.t() | nil,
          tenant_id: Ecto.UUID.t() | nil,
          external_id: String.t() | nil,
          pseudonym: String.t() | nil,
          expertise_weights: map() | nil,
          blocklisted_queues: [Ecto.UUID.t()],
          max_concurrent_assignments: integer(),
          assignments: [Anvil.Schema.Assignment.t()] | Ecto.Association.NotLoaded.t(),
          inserted_at: DateTime.t() | nil,
          updated_at: DateTime.t() | nil
        }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "labelers" do
    field(:tenant_id, :binary_id)
    field(:external_id, :string)
    field(:pseudonym, :string)
    field(:expertise_weights, :map)
    field(:blocklisted_queues, {:array, :binary_id}, default: [])
    field(:max_concurrent_assignments, :integer, default: 5)

    has_many(:assignments, Anvil.Schema.Assignment)

    timestamps(type: :utc_datetime)
  end

  @doc false
  def changeset(labeler, attrs) do
    labeler
    |> cast(attrs, [
      :tenant_id,
      :external_id,
      :pseudonym,
      :expertise_weights,
      :blocklisted_queues,
      :max_concurrent_assignments
    ])
    |> validate_required([:external_id])
    |> unique_constraint([:tenant_id, :external_id])
    |> validate_number(:max_concurrent_assignments, greater_than: 0)
  end
end
</file>

<file path="anvil/schema/migration.ex">
defmodule Anvil.Schema.Migration do
  @moduledoc """
  Forward-only migrations between schema versions.

  Provides utilities for migrating labels from one schema version to another
  using transform callbacks.
  """

  alias Anvil.Schema.{SchemaVersion, Label}
  alias Anvil.Repo

  @doc """
  Transform callback behaviour for schema migrations.

  Transform modules must implement forward/3 to convert labels from one version to another.
  """
  @callback transform(old_label :: map(), from_version :: integer(), to_version :: integer()) ::
              {:ok, new_label :: map()} | {:error, :incompatible}

  @doc """
  Migrates labels from one schema version to another.

  ## Options

    * `:batch_size` - Number of labels to process per batch (default: 100)
    * `:dry_run` - If true, only validates transformations without creating new labels
    * `:transformer` - Module implementing transform/3 callback

  ## Examples

      iex> migrate_labels(labels, 1, 2, MyApp.Transforms.V1ToV2)
      {:ok, %{migrated: 150, failed: 0}}

      iex> migrate_labels(labels, 1, 2, MyApp.Transforms.V1ToV2, dry_run: true)
      {:ok, %{valid: 150, invalid: 0}}

  """
  @spec migrate_labels(
          [Label.t()],
          non_neg_integer(),
          non_neg_integer(),
          module(),
          keyword()
        ) ::
          {:ok, map()} | {:error, term()}
  def migrate_labels(labels, from_version, to_version, transformer, opts \\ []) do
    dry_run = Keyword.get(opts, :dry_run, false)
    batch_size = Keyword.get(opts, :batch_size, 100)

    results =
      labels
      |> Stream.chunk_every(batch_size)
      |> Enum.reduce(%{migrated: 0, failed: 0, errors: []}, fn batch, acc ->
        batch_results = process_batch(batch, from_version, to_version, transformer, dry_run)
        merge_results(acc, batch_results)
      end)

    {:ok, results}
  end

  defp process_batch(batch, from_version, to_version, transformer, dry_run) do
    Enum.reduce(batch, %{migrated: 0, failed: 0, errors: []}, fn label, acc ->
      case transformer.transform(label.payload, from_version, to_version) do
        {:ok, new_payload} ->
          if dry_run do
            %{acc | migrated: acc.migrated + 1}
          else
            case create_migrated_label(label, new_payload, to_version) do
              {:ok, _new_label} ->
                %{acc | migrated: acc.migrated + 1}

              {:error, reason} ->
                %{
                  acc
                  | failed: acc.failed + 1,
                    errors: [{label.id, reason} | acc.errors]
                }
            end
          end

        {:error, :incompatible} ->
          %{
            acc
            | failed: acc.failed + 1,
              errors: [{label.id, :incompatible_schema} | acc.errors]
          }
      end
    end)
  end

  defp create_migrated_label(old_label, new_payload, to_version) do
    # Find the target schema version
    case Repo.get_by(SchemaVersion,
           queue_id: get_queue_id(old_label),
           version_number: to_version
         ) do
      nil ->
        {:error, :schema_version_not_found}

      schema_version ->
        new_label = %Label{
          assignment_id: old_label.assignment_id,
          labeler_id: old_label.labeler_id,
          schema_version_id: schema_version.id,
          payload: new_payload,
          submitted_at: old_label.submitted_at
        }

        Repo.insert(new_label)
    end
  end

  defp get_queue_id(label) do
    # This would need to fetch the queue_id from the assignment
    # For now, we'll need to preload this relationship
    case label do
      %{assignment: %{queue_id: queue_id}} -> queue_id
      _ -> nil
    end
  end

  defp merge_results(acc, batch_results) do
    %{
      migrated: acc.migrated + batch_results.migrated,
      failed: acc.failed + batch_results.failed,
      errors: acc.errors ++ batch_results.errors
    }
  end

  @doc """
  Validates that a label payload conforms to a schema version.

  Returns {:ok, payload} if valid, {:error, errors} if invalid.
  """
  @spec validate_against_schema(map(), SchemaVersion.t()) ::
          {:ok, map()} | {:error, [term()]}
  def validate_against_schema(label_values, schema_version) do
    # Simple validation - check that all required fields are present
    # In a real implementation, this would use the schema_definition
    # to perform comprehensive validation

    case schema_version.schema_definition do
      %{"required" => required_fields} ->
        missing_fields =
          required_fields
          |> Enum.reject(&Map.has_key?(label_values, &1))

        if Enum.empty?(missing_fields) do
          {:ok, label_values}
        else
          {:error, [{:required_fields_missing, missing_fields}]}
        end

      _ ->
        # No required fields specified, accept payload
        {:ok, label_values}
    end
  end

  @doc """
  Freezes a schema version when the first label is submitted.

  This is typically called automatically via a database trigger or
  application hook.
  """
  @spec freeze_schema_version(binary()) :: {:ok, SchemaVersion.t()} | {:error, term()}
  def freeze_schema_version(schema_version_id) do
    case Repo.get(SchemaVersion, schema_version_id) do
      nil ->
        {:error, :not_found}

      schema_version ->
        if schema_version.frozen_at do
          {:ok, schema_version}
        else
          schema_version
          |> SchemaVersion.freeze()
          |> Repo.update()
        end
    end
  end
end
</file>

<file path="anvil/schema/queue.ex">
defmodule Anvil.Schema.Queue do
  @moduledoc """
  Ecto schema for labeling queues.

  Stores queue configuration including policy settings and schema version.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @type t :: %__MODULE__{
          id: Ecto.UUID.t() | nil,
          tenant_id: Ecto.UUID.t() | nil,
          name: String.t() | nil,
          policy: map() | nil,
          status: :active | :paused | :archived,
          schema_version_id: Ecto.UUID.t() | nil,
          schema_version: Anvil.Schema.SchemaVersion.t() | Ecto.Association.NotLoaded.t() | nil,
          assignments: [Anvil.Schema.Assignment.t()] | Ecto.Association.NotLoaded.t(),
          inserted_at: DateTime.t() | nil,
          updated_at: DateTime.t() | nil
        }

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "queues" do
    field(:tenant_id, :binary_id)
    field(:name, :string)
    field(:policy, :map)
    field(:status, Ecto.Enum, values: [:active, :paused, :archived], default: :active)

    belongs_to(:schema_version, Anvil.Schema.SchemaVersion)
    has_many(:assignments, Anvil.Schema.Assignment)

    timestamps(type: :utc_datetime)
  end

  @doc false
  def changeset(queue, attrs) do
    queue
    |> cast(attrs, [:tenant_id, :name, :schema_version_id, :policy, :status])
    |> validate_required([:name, :schema_version_id, :policy])
    |> unique_constraint([:tenant_id, :name])
  end
end
</file>

<file path="anvil/schema/sample_ref.ex">
defmodule Anvil.Schema.SampleRef do
  @moduledoc """
  Ecto schema for sample references.

  Stores references to samples managed by Forge, supporting either
  foreign key constraints (Option A) or logical references (Option B).
  """

  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  schema "sample_refs" do
    field(:sample_id, :binary_id)
    field(:metadata, :map, default: %{})

    has_many(:assignments, Anvil.Schema.Assignment, foreign_key: :sample_id)

    timestamps(type: :utc_datetime)
  end

  @doc false
  def changeset(sample_ref, attrs) do
    sample_ref
    |> cast(attrs, [:sample_id, :metadata])
    |> validate_required([:sample_id])
    |> unique_constraint(:sample_id)
  end
end
</file>

<file path="anvil/schema/schema_version.ex">
defmodule Anvil.Schema.SchemaVersion do
  @moduledoc """
  Ecto schema for label schema versions.

  Tracks evolution of label schemas with immutability guarantees.
  Once a schema version is frozen (first label written), it becomes read-only.
  """

  use Ecto.Schema
  import Ecto.Changeset

  @primary_key {:id, :binary_id, autogenerate: true}
  @foreign_key_type :binary_id

  @type t :: %__MODULE__{
          id: Ecto.UUID.t(),
          queue_id: Ecto.UUID.t(),
          version_number: integer(),
          schema_definition: map(),
          transform_from_previous: String.t() | nil,
          frozen_at: DateTime.t() | nil,
          label_count: integer(),
          inserted_at: DateTime.t()
        }

  schema "schema_versions" do
    field(:queue_id, :binary_id)
    field(:version_number, :integer)
    field(:schema_definition, :map)
    field(:transform_from_previous, :string)
    field(:frozen_at, :utc_datetime)
    field(:label_count, :integer, default: 0, virtual: true)

    timestamps(type: :utc_datetime, updated_at: false)
  end

  @doc false
  def changeset(schema_version, attrs) do
    schema_version
    |> cast(attrs, [:queue_id, :version_number, :schema_definition, :transform_from_previous])
    |> validate_required([:queue_id, :version_number, :schema_definition])
    |> unique_constraint([:queue_id, :version_number])
    |> validate_number(:version_number, greater_than: 0)
  end

  @doc """
  Checks if schema version can be modified.

  A schema version is mutable if:
  - It has not been frozen (frozen_at is nil)
  - It has no labels (label_count is 0)
  """
  @spec mutable?(t()) :: boolean()
  def mutable?(%__MODULE__{frozen_at: nil, label_count: 0}), do: true
  def mutable?(_), do: false

  @doc """
  Freezes a schema version, making it immutable.
  """
  def freeze(schema_version) do
    if schema_version.frozen_at do
      schema_version
      |> change()
      |> add_error(:frozen_at, "schema is already frozen")
    else
      schema_version
      |> change(%{frozen_at: DateTime.utc_now() |> DateTime.truncate(:second)})
    end
  end
end
</file>

<file path="anvil/storage/ets.ex">
defmodule Anvil.Storage.ETS do
  @moduledoc """
  ETS-based storage implementation for testing and development.
  """

  @behaviour Anvil.Storage

  @impl true
  def init(opts) do
    queue_id = Keyword.fetch!(opts, :queue_id)

    state = %{
      queue_id: queue_id,
      assignments: :ets.new(:"assignments_#{queue_id}", [:set, :public]),
      labels: :ets.new(:"labels_#{queue_id}", [:set, :public]),
      samples: :ets.new(:"samples_#{queue_id}", [:set, :public])
    }

    {:ok, state}
  end

  @impl true
  def put_assignment(state, assignment) do
    :ets.insert(state.assignments, {assignment.id, assignment})
    {:ok, state}
  end

  @impl true
  def get_assignment(state, id) do
    case :ets.lookup(state.assignments, id) do
      [{^id, assignment}] -> {:ok, assignment, state}
      [] -> {:error, :not_found}
    end
  end

  @impl true
  def list_assignments(state, filters) do
    assignments =
      state.assignments
      |> :ets.tab2list()
      |> Enum.map(&elem(&1, 1))
      |> apply_filters(filters)

    {:ok, assignments, state}
  end

  @impl true
  def put_label(state, label) do
    :ets.insert(state.labels, {label.id, label})
    {:ok, state}
  end

  @impl true
  def get_label(state, id) do
    case :ets.lookup(state.labels, id) do
      [{^id, label}] -> {:ok, label, state}
      [] -> {:error, :not_found}
    end
  end

  @impl true
  def list_labels(state, filters) do
    labels =
      state.labels
      |> :ets.tab2list()
      |> Enum.map(&elem(&1, 1))
      |> apply_filters(filters)

    {:ok, labels, state}
  end

  @impl true
  def put_sample(state, sample) do
    :ets.insert(state.samples, {sample.id, sample})
    {:ok, state}
  end

  @impl true
  def get_sample(state, id) do
    case :ets.lookup(state.samples, id) do
      [{^id, sample}] -> {:ok, sample, state}
      [] -> {:error, :not_found}
    end
  end

  @impl true
  def list_samples(state, filters) do
    samples =
      state.samples
      |> :ets.tab2list()
      |> Enum.map(&elem(&1, 1))
      |> apply_filters(filters)

    {:ok, samples, state}
  end

  defp apply_filters(items, []), do: items

  defp apply_filters(items, [{:status, status} | rest]) when is_atom(status) do
    items
    |> Enum.filter(&(&1.status == status))
    |> apply_filters(rest)
  end

  defp apply_filters(items, [{:status, statuses} | rest]) when is_list(statuses) do
    items
    |> Enum.filter(&(&1.status in statuses))
    |> apply_filters(rest)
  end

  defp apply_filters(items, [{:sample_id, sample_id} | rest]) do
    items
    |> Enum.filter(&(&1.sample_id == sample_id))
    |> apply_filters(rest)
  end

  defp apply_filters(items, [{:labeler_id, labeler_id} | rest]) do
    items
    |> Enum.filter(&(&1.labeler_id == labeler_id))
    |> apply_filters(rest)
  end

  defp apply_filters(items, [{:valid?, valid?} | rest]) do
    items
    |> Enum.filter(&(&1.valid? == valid?))
    |> apply_filters(rest)
  end

  defp apply_filters(items, [_ | rest]), do: apply_filters(items, rest)
end
</file>

<file path="anvil/storage/postgres.ex">
defmodule Anvil.Storage.Postgres do
  @moduledoc """
  Postgres storage adapter for Anvil.

  Implements the Anvil.Storage behaviour using Ecto and Postgres for
  durable, scalable storage with multi-tenancy support.
  """

  @behaviour Anvil.Storage

  alias Anvil.Schema.{Assignment, Label, SampleRef}
  alias Anvil.Telemetry
  import Ecto.Query

  @impl true
  def init(opts) do
    repo = Keyword.get(opts, :repo, Anvil.Repo)
    {:ok, %{repo: repo}}
  end

  @impl true
  def put_sample(state, sample) do
    Telemetry.span_storage_query("put_sample", %{}, fn ->
      attrs = %{
        sample_id: sample.id || sample[:id],
        metadata: Map.get(sample, :metadata, %{})
      }

      changeset = SampleRef.changeset(%SampleRef{}, attrs)

      case state.repo.insert(changeset, on_conflict: :nothing) do
        {:ok, _sample_ref} -> {{:ok, state}, %{}}
        {:error, changeset} -> {{:error, {:invalid_sample, changeset}}, %{error: :invalid_sample}}
      end
    end)
  end

  @impl true
  def get_sample(state, id) do
    case state.repo.get_by(SampleRef, sample_id: id) do
      nil -> {:error, :not_found}
      sample_ref -> {:ok, to_sample_map(sample_ref), state}
    end
  end

  @impl true
  def list_samples(state, filters) do
    Telemetry.span_storage_query("list_samples", %{filter_count: length(filters)}, fn ->
      query = from(s in SampleRef)

      query =
        Enum.reduce(filters, query, fn
          {:sample_ids, ids}, q ->
            where(q, [s], s.sample_id in ^ids)

          _, q ->
            q
        end)

      samples =
        state.repo.all(query)
        |> Enum.map(&to_sample_map/1)

      {{:ok, samples, state}, %{row_count: length(samples)}}
    end)
  end

  @impl true
  def put_assignment(state, assignment) do
    # Map the in-memory Assignment struct status to Ecto schema status
    ecto_status = map_status_to_ecto(assignment.status)

    attrs = %{
      id: parse_uuid(assignment.id),
      queue_id: assignment.queue_id,
      sample_id: assignment.sample_id,
      labeler_id: assignment.labeler_id,
      status: ecto_status,
      reserved_at: assignment.started_at,
      deadline: assignment.deadline,
      timeout_seconds: if(assignment.deadline, do: 3600, else: nil),
      skip_reason: assignment.skip_reason
    }

    changeset = Assignment.changeset(%Assignment{}, attrs)

    case state.repo.insert(changeset,
           on_conflict: {:replace_all_except, [:id, :inserted_at]},
           conflict_target: :id
         ) do
      {:ok, _assignment} -> {:ok, state}
      {:error, changeset} -> {:error, {:invalid_assignment, changeset}}
    end
  end

  @impl true
  def get_assignment(state, id) do
    case state.repo.get(Assignment, parse_uuid(id)) do
      nil -> {:error, :not_found}
      assignment -> {:ok, to_assignment_struct(assignment), state}
    end
  end

  @impl true
  def list_assignments(state, filters) do
    Telemetry.span_storage_query("list_assignments", %{filter_count: length(filters)}, fn ->
      query = from(a in Assignment)

      query =
        Enum.reduce(filters, query, fn
          {:queue_id, queue_id}, q ->
            where(q, [a], a.queue_id == ^queue_id)

          {:labeler_id, labeler_id}, q ->
            where(q, [a], a.labeler_id == ^labeler_id)

          {:status, status}, q ->
            where(q, [a], a.status == ^status)

          {:sample_id, sample_id}, q ->
            where(q, [a], a.sample_id == ^sample_id)

          _, q ->
            q
        end)

      assignments =
        state.repo.all(query)
        |> Enum.map(&to_assignment_struct/1)

      {{:ok, assignments, state}, %{row_count: length(assignments)}}
    end)
  end

  @impl true
  def put_label(state, label) do
    attrs = %{
      id: parse_uuid(label.id),
      assignment_id: parse_uuid(label.assignment_id),
      labeler_id: label.labeler_id,
      schema_version_id: get_schema_version_id(state, label),
      payload: label.values,
      submitted_at: label.created_at
    }

    changeset = Label.changeset(%Label{}, attrs)

    case state.repo.insert(changeset) do
      {:ok, _label} -> {:ok, state}
      {:error, changeset} -> {:error, {:invalid_label, changeset}}
    end
  end

  @impl true
  def get_label(state, id) do
    case state.repo.get(Label, parse_uuid(id)) do
      nil -> {:error, :not_found}
      label -> {:ok, to_label_struct(label), state}
    end
  end

  @impl true
  def list_labels(state, filters) do
    Telemetry.span_storage_query("list_labels", %{filter_count: length(filters)}, fn ->
      query = from(l in Label)

      query =
        Enum.reduce(filters, query, fn
          {:assignment_id, assignment_id}, q ->
            where(q, [l], l.assignment_id == ^assignment_id)

          {:labeler_id, labeler_id}, q ->
            where(q, [l], l.labeler_id == ^labeler_id)

          {:sample_id, sample_id}, q ->
            join(q, :inner, [l], a in Assignment, on: l.assignment_id == a.id)
            |> where([l, a], a.sample_id == ^sample_id)

          _, q ->
            q
        end)

      labels =
        state.repo.all(query)
        |> Enum.map(&to_label_struct/1)

      {{:ok, labels, state}, %{row_count: length(labels)}}
    end)
  end

  # Helper functions

  defp to_sample_map(sample_ref) do
    %{
      id: sample_ref.sample_id,
      metadata: sample_ref.metadata
    }
  end

  defp to_assignment_struct(assignment) do
    %Anvil.Assignment{
      id: assignment.id,
      queue_id: assignment.queue_id,
      sample_id: assignment.sample_id,
      labeler_id: assignment.labeler_id,
      status: map_status(assignment.status),
      deadline: assignment.deadline,
      attempts: assignment.requeue_attempts,
      label_id: nil,
      skip_reason: assignment.skip_reason,
      created_at: assignment.inserted_at,
      started_at: assignment.reserved_at,
      completed_at: if(assignment.status == :completed, do: assignment.updated_at, else: nil),
      expired_at: if(assignment.status == :timed_out, do: assignment.updated_at, else: nil),
      skipped_at: if(assignment.status == :skipped, do: assignment.updated_at, else: nil)
    }
  end

  defp to_label_struct(label) do
    %Anvil.Label{
      id: label.id,
      assignment_id: label.assignment_id,
      labeler_id: label.labeler_id,
      sample_id: get_sample_id_from_assignment(label.assignment_id),
      values: label.payload,
      valid?: true,
      errors: [],
      labeling_time_seconds: nil,
      created_at: label.inserted_at
    }
  end

  # Map Anvil.Assignment status to Ecto schema status
  defp map_status_to_ecto(:pending), do: :pending
  defp map_status_to_ecto(:in_progress), do: :reserved
  defp map_status_to_ecto(:completed), do: :completed
  defp map_status_to_ecto(:expired), do: :timed_out
  defp map_status_to_ecto(:skipped), do: :skipped
  defp map_status_to_ecto(status), do: status

  # Map Ecto schema status back to Anvil.Assignment status
  defp map_status(:pending), do: :pending
  defp map_status(:reserved), do: :in_progress
  defp map_status(:completed), do: :completed
  defp map_status(:timed_out), do: :expired
  defp map_status(:skipped), do: :skipped
  defp map_status(:requeued), do: :pending
  defp map_status(status), do: status

  defp parse_uuid(id) when is_binary(id) do
    case Ecto.UUID.cast(id) do
      {:ok, uuid} -> uuid
      :error -> id
    end
  end

  defp parse_uuid(id), do: id

  defp get_schema_version_id(state, label) do
    # Fetch the schema_version_id from the queue associated with the assignment
    query =
      from(a in Assignment,
        join: q in Anvil.Schema.Queue,
        on: a.queue_id == q.id,
        where: a.id == ^parse_uuid(label.assignment_id),
        select: q.schema_version_id
      )

    case state.repo.one(query) do
      nil -> raise "Assignment #{label.assignment_id} not found or has no associated queue"
      schema_version_id -> schema_version_id
    end
  end

  defp get_sample_id_from_assignment(_assignment_id) do
    # For now, return nil. In a real implementation, this would
    # fetch the sample_id from the assignment
    nil
  end
end
</file>

<file path="anvil/telemetry/alerts.ex">
defmodule Anvil.Telemetry.Alerts do
  @moduledoc """
  Alerting hooks for critical Anvil telemetry events.

  Provides configurable alerting for:
  - Low agreement scores (quality degradation)
  - Queue backup (throughput issues)
  - Export failures (operational issues)
  - Assignment timeout spikes (policy issues)

  ## Usage

  Attach alerting handlers in your application supervisor:

      def start(_type, _args) do
        children = [
          # ... other children
        ]

        # Attach alert handlers after supervisor starts
        :ok = Anvil.Telemetry.Alerts.attach_handlers()

        Supervisor.start_link(children, strategy: :one_for_one)
      end

  ## Configuration

      config :anvil, :alerts,
        enabled: true,
        low_agreement_threshold: 0.4,
        queue_backup_threshold: 100,
        timeout_spike_threshold: 50,
        timeout_spike_window: :hour,
        handlers: [
          log: true,
          slack: [enabled: false, webhook_url: nil],
          pagerduty: [enabled: false, api_key: nil]
        ]
  """

  require Logger

  @doc """
  Attaches telemetry handlers for alerting.

  Call this once during application startup, typically in Application.start/2.
  """
  @spec attach_handlers() :: :ok
  def attach_handlers do
    config = get_alert_config()

    if config.enabled do
      :telemetry.attach_many(
        "anvil-alerting",
        [
          [:anvil, :agreement, :low_score],
          [:anvil, :assignment, :timed_out],
          [:anvil, :export, :failed],
          [:anvil, :label, :validation_failed]
        ],
        &handle_alert_event/4,
        config
      )

      Logger.info("[Anvil.Telemetry.Alerts] Alert handlers attached")
    else
      Logger.info("[Anvil.Telemetry.Alerts] Alerting disabled by configuration")
    end

    :ok
  end

  @doc """
  Detaches telemetry handlers for alerting.
  """
  @spec detach_handlers() :: :ok | {:error, :not_found}
  def detach_handlers do
    :telemetry.detach("anvil-alerting")
  end

  # Event Handlers

  @doc false
  def handle_alert_event(event, measurements, metadata, config) do
    case event do
      [:anvil, :agreement, :low_score] ->
        handle_low_agreement(measurements, metadata, config)

      [:anvil, :assignment, :timed_out] ->
        handle_timeout_spike(measurements, metadata, config)

      [:anvil, :export, :failed] ->
        handle_export_failure(measurements, metadata, config)

      [:anvil, :label, :validation_failed] ->
        handle_validation_failure(measurements, metadata, config)

      _ ->
        :ok
    end
  end

  # Alert Handlers

  defp handle_low_agreement(measurements, metadata, config) do
    score = measurements.value
    threshold = config.low_agreement_threshold

    if score < threshold do
      send_alert(
        :critical,
        "Low Agreement Score Detected",
        """
        Inter-rater agreement below threshold #{threshold}.

        Current Score: #{Float.round(score, 3)}
        Dimension: #{metadata[:dimension] || "overall"}
        Metric: #{metadata[:metric] || "auto"}
        Sample ID: #{metadata[:sample_id]}

        This may indicate:
        - Ambiguous labeling guidelines
        - Insufficient labeler training
        - Complex/edge-case sample
        - Schema design issues

        Recommended Actions:
        1. Review labeling guidelines for this dimension
        2. Examine the specific sample for clarity
        3. Provide additional training to labelers
        4. Consider expert review of disagreements
        """,
        metadata,
        config
      )
    end
  end

  defp handle_timeout_spike(measurements, metadata, config) do
    count = measurements.count
    threshold = config.timeout_spike_threshold

    if count >= threshold do
      send_alert(
        :warning,
        "Assignment Timeout Spike",
        """
        High assignment timeout rate detected.

        Timeout Count: #{count} assignments (#{metadata[:requeued] || 0} requeued, #{metadata[:escalated] || 0} escalated)
        Queue ID: #{metadata[:queue_id]}
        Threshold: #{threshold} per #{config.timeout_spike_window}

        This may indicate:
        - Assignment timeout too short
        - Samples too complex
        - Labeler availability issues
        - System performance problems

        Recommended Actions:
        1. Review assignment timeout configuration
        2. Analyze sample complexity distribution
        3. Check labeler engagement metrics
        4. Monitor system performance
        """,
        metadata,
        config
      )
    end
  end

  defp handle_export_failure(_measurements, metadata, config) do
    send_alert(
      :error,
      "Export Failure",
      """
      Export generation failed.

      Export ID: #{metadata[:export_id]}
      Queue ID: #{metadata[:queue_id]}
      Format: #{metadata[:format]}
      Reason: #{inspect(metadata[:reason])}

      This requires immediate attention:
      1. Check export logs for detailed error
      2. Verify storage availability and permissions
      3. Check database connectivity
      4. Monitor disk space

      Exports are critical for downstream ML pipelines.
      """,
      metadata,
      config
    )
  end

  defp handle_validation_failure(measurements, metadata, config) do
    error_count = measurements.error_count

    # Only alert if we see many validation failures (potential schema issue)
    if error_count >= 5 do
      send_alert(
        :warning,
        "High Validation Error Count",
        """
        Label submission had #{error_count} validation errors.

        Assignment ID: #{metadata[:assignment_id]}
        Queue ID: #{metadata[:queue_id]}
        Errors: #{inspect(metadata[:errors])}

        Multiple validation errors may indicate:
        - Schema/UI mismatch
        - Incomplete form submission
        - Labeler confusion
        - Bug in validation logic

        Recommended Actions:
        1. Review validation errors for patterns
        2. Check UI/schema consistency
        3. Verify labeler training on new fields
        """,
        metadata,
        config
      )
    end
  end

  # Alert Dispatch

  defp send_alert(severity, title, message, metadata, config) do
    alert = %{
      severity: severity,
      title: title,
      message: String.trim(message),
      metadata: metadata,
      timestamp: DateTime.utc_now()
    }

    # Log alert
    if get_in(config.handlers, [:log]) do
      log_alert(alert)
    end

    # Send to Slack
    if get_in(config.handlers, [:slack, :enabled]) do
      send_slack_alert(alert, config.handlers.slack)
    end

    # Send to PagerDuty
    if get_in(config.handlers, [:pagerduty, :enabled]) do
      send_pagerduty_alert(alert, config.handlers.pagerduty)
    end

    :ok
  end

  defp log_alert(alert) do
    log_fn =
      case alert.severity do
        :critical -> &Logger.error/1
        :error -> &Logger.error/1
        :warning -> &Logger.warning/1
      end

    log_fn.("[Anvil Alert] [#{alert.severity}] #{alert.title}\n#{alert.message}")
  end

  defp send_slack_alert(alert, slack_config) do
    webhook_url = slack_config[:webhook_url]

    if webhook_url do
      # Format Slack message
      payload = %{
        text: "*[Anvil Alert] #{alert.title}*",
        attachments: [
          %{
            color: severity_color(alert.severity),
            text: alert.message,
            fields: [
              %{title: "Severity", value: to_string(alert.severity), short: true},
              %{
                title: "Timestamp",
                value: Calendar.strftime(alert.timestamp, "%Y-%m-%d %H:%M:%S UTC"),
                short: true
              }
            ],
            footer: "Anvil Telemetry Alerts"
          }
        ]
      }

      # Send to Slack (would use HTTPoison or similar in production)
      # HTTPoison.post(webhook_url, Jason.encode!(payload), [{"Content-Type", "application/json"}])
      Logger.debug("[Anvil.Telemetry.Alerts] Would send to Slack: #{inspect(payload)}")
    end

    :ok
  end

  defp send_pagerduty_alert(alert, pagerduty_config) do
    api_key = pagerduty_config[:api_key]
    routing_key = pagerduty_config[:routing_key]

    if api_key && routing_key do
      # Format PagerDuty event
      event = %{
        routing_key: routing_key,
        event_action: "trigger",
        payload: %{
          summary: alert.title,
          severity: pagerduty_severity(alert.severity),
          source: "anvil-telemetry",
          custom_details: %{
            message: alert.message,
            metadata: alert.metadata
          }
        }
      }

      # Send to PagerDuty (would use HTTPoison or similar in production)
      # HTTPoison.post("https://events.pagerduty.com/v2/enqueue", Jason.encode!(event), ...)
      Logger.debug("[Anvil.Telemetry.Alerts] Would send to PagerDuty: #{inspect(event)}")
    end

    :ok
  end

  # Helpers

  defp get_alert_config do
    defaults = %{
      enabled: true,
      low_agreement_threshold: 0.4,
      queue_backup_threshold: 100,
      timeout_spike_threshold: 50,
      timeout_spike_window: :hour,
      handlers: %{
        log: true,
        slack: %{enabled: false, webhook_url: nil},
        pagerduty: %{enabled: false, api_key: nil, routing_key: nil}
      }
    }

    config = Application.get_env(:anvil, :alerts, [])
    deep_merge(defaults, Map.new(config))
  end

  defp deep_merge(left, right) do
    Map.merge(left, right, fn
      _key, left_val, right_val when is_map(left_val) and is_map(right_val) ->
        deep_merge(left_val, right_val)

      _key, _left_val, right_val ->
        right_val
    end)
  end

  defp severity_color(:critical), do: "danger"
  defp severity_color(:error), do: "danger"
  defp severity_color(:warning), do: "warning"

  defp pagerduty_severity(:critical), do: "critical"
  defp pagerduty_severity(:error), do: "error"
  defp pagerduty_severity(:warning), do: "warning"
end
</file>

<file path="anvil/telemetry/metrics.ex">
defmodule Anvil.Telemetry.Metrics do
  @moduledoc """
  TelemetryMetrics definitions for Anvil.

  Defines counters, distributions, and summaries for monitoring labeling operations.
  Compatible with TelemetryMetricsStatsd, Prometheus, and other exporters.

  ## Metric Categories

  - **Counters**: Total counts (queues created, labels submitted, assignments)
  - **Distributions**: Latency histograms (assignment dispatch, agreement compute)
  - **Summaries**: Percentiles (agreement scores)
  - **Last Values**: Current state (export progress, queue depth)

  ## Usage

  In your application supervisor:

      children = [
        # Other children...
        {TelemetryMetricsStatsd, metrics: Anvil.Telemetry.Metrics.metrics(), port: 8125}
      ]

  Or with Prometheus:

      children = [
        {TelemetryMetricsPrometheus, metrics: Anvil.Telemetry.Metrics.metrics()}
      ]
  """

  import Telemetry.Metrics

  @doc """
  Returns all metric definitions for Anvil.
  """
  @spec metrics() :: [Telemetry.Metrics.t()]
  def metrics do
    [
      # Queue metrics
      counter("anvil.queue.created.count",
        description: "Total number of queues created",
        tags: [:policy_type]
      ),
      counter("anvil.queue.status_changed.count",
        description: "Total number of queue status changes",
        tags: [:queue_id, :from_status, :to_status]
      ),

      # Assignment metrics
      counter("anvil.assignment.created.count",
        description: "Total number of assignments created",
        tags: [:queue_id, :labeler_id]
      ),
      counter("anvil.assignment.completed.count",
        description: "Total number of assignments completed",
        tags: [:queue_id]
      ),
      counter("anvil.assignment.expired.count",
        description: "Total number of assignments expired",
        tags: [:queue_id]
      ),
      counter("anvil.assignment.timed_out.count",
        description: "Total number of assignments timed out (batch)",
        tags: [:queue_id]
      ),
      distribution("anvil.assignment.dispatch.duration",
        description: "Assignment dispatch latency",
        unit: {:native, :millisecond},
        tags: [:queue_id, :policy_type],
        reporter_options: [buckets: [10, 25, 50, 100, 250, 500, 1000]]
      ),

      # Label metrics
      counter("anvil.label.submit.count",
        description: "Total number of labels submitted",
        tags: [:queue_id, :labeler_id]
      ),
      counter("anvil.label.validation_failed.count",
        description: "Total number of label validation failures",
        tags: [:queue_id]
      ),
      distribution("anvil.label.submit.duration",
        description: "Label submission duration",
        unit: {:native, :millisecond},
        tags: [:queue_id],
        reporter_options: [buckets: [25, 50, 100, 250, 500, 1000]]
      ),
      summary("anvil.label.validation_failed.error_count",
        description: "Number of validation errors per failed submission",
        tags: [:queue_id]
      ),

      # Agreement metrics
      counter("anvil.agreement.compute.count",
        description: "Total number of agreement computations",
        tags: [:metric, :dimension]
      ),
      counter("anvil.agreement.low_score.count",
        description: "Total number of low agreement score detections",
        tags: [:dimension]
      ),
      distribution("anvil.agreement.compute.duration",
        description: "Agreement computation duration",
        unit: {:native, :millisecond},
        tags: [:metric, :dimension],
        reporter_options: [buckets: [10, 25, 50, 100, 250, 500, 1000]]
      ),
      distribution("anvil.agreement.batch_recompute.duration",
        description: "Batch agreement recomputation duration",
        unit: {:native, :second},
        tags: [:queue_id],
        reporter_options: [buckets: [1, 5, 10, 30, 60, 300, 600]]
      ),
      summary("anvil.agreement.low_score.value",
        description: "Agreement score percentiles",
        tags: [:dimension],
        reporter_options: [percentiles: [0.5, 0.9, 0.95, 0.99]]
      ),

      # Export metrics
      counter("anvil.export.generate.count",
        description: "Total number of exports generated",
        tags: [:format, :queue_id]
      ),
      counter("anvil.export.completed.count",
        description: "Total number of exports completed",
        tags: [:format]
      ),
      counter("anvil.export.failed.count",
        description: "Total number of export failures",
        tags: [:format, :reason]
      ),
      distribution("anvil.export.generate.duration",
        description: "Export generation duration",
        unit: {:native, :second},
        tags: [:format, :queue_id],
        reporter_options: [buckets: [1, 5, 10, 30, 60, 300, 600]]
      ),
      last_value("anvil.export.progress.rows_processed",
        description: "Current export progress (rows processed)",
        tags: [:export_id]
      ),

      # Storage metrics
      counter("anvil.storage.query.count",
        description: "Total number of storage queries",
        tags: [:operation]
      ),
      distribution("anvil.storage.query.duration",
        description: "Storage query duration",
        unit: {:native, :millisecond},
        tags: [:operation],
        reporter_options: [buckets: [1, 5, 10, 25, 50, 100, 250, 500]]
      ),

      # Schema metrics
      counter("anvil.schema.validation.count",
        description: "Total number of schema validations",
        tags: [:schema_id, :valid?]
      ),
      counter("anvil.schema.migration.count",
        description: "Total number of schema migrations",
        tags: [:from_version, :to_version]
      )
    ]
  end

  @doc """
  Returns core metrics for basic monitoring (subset of all metrics).

  Use this for minimal overhead monitoring or when cardinality is a concern.
  """
  @spec core_metrics() :: [Telemetry.Metrics.t()]
  def core_metrics do
    [
      # Essential counters
      counter("anvil.label.submit.count",
        description: "Total number of labels submitted",
        tags: [:queue_id]
      ),
      counter("anvil.assignment.completed.count",
        description: "Total number of assignments completed",
        tags: [:queue_id]
      ),
      counter("anvil.assignment.expired.count",
        description: "Total number of assignments expired",
        tags: [:queue_id]
      ),

      # Essential latency metrics
      distribution("anvil.assignment.dispatch.duration",
        description: "Assignment dispatch latency",
        unit: {:native, :millisecond},
        tags: [:policy_type],
        reporter_options: [buckets: [10, 50, 100, 250, 500, 1000]]
      ),
      distribution("anvil.agreement.compute.duration",
        description: "Agreement computation duration",
        unit: {:native, :millisecond},
        tags: [:metric],
        reporter_options: [buckets: [10, 50, 100, 250, 500, 1000]]
      ),

      # Quality metrics
      summary("anvil.agreement.low_score.value",
        description: "Agreement score percentiles",
        tags: [:dimension],
        reporter_options: [percentiles: [0.5, 0.95, 0.99]]
      )
    ]
  end

  @doc """
  Extracts metadata tags from telemetry metadata map.

  Filters to only include known tag keys to prevent cardinality explosion.
  """
  @spec extract_tags(map()) :: map()
  def extract_tags(metadata) do
    allowed_keys = [
      :queue_id,
      :labeler_id,
      :assignment_id,
      :export_id,
      :schema_id,
      :policy_type,
      :format,
      :metric,
      :dimension,
      :operation,
      :from_status,
      :to_status,
      :from_version,
      :to_version,
      :valid?,
      :reason
    ]

    metadata
    |> Map.take(allowed_keys)
    |> Map.new(fn {k, v} -> {k, to_string(v)} end)
  end
end
</file>

<file path="anvil/workers/agreement_recompute.ex">
defmodule Anvil.Workers.AgreementRecompute do
  @moduledoc """
  Background job worker that recomputes agreement metrics for labeled samples.

  This worker can be scheduled to run periodically (nightly at 2 AM) or triggered
  on-demand for specific queues. It processes samples in batches to compute
  inter-rater agreement scores when multiple labelers have labeled the same sample.

  ## Configuration

  Queue: `:agreement`
  Max Attempts: 5
  Priority: 2 (medium priority)

  ## Scheduled Execution

  Configured in `config.exs` via Oban.Plugins.Cron:
  ```elixir
  {"0 2 * * *", Anvil.Workers.AgreementRecompute}
  ```

  ## Job Arguments

  - `queue_id` (optional) - If provided, only recomputes agreement for samples in this queue
  - `batch_size` (optional) - Number of samples to process per batch (default: 100)

  ## Telemetry

  Emits the following telemetry events:
  - `[:anvil, :workers, :agreement_recompute, :started]` - Job execution started
  - `[:anvil, :workers, :agreement_recompute, :batch_processed]` - Batch completed
  - `[:anvil, :workers, :agreement_recompute, :completed]` - Job completed successfully
  """

  use Oban.Worker,
    queue: :agreement,
    max_attempts: 5,
    priority: 2

  import Ecto.Query
  alias Anvil.Repo
  alias Anvil.Schema.{Assignment, Label}
  alias Anvil.Agreement

  @default_batch_size 100

  @impl Oban.Worker
  def perform(%Oban.Job{args: args} = _job) do
    queue_id = args["queue_id"]
    batch_size = args["batch_size"] || @default_batch_size

    :telemetry.execute(
      [:anvil, :workers, :agreement_recompute, :started],
      %{},
      %{queue_id: queue_id}
    )

    # Build query for samples with multiple labels
    base_query = build_sample_query(queue_id)

    # Fetch and process samples in batches
    # Note: Using Repo.all() instead of Repo.stream() for test compatibility
    sample_ids = Repo.all(base_query)

    processed_count =
      sample_ids
      |> Enum.chunk_every(batch_size)
      |> Enum.map(&process_batch/1)
      |> Enum.sum()

    :telemetry.execute(
      [:anvil, :workers, :agreement_recompute, :completed],
      %{samples_processed: processed_count},
      %{queue_id: queue_id}
    )

    :ok
  end

  @doc """
  Enqueues an agreement recomputation job for a specific queue.

  Uses uniqueness constraints to prevent duplicate jobs for the same queue
  within a 24-hour period.

  ## Options

  - `:batch_size` - Number of samples per batch (default: 100)
  - `:unique_period` - Uniqueness window in milliseconds (default: 24 hours)
  """
  @spec enqueue(String.t(), keyword()) :: {:ok, Oban.Job.t()} | {:error, term()}
  def enqueue(queue_id, opts \\ []) do
    batch_size = Keyword.get(opts, :batch_size, @default_batch_size)
    unique_period = Keyword.get(opts, :unique_period, :timer.hours(24))

    %{queue_id: queue_id, batch_size: batch_size}
    |> __MODULE__.new(unique: [period: unique_period, keys: [:queue_id]])
    |> Oban.insert()
  end

  # Builds a query for samples that have multiple labels
  defp build_sample_query(nil) do
    # Get all samples with 2+ labels
    from(a in Assignment,
      join: l in Label,
      on: l.assignment_id == a.id,
      group_by: a.sample_id,
      having: count(l.id) >= 2,
      select: a.sample_id,
      distinct: true
    )
  end

  defp build_sample_query(queue_id) do
    # Get samples in specific queue with 2+ labels
    from(a in Assignment,
      join: l in Label,
      on: l.assignment_id == a.id,
      where: a.queue_id == ^queue_id,
      group_by: a.sample_id,
      having: count(l.id) >= 2,
      select: a.sample_id,
      distinct: true
    )
  end

  # Processes a batch of sample IDs
  defp process_batch(sample_ids) do
    sample_ids
    |> Enum.map(&compute_sample_agreement/1)
    |> Enum.count(&match?(:ok, &1))
  end

  # Computes agreement for a single sample
  defp compute_sample_agreement(sample_id) do
    # Fetch all labels for this sample
    labels =
      from(l in Label,
        join: a in Assignment,
        on: l.assignment_id == a.id,
        where: a.sample_id == ^sample_id,
        select: %{
          labeler_id: l.labeler_id,
          values: l.payload
        }
      )
      |> Repo.all()

    case Agreement.compute(labels) do
      {:ok, _score} ->
        :telemetry.execute(
          [:anvil, :workers, :agreement_recompute, :sample_computed],
          %{},
          %{sample_id: sample_id}
        )

        :ok

      {:error, _reason} ->
        # Log but don't fail the entire job for individual sample failures
        :error
    end
  rescue
    _error -> :error
  end
end
</file>

<file path="anvil/workers/retention_sweep.ex">
defmodule Anvil.Workers.RetentionSweep do
  @moduledoc """
  Background job worker that enforces data retention policies.

  This worker runs periodically (daily at 3 AM) to clean up old data according
  to retention policies. It handles:

  - Deletion of audit logs older than the retention period (default: 7 years)
  - Redaction of PII in labels past their retention period (field-level retention)
  - Cleanup of expired assignments and temporary data

  ## PII Retention Actions

  The worker supports three retention actions for labels with expired PII fields:
  - `:field_redaction` - Redact only expired fields, keep unexpired (default)
  - `:soft_delete` - Keep metadata, strip entire payload
  - `:hard_delete` - Permanent deletion (breaks reproducibility)

  ## Configuration

  Queue: `:maintenance`
  Max Attempts: 3
  Priority: 2 (medium priority)

  ## Scheduled Execution

  Configured in `config.exs` via Oban.Plugins.Cron:
  ```elixir
  {"0 3 * * *", Anvil.Workers.RetentionSweep}
  ```

  ## Job Arguments

  - `retention_days` (optional) - Number of days to retain audit logs (default: 2555 = ~7 years)
  - `dry_run` (optional) - If true, only counts records without deleting (default: false)
  - `pii_retention_action` (optional) - Retention action for PII fields (`:field_redaction`, `:soft_delete`, `:hard_delete`) (default: `:field_redaction`)
  - `process_pii` (optional) - If true, process PII retention (default: true)

  ## Telemetry

  Emits the following telemetry events:
  - `[:anvil, :workers, :retention_sweep, :started]` - Job execution started
  - `[:anvil, :workers, :retention_sweep, :completed]` - Job completed successfully
  """

  use Oban.Worker,
    queue: :maintenance,
    max_attempts: 3,
    priority: 2

  import Ecto.Query
  alias Anvil.Repo
  alias Anvil.Schema.AuditLog
  alias Anvil.PII.Retention

  @default_retention_days 2555

  @impl Oban.Worker
  def perform(%Oban.Job{args: args} = _job) do
    retention_days = args["retention_days"] || @default_retention_days
    dry_run = args["dry_run"] || false
    process_pii = Map.get(args, "process_pii", true)

    pii_retention_action =
      String.to_existing_atom(Map.get(args, "pii_retention_action", "field_redaction"))

    :telemetry.execute(
      [:anvil, :workers, :retention_sweep, :started],
      %{},
      %{retention_days: retention_days, dry_run: dry_run, process_pii: process_pii}
    )

    cutoff = DateTime.add(DateTime.utc_now(), -retention_days, :day)

    # Delete old audit logs
    {audit_logs_deleted, _} = delete_old_audit_logs(cutoff, dry_run)

    # Process PII retention if enabled
    labels_processed =
      if process_pii do
        case Retention.process_expired_labels(
               dry_run: dry_run,
               action: pii_retention_action
             ) do
          {:ok, count} -> count
        end
      else
        0
      end

    :telemetry.execute(
      [:anvil, :workers, :retention_sweep, :completed],
      %{
        audit_logs_deleted: audit_logs_deleted,
        labels_processed: labels_processed
      },
      %{retention_days: retention_days, dry_run: dry_run, process_pii: process_pii}
    )

    :ok
  end

  @doc """
  Deletes audit logs older than the specified cutoff date.

  ## Parameters

  - `cutoff` - DateTime before which logs should be deleted
  - `dry_run` - If true, only counts without deleting

  ## Returns

  A tuple `{count, nil}` where count is the number of records deleted (or counted in dry run mode).
  """
  @spec delete_old_audit_logs(DateTime.t(), boolean()) :: {non_neg_integer(), nil}
  def delete_old_audit_logs(cutoff, dry_run \\ false) do
    query = from(a in AuditLog, where: a.occurred_at < ^cutoff)

    if dry_run do
      count = Repo.aggregate(query, :count, :id)
      {count, nil}
    else
      Repo.delete_all(query)
    end
  end

  @doc """
  Enqueues a retention sweep job.

  ## Options

  - `:retention_days` - Number of days to retain audit logs (default: 2555)
  - `:dry_run` - If true, only counts records without deleting (default: false)
  - `:process_pii` - If true, process PII retention (default: true)
  - `:pii_retention_action` - Retention action for PII (default: :field_redaction)
  """
  @spec enqueue(keyword()) :: {:ok, Oban.Job.t()} | {:error, term()}
  def enqueue(opts \\ []) do
    retention_days = Keyword.get(opts, :retention_days, @default_retention_days)
    dry_run = Keyword.get(opts, :dry_run, false)
    process_pii = Keyword.get(opts, :process_pii, true)
    pii_retention_action = Keyword.get(opts, :pii_retention_action, :field_redaction)

    %{
      retention_days: retention_days,
      dry_run: dry_run,
      process_pii: process_pii,
      pii_retention_action: Atom.to_string(pii_retention_action)
    }
    |> __MODULE__.new()
    |> Oban.insert()
  end
end
</file>

<file path="anvil/workers/timeout_checker.ex">
defmodule Anvil.Workers.TimeoutChecker do
  @moduledoc """
  Background job worker that checks for timed-out assignments and requeues them.

  This worker runs periodically (every 5 minutes via cron) to find assignments
  that have exceeded their deadline and transitions them from :reserved to
  :timed_out status, then requeues them for another labeler.

  ## Configuration

  Queue: `:timeouts`
  Max Attempts: 3
  Priority: 1 (high priority)

  ## Scheduled Execution

  Configured in `config.exs` via Oban.Plugins.Cron:
  ```elixir
  {"*/5 * * * *", Anvil.Workers.TimeoutChecker}
  ```

  ## Telemetry

  Emits the following telemetry events:
  - `[:anvil, :workers, :timeout_checker, :started]` - Job execution started
  - `[:anvil, :workers, :timeout_checker, :completed]` - Job completed successfully
  - `[:anvil, :workers, :timeout_checker, :failed]` - Job failed
  """

  use Oban.Worker,
    queue: :timeouts,
    max_attempts: 3,
    priority: 1

  import Ecto.Query
  alias Anvil.Repo
  alias Anvil.Schema.Assignment

  @impl Oban.Worker
  def perform(%Oban.Job{} = _job) do
    :telemetry.execute([:anvil, :workers, :timeout_checker, :started], %{}, %{})

    now = DateTime.utc_now()

    # Find all reserved assignments that have passed their deadline
    timed_out_assignments =
      from(a in Assignment,
        where: a.status == :reserved and a.deadline < ^now
      )
      |> Repo.all()

    results = Enum.map(timed_out_assignments, &handle_timeout/1)

    timed_out_count = length(timed_out_assignments)
    requeued_count = Enum.count(results, &match?({:ok, :requeued}, &1))
    failed_count = Enum.count(results, &match?({:error, _}, &1))

    :telemetry.execute(
      [:anvil, :workers, :timeout_checker, :completed],
      %{
        timed_out: timed_out_count,
        requeued: requeued_count,
        failed: failed_count
      },
      %{}
    )

    if failed_count > 0 do
      {:error, "Failed to process #{failed_count} assignments"}
    else
      :ok
    end
  end

  @doc """
  Handles a single timed-out assignment by marking it as timed_out and requeuing it.
  """
  @spec handle_timeout(Assignment.t()) :: {:ok, :requeued} | {:error, term()}
  def handle_timeout(%Assignment{} = assignment) do
    Repo.transaction(fn ->
      # First mark as timed out
      {:ok, timed_out_assignment} =
        assignment
        |> Assignment.timeout()
        |> Repo.update()

      # Then requeue for retry
      timed_out_assignment
      |> Assignment.requeue()
      |> Repo.update!()

      :requeued
    end)
  rescue
    error -> {:error, error}
  end
end
</file>

<file path="anvil/agreement.ex">
defmodule Anvil.Agreement do
  @moduledoc """
  Inter-rater agreement metrics for measuring labeler consistency.

  Automatically selects the appropriate metric based on the data.
  """

  alias Anvil.Agreement.{Cohen, Fleiss, Krippendorff}
  alias Anvil.Telemetry

  @doc """
  Computes agreement metric, automatically selecting the appropriate algorithm.

  ## Options

    * `:metric` - Force a specific metric (:cohen, :fleiss, :krippendorff)
    * `:field` - Field name to compute agreement for (default: uses all fields)

  """
  @spec compute([Anvil.Label.t()], keyword()) :: {:ok, float()} | {:error, term()}
  def compute(labels, opts \\ []) do
    field = Keyword.get(opts, :field)
    metric = Keyword.get(opts, :metric)

    # Wrap computation in telemetry span
    Telemetry.span_agreement_compute(
      %{
        metric: metric || :auto,
        dimension: field,
        n_raters: labels |> Enum.map(& &1.labeler_id) |> Enum.uniq() |> length()
      },
      fn ->
        computed_result =
          cond do
            metric == :cohen -> Cohen.compute(labels, opts)
            metric == :fleiss -> Fleiss.compute(labels, opts)
            metric == :krippendorff -> Krippendorff.compute(labels, opts)
            true -> auto_select(labels, opts)
          end

        # Check for low agreement score
        case computed_result do
          {:ok, score} when score < 0.6 ->
            Telemetry.emit_low_agreement_score(score, %{
              dimension: field,
              threshold: 0.6,
              metric: metric || :auto
            })

          _ ->
            :ok
        end

        {computed_result, %{}}
      end
    )
  end

  @doc """
  Computes agreement for a specific dimension/field.

  ## Examples

      iex> labels = [
      ...>   %{labeler_id: "l1", values: %{"coherence" => 4, "grounded" => 3}},
      ...>   %{labeler_id: "l2", values: %{"coherence" => 4, "grounded" => 5}}
      ...> ]
      iex> Agreement.compute_for_field(labels, "coherence")
      {:ok, 1.0}  # Perfect agreement on coherence

  """
  @spec compute_for_field([map()], String.t(), keyword()) :: {:ok, float()} | {:error, term()}
  def compute_for_field(labels, field_name, opts \\ []) do
    # Extract field values from each label, preserving sample_id
    field_labels =
      labels
      |> Enum.map(fn label ->
        value = get_in(label, [Access.key(:values, %{}), field_name])

        %{
          sample_id: label[:sample_id],
          labeler_id: label[:labeler_id],
          values: %{field_name => value}
        }
      end)
      |> Enum.reject(fn label -> is_nil(get_in(label, [:values, field_name])) end)

    if length(field_labels) < 2 do
      {:error, :insufficient_labels}
    else
      compute(field_labels, Keyword.put(opts, :field, field_name))
    end
  end

  @doc """
  Computes agreement for all dimensions in the schema.

  Returns a map with per-dimension agreement scores.

  ## Examples

      iex> labels = [...]
      iex> schema = %{fields: ["coherence", "grounded", "balance"]}
      iex> Agreement.compute_all_dimensions(labels, schema)
      %{
        coherence: {:ok, 0.72},
        grounded: {:ok, 0.85},
        balance: {:ok, 0.45}
      }

  """
  @spec compute_all_dimensions([map()], map(), keyword()) :: map()
  def compute_all_dimensions(labels, schema, opts \\ []) do
    fields = Map.get(schema, :fields, [])

    for field <- fields, into: %{} do
      {String.to_atom(field), compute_for_field(labels, field, opts)}
    end
  end

  @doc """
  Returns a comprehensive agreement summary with per-dimension breakdown.

  ## Examples

      iex> labels = [...]
      iex> schema = %{fields: ["coherence", "grounded"]}
      iex> Agreement.summary(labels, schema)
      %{
        overall: {:ok, 0.78},
        by_dimension: %{
          coherence: {:ok, 0.72},
          grounded: {:ok, 0.85}
        },
        sample_count: 50,
        labeler_count: 3
      }

  """
  @spec summary([map()], map(), keyword()) :: map()
  def summary(labels, schema, opts \\ []) do
    %{
      overall: compute(labels, opts),
      by_dimension: compute_all_dimensions(labels, schema, opts),
      sample_count: labels |> Enum.map(& &1[:assignment_id]) |> Enum.uniq() |> length(),
      labeler_count: labels |> Enum.map(& &1[:labeler_id]) |> Enum.uniq() |> length()
    }
  end

  @doc """
  Batch recomputes agreement for all samples in a queue.

  This is useful for full recalculation after schema migrations or data changes.

  ## Options

    * `:batch_size` - Number of samples to process per batch (default: 100)
    * `:metric` - Force a specific metric for all computations

  """
  @spec recompute_all(binary(), keyword()) :: {:ok, map()} | {:error, term()}
  def recompute_all(queue_id, _opts \\ []) do
    # Wrap batch recomputation in telemetry span
    Telemetry.span_agreement_batch_recompute(
      %{queue_id: queue_id},
      fn ->
        # This would need to fetch labels from storage and recompute
        # For now, return a placeholder
        computed_result = {:ok, %{queue_id: queue_id, status: :not_implemented}}
        {computed_result, %{samples_processed: 0}}
      end
    )
  end

  defp auto_select(labels, opts) do
    labelers = labels |> Enum.map(& &1.labeler_id) |> Enum.uniq()

    cond do
      length(labelers) == 2 -> Cohen.compute(labels, opts)
      length(labelers) > 2 -> Fleiss.compute(labels, opts)
      true -> {:error, :insufficient_raters}
    end
  end
end
</file>

<file path="anvil/application.ex">
defmodule Anvil.Application do
  # See https://hexdocs.pm/elixir/Application.html
  # for more information on OTP Applications
  @moduledoc false

  use Application

  @impl true
  def start(_type, _args) do
    children =
      []
      |> maybe_child(Application.get_env(:anvil, :start_repo, true), Anvil.Repo)
      |> maybe_child(
        Application.get_env(:anvil, :start_oban, true),
        {Oban, Application.get_env(:anvil, Oban, [])}
      )
      |> maybe_child(true, {Registry, keys: :unique, name: Anvil.Registry})
      |> maybe_child(true, {Cachex, name: :forge_samples})
      |> maybe_child(true, {Task.Supervisor, name: Anvil.TaskSupervisor})
      |> maybe_child(api_enabled?(), Anvil.API.Server)

    # See https://hexdocs.pm/elixir/Supervisor.html
    # for other strategies and supported options
    opts = [strategy: :one_for_one, name: Anvil.Supervisor]
    Supervisor.start_link(children, opts)
  end

  defp maybe_child(children, true, child), do: children ++ [child]
  defp maybe_child(children, _flag, _child), do: children

  defp api_enabled? do
    config = Application.get_env(:anvil, :api_server, [])
    Keyword.get(config, :enabled, false)
  end
end
</file>

<file path="anvil/assignment.ex">
defmodule Anvil.Assignment do
  @moduledoc """
  Represents a labeling task assigned to a specific labeler.

  Tracks the lifecycle of an assignment from creation through completion,
  expiration, or skipping.
  """

  @type status :: :pending | :in_progress | :completed | :expired | :skipped

  @type t :: %__MODULE__{
          id: String.t(),
          sample_id: String.t(),
          labeler_id: String.t(),
          queue_id: String.t(),
          status: status(),
          deadline: DateTime.t() | nil,
          attempts: non_neg_integer(),
          label_id: String.t() | nil,
          skip_reason: String.t() | nil,
          created_at: DateTime.t(),
          started_at: DateTime.t() | nil,
          completed_at: DateTime.t() | nil,
          expired_at: DateTime.t() | nil,
          skipped_at: DateTime.t() | nil
        }

  defstruct [
    :id,
    :sample_id,
    :labeler_id,
    :queue_id,
    :status,
    :deadline,
    :attempts,
    :label_id,
    :skip_reason,
    :created_at,
    :started_at,
    :completed_at,
    :expired_at,
    :skipped_at
  ]

  @doc """
  Creates a new pending assignment.
  """
  @spec new(keyword()) :: t()
  def new(opts) do
    struct(
      __MODULE__,
      Keyword.merge(
        [
          id: generate_id(),
          status: :pending,
          attempts: 0,
          created_at: DateTime.utc_now()
        ],
        opts
      )
    )
  end

  @doc """
  Starts an assignment, transitioning from :pending to :in_progress.
  """
  @spec start(t(), pos_integer()) :: {:ok, t()} | {:error, term()}
  def start(%__MODULE__{status: :pending} = assignment, timeout_seconds) do
    now = DateTime.utc_now()
    deadline = DateTime.add(now, timeout_seconds, :second)

    {:ok,
     %{
       assignment
       | status: :in_progress,
         started_at: now,
         deadline: deadline,
         attempts: assignment.attempts + 1
     }}
  end

  def start(%__MODULE__{status: status}, _timeout) do
    {:error, {:invalid_transition, status, :in_progress}}
  end

  @doc """
  Completes an assignment with a label ID.
  """
  @spec complete(t(), String.t()) :: {:ok, t()} | {:error, term()}
  def complete(%__MODULE__{status: :in_progress} = assignment, label_id) do
    {:ok,
     %{
       assignment
       | status: :completed,
         completed_at: DateTime.utc_now(),
         label_id: label_id
     }}
  end

  def complete(%__MODULE__{status: status}, _label_id) do
    {:error, {:invalid_transition, status, :completed}}
  end

  @doc """
  Skips an assignment with an optional reason.
  """
  @spec skip(t(), String.t() | nil) :: {:ok, t()} | {:error, term()}
  def skip(assignment, reason \\ nil)

  def skip(%__MODULE__{status: :pending} = assignment, reason) do
    {:ok,
     %{
       assignment
       | status: :skipped,
         skipped_at: DateTime.utc_now(),
         skip_reason: reason
     }}
  end

  def skip(%__MODULE__{status: :in_progress} = assignment, reason) do
    {:ok,
     %{
       assignment
       | status: :skipped,
         skipped_at: DateTime.utc_now(),
         skip_reason: reason
     }}
  end

  def skip(%__MODULE__{status: status}, _reason) do
    {:error, {:invalid_transition, status, :skipped}}
  end

  @doc """
  Expires an assignment that has passed its deadline.
  """
  @spec expire(t()) :: {:ok, t()} | {:error, term()}
  def expire(%__MODULE__{status: status} = assignment)
      when status in [:pending, :in_progress] do
    {:ok, %{assignment | status: :expired, expired_at: DateTime.utc_now()}}
  end

  def expire(%__MODULE__{status: status}) do
    {:error, {:invalid_transition, status, :expired}}
  end

  @doc """
  Checks if the assignment is past its deadline.
  """
  @spec past_deadline?(t()) :: boolean()
  def past_deadline?(%__MODULE__{deadline: nil}), do: false

  def past_deadline?(%__MODULE__{deadline: deadline}) do
    DateTime.compare(DateTime.utc_now(), deadline) == :gt
  end

  @doc """
  Returns the labeling time in seconds, or nil if not completed.
  """
  @spec labeling_time_seconds(t()) :: non_neg_integer() | nil
  def labeling_time_seconds(%__MODULE__{started_at: nil}), do: nil
  def labeling_time_seconds(%__MODULE__{completed_at: nil}), do: nil

  def labeling_time_seconds(%__MODULE__{started_at: started, completed_at: completed}) do
    DateTime.diff(completed, started, :second)
  end

  defp generate_id do
    Ecto.UUID.generate()
  end
end
</file>

<file path="anvil/export.ex">
defmodule Anvil.Export do
  @moduledoc """
  Export labeled data in various formats with deterministic lineage tracking.

  This module provides two interfaces:
  1. New ADR-005 interface: `to_format/3` with streaming, deterministic ordering, and manifests
  2. Legacy interface: `export/2` for backward compatibility

  ## ADR-005 Interface

  The new interface requires explicit schema version specification and produces:
  - Deterministically ordered exports
  - Export manifests with SHA256 hashes
  - Streaming for large datasets

  ## Examples

      # New interface (recommended)
      {:ok, result} = Anvil.Export.to_format(:csv, queue_id, %{
        schema_version_id: schema_version_id,
        output_path: "/tmp/export.csv"
      })

      # Legacy interface (for backward compatibility)
      Anvil.Export.export(queue, format: :csv, path: "/tmp/export.csv")
  """

  alias Anvil.Export.{CSV, JSONL, Manifest}

  @type format :: :csv | :jsonl
  @type export_result :: %{
          manifest: Manifest.t(),
          output_path: String.t()
        }

  @doc """
  Exports labels to the specified format following ADR-005 specification.

  This is the recommended interface for exports, providing:
  - Deterministic ordering for reproducibility
  - Export manifests with cryptographic hashes
  - Streaming for memory safety
  - Version pinning

  ## Parameters

    * `format` - Export format (`:csv` or `:jsonl`)
    * `queue_id` - UUID of the queue to export
    * `opts` - Export options (map)

  ## Options

    * `:schema_version_id` - (required) UUID of the schema version
    * `:output_path` - (required) File path for export
    * `:sample_version` - (optional) Forge version tag
    * `:limit` - (optional) Maximum number of rows
    * `:offset` - (optional) Number of rows to skip
    * `:filter` - (optional) Filter criteria

  ## Returns

    * `{:ok, %{manifest: manifest, output_path: path}}` on success
    * `{:error, reason}` on failure

  ## Examples

      iex> Anvil.Export.to_format(:csv, queue_id, %{
      ...>   schema_version_id: schema_v2_id,
      ...>   output_path: "/tmp/labels.csv"
      ...> })
      {:ok, %{manifest: %Manifest{...}, output_path: "/tmp/labels.csv"}}

      iex> Anvil.Export.to_format(:jsonl, queue_id, %{
      ...>   schema_version_id: schema_v2_id,
      ...>   output_path: "/tmp/labels.jsonl",
      ...>   limit: 1000,
      ...>   offset: 0
      ...> })
      {:ok, %{manifest: %Manifest{...}, output_path: "/tmp/labels.jsonl"}}
  """
  @spec to_format(format(), binary(), map()) :: {:ok, export_result()} | {:error, term()}
  def to_format(format, queue_id, opts) when is_map(opts) do
    case format do
      :csv -> CSV.to_format(queue_id, opts)
      :jsonl -> JSONL.to_format(queue_id, opts)
      _ -> {:error, {:unsupported_format, format}}
    end
  end

  @doc """
  Legacy export function for backward compatibility.

  This function is deprecated in favor of `to_format/3` which provides
  better reproducibility guarantees through deterministic ordering and
  export manifests.

  ## Options

    * `:format` - Export format (`:csv` or `:jsonl`)
    * `:path` - Output file path
    * `:filter` - Filter function to select labels
    * `:include_metadata` - Include labeling metadata (default: true)

  ## Examples

      iex> Anvil.Export.export(queue, format: :csv, path: "/tmp/labels.csv")
      :ok
  """
  @spec export(pid() | atom(), keyword()) :: :ok | {:error, term()}
  def export(queue, opts) do
    format = Keyword.fetch!(opts, :format)
    path = Keyword.fetch!(opts, :path)
    filter = Keyword.get(opts, :filter)

    labels = Anvil.Queue.get_labels(queue)

    labels =
      if filter do
        Enum.filter(labels, filter)
      else
        labels
      end

    case format do
      :csv -> CSV.export(labels, path, opts)
      :jsonl -> JSONL.export(labels, path, opts)
      _ -> {:error, {:unsupported_format, format}}
    end
  end

  @doc """
  Verifies export reproducibility by re-exporting and comparing hashes.

  ## Examples

      iex> manifest = Anvil.Export.Manifest.load("/tmp/export.csv.manifest.json")
      iex> Anvil.Export.verify_reproducibility(manifest)
      {:ok, :reproducible}
  """
  @spec verify_reproducibility(Manifest.t()) :: {:ok, :reproducible} | {:error, term()}
  def verify_reproducibility(%Manifest{} = manifest) do
    opts = %{
      schema_version_id: manifest.schema_version_id,
      output_path: manifest.output_path <> ".verify",
      sample_version: manifest.sample_version,
      limit: manifest.parameters[:limit],
      offset: manifest.parameters[:offset],
      filter: manifest.parameters[:filter]
    }

    case to_format(manifest.format, manifest.queue_id, opts) do
      {:ok, result} ->
        verify_path = manifest.output_path <> ".verify"

        try do
          if result.manifest.sha256_hash == manifest.sha256_hash do
            {:ok, :reproducible}
          else
            {:error, :hash_mismatch, old: manifest.sha256_hash, new: result.manifest.sha256_hash}
          end
        after
          File.rm(verify_path)
          File.rm(verify_path <> ".manifest.json")
        end

      {:error, reason} ->
        {:error, reason}
    end
  end
end
</file>

<file path="anvil/forge_bridge.ex">
defmodule Anvil.ForgeBridge do
  @moduledoc """
  Abstract interface for fetching samples from Forge.

  Supports multiple backends (direct DB, HTTP, cached) to enable different
  deployment topologies:
  - Direct: Same Postgres cluster, cross-schema queries
  - HTTP: Separate services with REST API
  - Cached: Performance wrapper with TTL-based caching

  ## Configuration

      # config/config.exs
      config :anvil,
        forge_bridge_backend: Anvil.ForgeBridge.Direct

  ## Usage

      {:ok, sample} = Anvil.ForgeBridge.fetch_sample(sample_id)
      %SampleDTO{id: id, content: content, version: version} = sample
  """

  alias Anvil.ForgeBridge.SampleDTO

  @type sample_id :: binary()
  @type sample_dto :: SampleDTO.t()
  @type error_reason :: :not_found | :forge_unavailable | atom()

  @doc """
  Fetches a single sample from Forge by ID.

  ## Options

  - `:version` - Fetch specific version (optional)
  - `:include_metadata` - Include full metadata (default: true)
  """
  @callback fetch_sample(sample_id(), opts :: keyword()) ::
              {:ok, sample_dto()} | {:error, error_reason()}

  @doc """
  Batch fetch multiple samples from Forge.

  Returns samples in same order as input IDs. Missing samples are omitted.
  """
  @callback fetch_samples([sample_id()], opts :: keyword()) ::
              {:ok, [sample_dto()]} | {:error, error_reason()}

  @doc """
  Verifies if a sample exists in Forge (lightweight check).
  """
  @callback verify_sample_exists(sample_id()) :: boolean()

  @doc """
  Fetches only the version tag for a sample (lightweight query).
  """
  @callback fetch_sample_version(sample_id()) :: {:ok, String.t()} | {:error, error_reason()}

  # Delegation functions

  @doc """
  Fetches a single sample using the configured backend.
  """
  @spec fetch_sample(sample_id(), keyword()) :: {:ok, sample_dto()} | {:error, error_reason()}
  def fetch_sample(sample_id, opts \\ []) do
    start_time = System.monotonic_time()

    result = backend().fetch_sample(sample_id, opts)

    duration = System.monotonic_time() - start_time

    :telemetry.execute(
      [:anvil, :forge_bridge, :fetch_sample],
      %{duration: duration},
      %{
        backend: backend(),
        sample_id: sample_id,
        result: elem(result, 0)
      }
    )

    result
  end

  @doc """
  Batch fetches samples using the configured backend.
  """
  @spec fetch_samples([sample_id()], keyword()) ::
          {:ok, [sample_dto()]} | {:error, error_reason()}
  def fetch_samples(sample_ids, opts \\ []) do
    start_time = System.monotonic_time()

    result = backend().fetch_samples(sample_ids, opts)

    duration = System.monotonic_time() - start_time

    :telemetry.execute(
      [:anvil, :forge_bridge, :fetch_samples],
      %{duration: duration, count: length(sample_ids)},
      %{backend: backend(), result: elem(result, 0)}
    )

    result
  end

  @doc """
  Verifies sample existence using the configured backend.
  """
  @spec verify_sample_exists(sample_id()) :: boolean()
  def verify_sample_exists(sample_id) do
    backend().verify_sample_exists(sample_id)
  end

  @doc """
  Fetches sample version using the configured backend.
  """
  @spec fetch_sample_version(sample_id()) :: {:ok, String.t()} | {:error, error_reason()}
  def fetch_sample_version(sample_id) do
    backend().fetch_sample_version(sample_id)
  end

  # Private helpers

  defp backend do
    Application.get_env(:anvil, :forge_bridge_backend, Anvil.ForgeBridge.Direct)
  end
end
</file>

<file path="anvil/label.ex">
defmodule Anvil.Label do
  @moduledoc """
  Represents a label submitted by a labeler for an assignment.

  Labels are validated against the queue's schema before being stored.
  """

  @type t :: %__MODULE__{
          id: String.t(),
          assignment_id: String.t(),
          sample_id: String.t(),
          labeler_id: String.t(),
          values: map(),
          valid?: boolean(),
          errors: [map()],
          labeling_time_seconds: non_neg_integer() | nil,
          created_at: DateTime.t()
        }

  defstruct [
    :id,
    :assignment_id,
    :sample_id,
    :labeler_id,
    :values,
    :valid?,
    :errors,
    :labeling_time_seconds,
    :created_at
  ]

  @doc """
  Creates a new label.
  """
  @spec new(keyword()) :: t()
  def new(opts) do
    struct(
      __MODULE__,
      Keyword.merge(
        [
          id: generate_id(),
          errors: [],
          created_at: DateTime.utc_now()
        ],
        opts
      )
    )
  end

  defp generate_id do
    Ecto.UUID.generate()
  end
end
</file>

<file path="anvil/pii.ex">
defmodule Anvil.PII do
  @moduledoc """
  PII (Personally Identifiable Information) field annotation handling.

  This module provides functions for managing PII metadata on schema fields,
  including PII levels, retention policies, and redaction strategies.

  ## PII Levels

  - `:none` - No PII expected (e.g., boolean labels, enums)
  - `:possible` - May contain PII (free-text fields with guidelines to avoid PII)
  - `:likely` - Expected to contain PII (e.g., labeler feedback, error reports)
  - `:definite` - Always contains PII (e.g., labeler email, IP address)

  ## Retention Policies

  - `:indefinite` - Keep forever (structural labels with no PII)
  - `<integer>` - Days until eligible for deletion (e.g., 90, 365)

  ## Redaction Policies

  - `:preserve` - Keep field unchanged (explicit opt-in)
  - `:strip` - Remove field entirely
  - `:truncate` - Truncate to first N characters
  - `:hash` - Hash value (preserves uniqueness for grouping)
  - `:regex_redact` - Apply regex-based redaction patterns

  ## Examples

      iex> field_metadata = %{pii: :possible, retention_days: 365, redaction_policy: :truncate}
      iex> Anvil.PII.pii_level(field_metadata)
      :possible

      iex> Anvil.PII.redaction_policy(field_metadata)
      :truncate
  """

  @type pii_level :: :none | :possible | :likely | :definite
  @type retention_policy :: :indefinite | pos_integer()
  @type redaction_policy :: :preserve | :strip | :truncate | :hash | :regex_redact

  @doc """
  Returns the PII level for a field based on its metadata.

  Defaults to `:none` if not specified.
  """
  @spec pii_level(map()) :: pii_level()
  def pii_level(metadata) when is_map(metadata) do
    case Map.get(metadata, :pii) do
      level when level in [:none, :possible, :likely, :definite] -> level
      _ -> :none
    end
  end

  @doc """
  Returns the retention policy for a field based on its metadata.

  Defaults to `:indefinite` if not specified.
  """
  @spec retention_policy(map()) :: retention_policy()
  def retention_policy(metadata) when is_map(metadata) do
    case Map.get(metadata, :retention_days) do
      :indefinite -> :indefinite
      days when is_integer(days) and days > 0 -> days
      _ -> :indefinite
    end
  end

  @doc """
  Returns the redaction policy for a field based on its metadata.

  Defaults to automatic policy based on PII level if not specified:
  - `:none` -> `:preserve`
  - `:possible` -> `:truncate`
  - `:likely` -> `:strip`
  - `:definite` -> `:strip`
  """
  @spec redaction_policy(map()) :: redaction_policy()
  def redaction_policy(metadata) when is_map(metadata) do
    case Map.get(metadata, :redaction_policy) do
      policy when policy in [:preserve, :strip, :truncate, :hash, :regex_redact] ->
        policy

      _ ->
        # Default policy based on PII level
        case pii_level(metadata) do
          :none -> :preserve
          :possible -> :truncate
          :likely -> :strip
          :definite -> :strip
        end
    end
  end

  @doc """
  Checks if a field should be redacted during export based on redaction mode.

  ## Redaction Modes

  - `:none` - No redaction (trusted internal exports)
  - `:automatic` - Apply schema-defined redaction policies
  - `:aggressive` - Strip all fields with PII level `:possible` or higher

  ## Examples

      iex> field_meta = %{pii: :none}
      iex> Anvil.PII.should_redact?(field_meta, :automatic)
      false

      iex> field_meta = %{pii: :possible, redaction_policy: :truncate}
      iex> Anvil.PII.should_redact?(field_meta, :automatic)
      true

      iex> field_meta = %{pii: :possible}
      iex> Anvil.PII.should_redact?(field_meta, :aggressive)
      true
  """
  @spec should_redact?(map(), :none | :automatic | :aggressive) :: boolean()
  def should_redact?(_metadata, :none), do: false

  def should_redact?(metadata, :automatic) do
    redaction_policy(metadata) != :preserve
  end

  def should_redact?(metadata, :aggressive) do
    pii_level(metadata) in [:possible, :likely, :definite]
  end

  @doc """
  Returns true if the field has any PII risk (level other than :none).
  """
  @spec has_pii_risk?(map()) :: boolean()
  def has_pii_risk?(metadata) do
    pii_level(metadata) != :none
  end

  @doc """
  Calculates the expiration date for a field based on retention policy.

  Returns `nil` for indefinite retention.

  ## Examples

      iex> submitted_at = ~U[2025-01-01 00:00:00Z]
      iex> metadata = %{retention_days: 90}
      iex> Anvil.PII.expiration_date(metadata, submitted_at)
      ~U[2025-04-01 00:00:00Z]

      iex> metadata = %{retention_days: :indefinite}
      iex> Anvil.PII.expiration_date(metadata, ~U[2025-01-01 00:00:00Z])
      nil
  """
  @spec expiration_date(map(), DateTime.t()) :: DateTime.t() | nil
  def expiration_date(metadata, submitted_at) do
    case retention_policy(metadata) do
      :indefinite -> nil
      days when is_integer(days) -> DateTime.add(submitted_at, days, :day)
    end
  end

  @doc """
  Checks if a field is expired based on retention policy.

  ## Examples

      iex> submitted_at = ~U[2024-01-01 00:00:00Z]
      iex> now = ~U[2025-12-01 00:00:00Z]
      iex> metadata = %{retention_days: 90}
      iex> Anvil.PII.expired?(metadata, submitted_at, now)
      true

      iex> metadata = %{retention_days: :indefinite}
      iex> Anvil.PII.expired?(metadata, submitted_at, now)
      false
  """
  @spec expired?(map(), DateTime.t(), DateTime.t()) :: boolean()
  def expired?(metadata, submitted_at, now \\ DateTime.utc_now()) do
    case expiration_date(metadata, submitted_at) do
      nil -> false
      expiry -> DateTime.compare(now, expiry) == :gt
    end
  end

  @doc """
  Validates PII metadata structure.

  Returns `:ok` if valid, `{:error, reason}` otherwise.
  """
  @spec validate_metadata(map()) :: :ok | {:error, String.t()}
  def validate_metadata(metadata) when is_map(metadata) do
    with :ok <- validate_pii_level(Map.get(metadata, :pii)),
         :ok <- validate_retention_days(Map.get(metadata, :retention_days)),
         :ok <- validate_redaction_policy(Map.get(metadata, :redaction_policy)) do
      :ok
    end
  end

  defp validate_pii_level(nil), do: :ok
  defp validate_pii_level(level) when level in [:none, :possible, :likely, :definite], do: :ok
  defp validate_pii_level(_), do: {:error, "invalid PII level"}

  defp validate_retention_days(nil), do: :ok
  defp validate_retention_days(:indefinite), do: :ok
  defp validate_retention_days(days) when is_integer(days) and days > 0, do: :ok
  defp validate_retention_days(_), do: {:error, "invalid retention_days"}

  defp validate_redaction_policy(nil), do: :ok

  defp validate_redaction_policy(policy)
       when policy in [:preserve, :strip, :truncate, :hash, :regex_redact],
       do: :ok

  defp validate_redaction_policy(_), do: {:error, "invalid redaction_policy"}
end
</file>

<file path="anvil/queue.ex">
defmodule Anvil.Queue do
  @moduledoc """
  GenServer that manages a labeling queue.

  Handles sample assignment, label submission, and queue state management.
  """

  use GenServer
  require Logger

  alias Anvil.{Assignment, Label, Schema, Storage, Telemetry}
  alias Anvil.Queue.Policy

  defstruct [
    :queue_id,
    :schema,
    :policy,
    :policy_config,
    :storage_module,
    :storage_state,
    :assignment_timeout,
    :labels_per_sample,
    :labelers,
    :policy_state
  ]

  # Client API

  @doc """
  Starts a queue process.
  """
  @spec start_link(keyword()) :: GenServer.on_start()
  def start_link(opts) do
    queue_id = Keyword.fetch!(opts, :queue_id)
    GenServer.start_link(__MODULE__, opts, name: via_tuple(queue_id))
  end

  @doc """
  Adds samples to the queue.
  """
  @spec add_samples(pid() | atom(), [map()]) :: :ok | {:error, term()}
  def add_samples(queue, samples) do
    GenServer.call(queue, {:add_samples, samples})
  end

  @doc """
  Adds labelers to the queue.
  """
  @spec add_labelers(pid() | atom(), [String.t()]) :: :ok
  def add_labelers(queue, labelers) do
    GenServer.call(queue, {:add_labelers, labelers})
  end

  @doc """
  Gets the next assignment for a labeler.
  """
  @spec get_next_assignment(pid() | atom(), String.t()) ::
          {:ok, Assignment.t()} | {:error, term()}
  def get_next_assignment(queue, labeler_id) do
    GenServer.call(queue, {:get_next_assignment, labeler_id})
  end

  @doc """
  Starts an assignment.
  """
  @spec start_assignment(pid() | atom(), String.t()) ::
          {:ok, Assignment.t()} | {:error, term()}
  def start_assignment(queue, assignment_id) do
    GenServer.call(queue, {:start_assignment, assignment_id})
  end

  @doc """
  Submits a label for an assignment.
  """
  @spec submit_label(pid() | atom(), String.t(), map()) ::
          {:ok, Label.t()} | {:error, term()}
  def submit_label(queue, assignment_id, values) do
    GenServer.call(queue, {:submit_label, assignment_id, values})
  end

  @doc """
  Skips an assignment.
  """
  @spec skip_assignment(pid() | atom(), String.t(), keyword()) ::
          {:ok, Assignment.t()} | {:error, term()}
  def skip_assignment(queue, assignment_id, opts \\ []) do
    reason = Keyword.get(opts, :reason)
    GenServer.call(queue, {:skip_assignment, assignment_id, reason})
  end

  @doc """
  Gets all labels for the queue.
  """
  @spec get_labels(pid() | atom(), keyword()) :: [Label.t()]
  def get_labels(queue, filters \\ []) do
    GenServer.call(queue, {:get_labels, filters})
  end

  @doc """
  Gets all assignments for the queue.
  """
  @spec get_assignments(pid() | atom(), keyword()) :: [Assignment.t()]
  def get_assignments(queue, filters \\ []) do
    GenServer.call(queue, {:get_assignments, filters})
  end

  # Server Callbacks

  @impl true
  def init(opts) do
    queue_id = Keyword.fetch!(opts, :queue_id)
    schema = Keyword.fetch!(opts, :schema)
    labels_per_sample = Keyword.get(opts, :labels_per_sample, 1)
    # Use redundancy policy by default when labels_per_sample > 1
    default_policy = if labels_per_sample > 1, do: :redundancy, else: :round_robin
    policy = Keyword.get(opts, :policy, default_policy)
    base_policy_config = Keyword.get(opts, :policy_config, %{})
    # Merge labels_per_sample into policy_config for redundancy policy
    policy_config = Map.merge(base_policy_config, %{labels_per_sample: labels_per_sample})
    storage_module = Keyword.get(opts, :storage, Storage.ETS)
    assignment_timeout = Keyword.get(opts, :assignment_timeout, 3600)

    {:ok, storage_state} = storage_module.init(queue_id: queue_id)
    {:ok, policy_state} = Policy.init_policy(policy, policy_config)

    state = %__MODULE__{
      queue_id: queue_id,
      schema: schema,
      policy: policy,
      policy_config: policy_config,
      storage_module: storage_module,
      storage_state: storage_state,
      assignment_timeout: assignment_timeout,
      labels_per_sample: labels_per_sample,
      labelers: [],
      policy_state: policy_state
    }

    # Emit queue created telemetry event
    Telemetry.emit_queue_created(queue_id, %{
      policy_type: policy,
      labels_per_sample: labels_per_sample
    })

    {:ok, state}
  end

  @impl true
  def handle_call({:add_samples, samples}, _from, state) do
    result =
      Enum.reduce_while(samples, {:ok, state}, fn sample, {:ok, acc_state} ->
        case state.storage_module.put_sample(acc_state.storage_state, sample) do
          {:ok, new_storage_state} ->
            {:cont, {:ok, %{acc_state | storage_state: new_storage_state}}}

          {:error, reason} ->
            {:halt, {:error, reason}}
        end
      end)

    case result do
      {:ok, new_state} -> {:reply, :ok, new_state}
      {:error, reason} -> {:reply, {:error, reason}, state}
    end
  end

  @impl true
  def handle_call({:add_labelers, labelers}, _from, state) do
    new_labelers = Enum.uniq(state.labelers ++ labelers)
    {:reply, :ok, %{state | labelers: new_labelers}}
  end

  @impl true
  def handle_call({:get_next_assignment, labeler_id}, _from, state) do
    with {:ok, available_samples} <- get_available_samples(state, labeler_id),
         {:ok, sample} <-
           Policy.next_sample(state.policy, state.policy_state, labeler_id, available_samples) do
      # Wrap assignment dispatch in telemetry span
      Telemetry.span_assignment_dispatch(
        %{queue_id: state.queue_id, labeler_id: labeler_id, policy_type: state.policy},
        fn ->
          assignment =
            Assignment.new(
              sample_id: sample.id,
              labeler_id: labeler_id,
              queue_id: state.queue_id
            )

          {:ok, new_storage_state} =
            state.storage_module.put_assignment(state.storage_state, assignment)

          new_policy_state = Policy.update_state(state.policy, state.policy_state, sample)

          new_state = %{
            state
            | storage_state: new_storage_state,
              policy_state: new_policy_state
          }

          # Emit assignment created event
          Telemetry.emit_assignment_created(assignment.id, %{
            queue_id: state.queue_id,
            labeler_id: labeler_id,
            sample_id: sample.id
          })

          {{:reply, {:ok, assignment}, new_state},
           %{sample_id: sample.id, eligible_samples: length(available_samples)}}
        end
      )
    else
      {:error, reason} -> {:reply, {:error, reason}, state}
    end
  end

  @impl true
  def handle_call({:start_assignment, assignment_id}, _from, state) do
    with {:ok, assignment, storage_state} <-
           state.storage_module.get_assignment(state.storage_state, assignment_id),
         {:ok, updated_assignment} <- Assignment.start(assignment, state.assignment_timeout),
         {:ok, new_storage_state} <-
           state.storage_module.put_assignment(storage_state, updated_assignment) do
      {:reply, {:ok, updated_assignment}, %{state | storage_state: new_storage_state}}
    else
      {:error, reason} -> {:reply, {:error, reason}, state}
    end
  end

  @impl true
  def handle_call({:submit_label, assignment_id, values}, _from, state) do
    # Wrap label submission in telemetry span
    Telemetry.span_label_submit(
      %{assignment_id: assignment_id, queue_id: state.queue_id},
      fn ->
        with {:ok, assignment, storage_state} <-
               state.storage_module.get_assignment(state.storage_state, assignment_id),
             {:ok, validated_values} <- Schema.validate(state.schema, values),
             labeling_time <- Assignment.labeling_time_seconds(assignment),
             label <- create_label(assignment, validated_values, labeling_time),
             {:ok, storage_state} <- state.storage_module.put_label(storage_state, label),
             {:ok, completed_assignment} <- Assignment.complete(assignment, label.id),
             {:ok, new_storage_state} <-
               state.storage_module.put_assignment(storage_state, completed_assignment) do
          # Emit assignment completed event
          Telemetry.emit_assignment_completed(assignment_id, %{
            queue_id: state.queue_id,
            labeler_id: assignment.labeler_id,
            labeling_time_seconds: labeling_time
          })

          {{:reply, {:ok, label}, %{state | storage_state: new_storage_state}},
           %{validation_errors: 0}}
        else
          {:error, errors} when is_list(errors) ->
            # Emit validation failed event
            Telemetry.emit_label_validation_failed(assignment_id, errors, %{
              queue_id: state.queue_id
            })

            {{:reply, {:error, {:validation_failed, errors}}, state},
             %{validation_errors: length(errors)}}

          {:error, reason} ->
            {{:reply, {:error, reason}, state}, %{error: reason}}
        end
      end
    )
  end

  @impl true
  def handle_call({:skip_assignment, assignment_id, reason}, _from, state) do
    with {:ok, assignment, storage_state} <-
           state.storage_module.get_assignment(state.storage_state, assignment_id),
         {:ok, skipped_assignment} <- Assignment.skip(assignment, reason),
         {:ok, new_storage_state} <-
           state.storage_module.put_assignment(storage_state, skipped_assignment) do
      {:reply, {:ok, skipped_assignment}, %{state | storage_state: new_storage_state}}
    else
      {:error, reason} -> {:reply, {:error, reason}, state}
    end
  end

  @impl true
  def handle_call({:get_labels, filters}, _from, state) do
    {:ok, labels, _storage_state} = state.storage_module.list_labels(state.storage_state, filters)
    {:reply, labels, state}
  end

  @impl true
  def handle_call({:get_assignments, filters}, _from, state) do
    {:ok, assignments, _storage_state} =
      state.storage_module.list_assignments(state.storage_state, filters)

    {:reply, assignments, state}
  end

  # Private Functions

  defp get_available_samples(state, labeler_id) do
    {:ok, all_samples, _} = state.storage_module.list_samples(state.storage_state, [])
    {:ok, assignments, _} = state.storage_module.list_assignments(state.storage_state, [])

    # Count labels per sample
    label_counts =
      assignments
      |> Enum.filter(&(&1.status == :completed))
      |> Enum.frequencies_by(& &1.sample_id)

    # Get samples that haven't been assigned to this labeler yet
    assigned_to_labeler =
      assignments
      |> Enum.filter(&(&1.labeler_id == labeler_id))
      |> Enum.map(& &1.sample_id)
      |> MapSet.new()

    available =
      all_samples
      |> Enum.reject(fn sample ->
        label_count = Map.get(label_counts, sample.id, 0)
        label_count >= state.labels_per_sample || MapSet.member?(assigned_to_labeler, sample.id)
      end)

    {:ok, available}
  end

  defp create_label(assignment, values, labeling_time) do
    Label.new(
      assignment_id: assignment.id,
      sample_id: assignment.sample_id,
      labeler_id: assignment.labeler_id,
      values: values,
      valid?: true,
      labeling_time_seconds: labeling_time
    )
  end

  defp via_tuple(queue_id) do
    {:via, Registry, {Anvil.Registry, queue_id}}
  end
end
</file>

<file path="anvil/repo.ex">
defmodule Anvil.Repo do
  @moduledoc """
  Ecto repository for Anvil's Postgres storage adapter.

  Provides database access for labeling queues, assignments, labels,
  and related entities.
  """

  use Ecto.Repo,
    otp_app: :anvil,
    adapter: Ecto.Adapters.Postgres
end
</file>

<file path="anvil/schema.ex">
defmodule Anvil.Schema do
  @moduledoc """
  Defines the structure and validation rules for labels.

  Schemas are domain-agnostic and support various field types for
  diverse annotation tasks.
  """

  alias Anvil.Schema.Field

  @type t :: %__MODULE__{
          name: String.t(),
          version: String.t(),
          fields: [Field.t()],
          metadata: map()
        }

  defstruct [
    :name,
    version: "1.0",
    fields: [],
    metadata: %{}
  ]

  @doc """
  Creates a new schema with the given options.

  ## Examples

      iex> schema = Anvil.Schema.new(
      ...>   name: "sentiment",
      ...>   fields: [
      ...>     %Anvil.Schema.Field{
      ...>       name: "score",
      ...>       type: :range,
      ...>       required: true,
      ...>       min: 1,
      ...>       max: 5
      ...>     }
      ...>   ]
      ...> )
      iex> schema.name
      "sentiment"
  """
  @spec new(keyword()) :: t()
  def new(opts) do
    struct(__MODULE__, opts)
  end

  @doc """
  Validates a map of values against the schema.

  Returns `{:ok, values}` if valid, or `{:error, errors}` with a list
  of validation errors.

  ## Examples

      iex> schema = Anvil.Schema.new(
      ...>   name: "test",
      ...>   fields: [
      ...>     %Anvil.Schema.Field{name: "category", type: :select, required: true, options: ["a", "b"]}
      ...>   ]
      ...> )
      iex> Anvil.Schema.validate(schema, %{"category" => "a"})
      {:ok, %{"category" => "a"}}
      iex> Anvil.Schema.validate(schema, %{"category" => "c"})
      {:error, [%{field: "category", error: "must be one of: a, b"}]}
  """
  @spec validate(t(), map()) :: {:ok, map()} | {:error, [map()]}
  def validate(%__MODULE__{fields: fields}, values) do
    errors =
      fields
      |> Enum.map(fn field ->
        value = Map.get(values, field.name)

        case Field.validate(field, value) do
          :ok -> nil
          {:error, message} -> %{field: field.name, error: message}
        end
      end)
      |> Enum.reject(&is_nil/1)

    if Enum.empty?(errors) do
      {:ok, values}
    else
      {:error, errors}
    end
  end

  @doc """
  Returns the field with the given name, or nil if not found.
  """
  @spec get_field(t(), String.t()) :: Field.t() | nil
  def get_field(%__MODULE__{fields: fields}, name) do
    Enum.find(fields, &(&1.name == name))
  end

  @doc """
  Returns all required field names.
  """
  @spec required_fields(t()) :: [String.t()]
  def required_fields(%__MODULE__{fields: fields}) do
    fields
    |> Enum.filter(& &1.required)
    |> Enum.map(& &1.name)
  end

  @doc """
  Returns all optional field names.
  """
  @spec optional_fields(t()) :: [String.t()]
  def optional_fields(%__MODULE__{fields: fields}) do
    fields
    |> Enum.reject(& &1.required)
    |> Enum.map(& &1.name)
  end
end
</file>

<file path="anvil/storage.ex">
defmodule Anvil.Storage do
  @moduledoc """
  Behaviour for storage backends.

  Allows pluggable storage implementations (ETS, Postgres, etc.)
  """

  @callback init(opts :: keyword()) :: {:ok, state :: any()} | {:error, term()}

  @callback put_assignment(state :: any(), assignment :: Anvil.Assignment.t()) ::
              {:ok, state :: any()} | {:error, term()}

  @callback get_assignment(state :: any(), id :: String.t()) ::
              {:ok, Anvil.Assignment.t(), state :: any()} | {:error, term()}

  @callback list_assignments(state :: any(), filters :: keyword()) ::
              {:ok, [Anvil.Assignment.t()], state :: any()}

  @callback put_label(state :: any(), label :: Anvil.Label.t()) ::
              {:ok, state :: any()} | {:error, term()}

  @callback get_label(state :: any(), id :: String.t()) ::
              {:ok, Anvil.Label.t(), state :: any()} | {:error, term()}

  @callback list_labels(state :: any(), filters :: keyword()) ::
              {:ok, [Anvil.Label.t()], state :: any()}

  @callback put_sample(state :: any(), sample :: map()) ::
              {:ok, state :: any()} | {:error, term()}

  @callback get_sample(state :: any(), id :: String.t()) ::
              {:ok, map(), state :: any()} | {:error, term()}

  @callback list_samples(state :: any(), filters :: keyword()) ::
              {:ok, [map()], state :: any()}
end
</file>

<file path="anvil/telemetry.ex">
defmodule Anvil.Telemetry do
  @moduledoc """
  Telemetry integration for Anvil labeling system.

  Provides instrumentation for all core operations following the event naming convention:
  `[:anvil, domain, action, lifecycle?]`

  ## Event Categories

  - **Queue Events**: queue creation, status changes
  - **Assignment Events**: dispatch, timeout, completion
  - **Label Events**: submission, validation
  - **Agreement Events**: computation, low score detection
  - **Export Events**: generation, progress tracking
  - **Storage Events**: query timing

  ## Usage

  ### Emitting Events

      # Count event
      Anvil.Telemetry.emit_queue_created(queue_id, metadata)

      # Duration event (using span)
      Anvil.Telemetry.span(:assignment_dispatch, metadata, fn ->
        result = perform_dispatch()
        {result, additional_metadata}
      end)

  ### Attaching Handlers

      :telemetry.attach(
        "my-handler",
        [:anvil, :label, :submit, :stop],
        &MyModule.handle_event/4,
        nil
      )

  ### Testing

      import Telemetry.Test

      test "emits telemetry event" do
        attach_telemetry_handler([:anvil, :queue, :created])

        Anvil.Telemetry.emit_queue_created(queue_id, %{})

        assert_received {:telemetry, [:anvil, :queue, :created], %{}, %{queue_id: ^queue_id}}
      end
  """

  # Queue Events

  @doc """
  Emits a queue created event.
  """
  @spec emit_queue_created(binary(), map()) :: :ok
  def emit_queue_created(queue_id, metadata) do
    :telemetry.execute(
      [:anvil, :queue, :created],
      %{},
      Map.merge(metadata, %{queue_id: queue_id})
    )
  end

  @doc """
  Emits a queue status changed event.
  """
  @spec emit_queue_status_changed(binary(), atom(), atom(), map()) :: :ok
  def emit_queue_status_changed(queue_id, from_status, to_status, metadata) do
    :telemetry.execute(
      [:anvil, :queue, :status_changed],
      %{},
      Map.merge(metadata, %{
        queue_id: queue_id,
        from_status: from_status,
        to_status: to_status
      })
    )
  end

  # Assignment Events

  @doc """
  Wraps assignment dispatch in a telemetry span.

  Returns `{result, metadata}` tuple from the function.
  """
  @spec span_assignment_dispatch(map(), (-> {any(), map()})) :: any()
  def span_assignment_dispatch(metadata, fun) do
    :telemetry.span(
      [:anvil, :assignment, :dispatch],
      metadata,
      fun
    )
  end

  @doc """
  Emits an assignment created event.
  """
  @spec emit_assignment_created(binary(), map()) :: :ok
  def emit_assignment_created(assignment_id, metadata) do
    :telemetry.execute(
      [:anvil, :assignment, :created],
      %{},
      Map.merge(metadata, %{assignment_id: assignment_id})
    )
  end

  @doc """
  Emits an assignment completed event.
  """
  @spec emit_assignment_completed(binary(), map()) :: :ok
  def emit_assignment_completed(assignment_id, metadata) do
    :telemetry.execute(
      [:anvil, :assignment, :completed],
      %{},
      Map.merge(metadata, %{assignment_id: assignment_id})
    )
  end

  @doc """
  Emits an assignment expired event.
  """
  @spec emit_assignment_expired(binary(), map()) :: :ok
  def emit_assignment_expired(assignment_id, metadata) do
    :telemetry.execute(
      [:anvil, :assignment, :expired],
      %{},
      Map.merge(metadata, %{assignment_id: assignment_id})
    )
  end

  @doc """
  Emits an assignment timeout event (batch).
  """
  @spec emit_assignment_timed_out(integer(), map()) :: :ok
  def emit_assignment_timed_out(count, metadata) do
    :telemetry.execute(
      [:anvil, :assignment, :timed_out],
      %{count: count},
      metadata
    )
  end

  # Label Events

  @doc """
  Wraps label submission in a telemetry span.
  """
  @spec span_label_submit(map(), (-> {any(), map()})) :: any()
  def span_label_submit(metadata, fun) do
    :telemetry.span(
      [:anvil, :label, :submit],
      metadata,
      fun
    )
  end

  @doc """
  Emits a label validation failed event.
  """
  @spec emit_label_validation_failed(binary(), list(), map()) :: :ok
  def emit_label_validation_failed(assignment_id, errors, metadata) do
    :telemetry.execute(
      [:anvil, :label, :validation_failed],
      %{error_count: length(errors)},
      Map.merge(metadata, %{
        assignment_id: assignment_id,
        errors: errors
      })
    )
  end

  # Agreement Events

  @doc """
  Wraps agreement computation in a telemetry span.
  """
  @spec span_agreement_compute(map(), (-> {any(), map()})) :: any()
  def span_agreement_compute(metadata, fun) do
    :telemetry.span(
      [:anvil, :agreement, :compute],
      metadata,
      fun
    )
  end

  @doc """
  Emits a low agreement score event.
  """
  @spec emit_low_agreement_score(float(), map()) :: :ok
  def emit_low_agreement_score(score, metadata) do
    :telemetry.execute(
      [:anvil, :agreement, :low_score],
      %{value: score},
      metadata
    )
  end

  @doc """
  Wraps batch agreement recomputation in a telemetry span.
  """
  @spec span_agreement_batch_recompute(map(), (-> {any(), map()})) :: any()
  def span_agreement_batch_recompute(metadata, fun) do
    :telemetry.span(
      [:anvil, :agreement, :batch_recompute],
      metadata,
      fun
    )
  end

  # Export Events

  @doc """
  Wraps export generation in a telemetry span.
  """
  @spec span_export_generate(map(), (-> {any(), map()})) :: any()
  def span_export_generate(metadata, fun) do
    :telemetry.span(
      [:anvil, :export, :generate],
      metadata,
      fun
    )
  end

  @doc """
  Emits an export progress event.
  """
  @spec emit_export_progress(integer(), map()) :: :ok
  def emit_export_progress(rows_processed, metadata) do
    :telemetry.execute(
      [:anvil, :export, :progress],
      %{rows_processed: rows_processed},
      metadata
    )
  end

  @doc """
  Emits an export completed event.
  """
  @spec emit_export_completed(binary(), map()) :: :ok
  def emit_export_completed(export_id, metadata) do
    :telemetry.execute(
      [:anvil, :export, :completed],
      %{},
      Map.merge(metadata, %{export_id: export_id})
    )
  end

  @doc """
  Emits an export failed event.
  """
  @spec emit_export_failed(binary(), term(), map()) :: :ok
  def emit_export_failed(export_id, reason, metadata) do
    :telemetry.execute(
      [:anvil, :export, :failed],
      %{},
      Map.merge(metadata, %{
        export_id: export_id,
        reason: reason
      })
    )
  end

  # Storage Events

  @doc """
  Wraps storage query in a telemetry span.
  """
  @spec span_storage_query(binary(), map(), (-> {any(), map()})) :: any()
  def span_storage_query(operation, metadata, fun) do
    :telemetry.span(
      [:anvil, :storage, :query],
      Map.merge(metadata, %{operation: operation}),
      fun
    )
  end

  # Schema Migration Events

  @doc """
  Emits a schema validation event.
  """
  @spec emit_schema_validation(binary(), boolean(), map()) :: :ok
  def emit_schema_validation(schema_id, valid?, metadata) do
    :telemetry.execute(
      [:anvil, :schema, :validation],
      %{valid: if(valid?, do: 1, else: 0)},
      Map.merge(metadata, %{schema_id: schema_id, valid?: valid?})
    )
  end

  @doc """
  Emits a schema migration event.
  """
  @spec emit_schema_migration(binary(), binary(), map()) :: :ok
  def emit_schema_migration(from_version, to_version, metadata) do
    :telemetry.execute(
      [:anvil, :schema, :migration],
      %{},
      Map.merge(metadata, %{
        from_version: from_version,
        to_version: to_version
      })
    )
  end

  # Generic span helper

  @doc """
  Generic telemetry span wrapper.

  The function must return a `{result, metadata}` tuple where metadata will be
  merged with the initial metadata for the stop/exception events.

  ## Examples

      Anvil.Telemetry.span(:my_operation, %{queue_id: id}, fn ->
        result = do_work()
        {result, %{work_count: 42}}
      end)

  This emits:
  - `[:anvil, :my_operation, :start]` with initial metadata
  - `[:anvil, :my_operation, :stop]` with duration and merged metadata
  - `[:anvil, :my_operation, :exception]` if an error occurs
  """
  @spec span(atom(), map(), (-> {any(), map()})) :: any()
  def span(operation, metadata, fun) do
    :telemetry.span(
      [:anvil, operation],
      metadata,
      fun
    )
  end
end
</file>

<file path="anvil.ex">
defmodule Anvil do
  @moduledoc """
  Labeling queue library for managing human labeling workflows.

  Anvil is a domain-agnostic system for orchestrating human annotation tasks
  across any sample type - images, text, audio, video, or custom data structures.

  ## Quick Start

      # 1. Define a schema
      schema = Anvil.Schema.new(
        name: "sentiment",
        fields: [
          %Anvil.Schema.Field{
            name: "sentiment",
            type: :select,
            required: true,
            options: ["positive", "negative", "neutral"]
          }
        ]
      )

      # 2. Create a queue
      {:ok, queue} = Anvil.create_queue(
        queue_id: "sentiment_queue",
        schema: schema
      )

      # 3. Add samples
      Anvil.add_samples(queue, [
        %{id: "s1", text: "Great product!"},
        %{id: "s2", text: "Not good."}
      ])

      # 4. Add labelers
      Anvil.add_labelers(queue, ["labeler_1", "labeler_2"])

      # 5. Get assignment
      {:ok, assignment} = Anvil.get_next_assignment(queue, "labeler_1")

      # 6. Submit label
      {:ok, label} = Anvil.submit_label(queue, assignment.id, %{"sentiment" => "positive"})

  """

  alias Anvil.Queue

  @doc """
  Creates a new labeling queue.

  ## Options

    * `:queue_id` - Unique identifier for the queue (required)
    * `:schema` - LabelSchema defining the label structure (required)
    * `:policy` - Assignment policy (`:round_robin`, `:random`, `:expertise`) (default: `:round_robin`)
    * `:labels_per_sample` - Number of labels needed per sample (default: 1)
    * `:assignment_timeout` - Timeout in seconds (default: 3600)

  """
  @spec create_queue(keyword()) :: {:ok, pid()} | {:error, term()}
  def create_queue(opts) do
    Queue.start_link(opts)
  end

  @doc """
  Adds samples to a queue for labeling.
  """
  @spec add_samples(pid() | atom(), [map()]) :: :ok | {:error, term()}
  defdelegate add_samples(queue, samples), to: Queue

  @doc """
  Adds labelers to a queue.
  """
  @spec add_labelers(pid() | atom(), [String.t()]) :: :ok
  defdelegate add_labelers(queue, labelers), to: Queue

  @doc """
  Gets the next assignment for a labeler.
  """
  @spec get_next_assignment(pid() | atom(), String.t()) ::
          {:ok, Anvil.Assignment.t()} | {:error, term()}
  defdelegate get_next_assignment(queue, labeler_id), to: Queue

  @doc """
  Submits a label for an assignment.
  """
  @spec submit_label(pid() | atom(), String.t(), map()) ::
          {:ok, Anvil.Label.t()} | {:error, term()}
  defdelegate submit_label(queue, assignment_id, values), to: Queue

  @doc """
  Skips an assignment.
  """
  @spec skip_assignment(pid() | atom(), String.t(), keyword()) ::
          {:ok, Anvil.Assignment.t()} | {:error, term()}
  defdelegate skip_assignment(queue, assignment_id, opts \\ []), to: Queue

  @doc """
  Computes inter-rater agreement metrics.
  """
  @spec compute_agreement(pid() | atom(), keyword()) :: {:ok, float()} | {:error, term()}
  def compute_agreement(queue, opts \\ []) do
    labels = Queue.get_labels(queue)
    Anvil.Agreement.compute(labels, opts)
  end

  @doc """
  Exports labeled data.
  """
  @spec export(pid() | atom(), keyword()) :: :ok | {:error, term()}
  defdelegate export(queue, opts), to: Anvil.Export
end
</file>

</files>
